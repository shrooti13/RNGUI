<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml">  <head>    <meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8" />    <!-- ///*** Insert title below ***/// -->    <title>NSX for vSphere 6.2.5 Release Notes</title>  </head>  <body>    <div id="ibg_styles"></div>    <!-- import background styles -->    <div id="container">      <div id="ivmware_logo"></div>      <!-- import vmware logo -->      <div id="content-container">        <div id="content">          <table cellspacing="0" cellpadding="0" id="main-table">            <tbody>              <tr>                <td id="main-body">                  <!-- ///*** start of content area ***/// -->                  <h1>NSX for vSphere 6.2.5 Release Notes</h1>                  <table width="100%" cellspacing="0" cellpadding="10" border="0">                    <tbody>                      <tr>                        <td class="features">                          <p><strong>Document updated 30 November 2016</strong><br />                            NSX for vSphere 6.2.4   |   Released 25 August 2016                              |   Build 4292526 </p>                        </td>                      </tr>                    </tbody>                  </table>                  <h2>What's in the Release Notes</h2>                  <p>The release notes cover the following topics:</p>                  <ul>                    <!-- <li><a href="#importantupdates">Important</a></li> -->                    <li><a href="#whatsnew">What's New</a></li>                    <li><a href="#sysreqs">Recommended Versions, System                        Requirements and Installation</a></li>                    <li><a href="#deprecs">Deprecated and Discontinued                        Functionality</a></li>                    <li><a href="#upgradenotes">Upgrade Notes</a></li>                    <li><a href="#knownissues">Known Issues</a></li>                    <li><a href="#resolvedissues">Resolved Issues</a></li>                    <li><a href="#revisionhistory">Document Revision History</a></li>                  </ul>                  <a name="whatsnew"></a>                  <h2>What's New</h2>                  <!-- NOTE TOC only needed in maintenance releases, not in dot-oh release. -->                  <p>See what's new and changed in NSX                    <a href="#newin625">6.2.5</a>                    <a href="#newin624">6.2.4</a>,                    <a href="#newin623">6.2.3</a>,                    <a href="#newin622">6.2.2</a>,                    <a href="#newin621">6.2.1</a> and                    <a href="#newin620">6.2.0</a>.                  </p>                  <p>See <a href="#important623">important information about                      NSX 6.2.3</a>.</p>                  <p></p>                  <!--==================================New in 6.2.4==============================================================--->                  <!--==================================New in 6.2.4==============================================================--->                  <a name="newin625"></a>                  <h3>New in 6.2.5</h3>                  <i>{Placeholder}</i>                  <a name="newin624"></a>                  <h3>New in 6.2.4</h3>                  <p>The 6.2.4 release includes the following new features. It                    also delivers a number of bug fixes that have been                    documented in the <a href="#fixedin624">Resolved Issues</a>                    section. </p>                  <ul>                    <li><strong>Changes in firewall status API (<tt>GET                          /api/4.0/firewall/globalroot-0/status</tt>)</strong>                      <ul>                        <br />                        <li><strong>Firewall Status API has been enhanced to                            include status of object updates used in firewall                            rules:</strong> The firewall status API displays a                          generation number (<tt>generationNumber</tt>) for each                          rule set, which can be used to verify whether a change                          in rule sets has propagated to a host. In 6.2.4, a                          generation number for objects (<tt>generationNumberObjects</tt>)                          has been added to the status API. This allows you to                          verify whether a change in objects consumed in                          firewall rules has propagated to a host. Note that the                          object generation number may change frequently and                          will always be equal to or greater than the ruleset                          generation number.                        </li>                        <br />                        <li><strong>Hosts and clusters not participating in                            firewall are excluded from output:</strong> Clusters                          (and hosts inside the clusters) are no longer included                          in the status output if distributed firewall is                          disabled at the cluster level, or if the cluster is                          not prepared (NSX VIBs are not installed). In earlier                          versions of NSX these clusters and hosts are included                          in the output. However, because they are not                          configured for firewall, after a firewall rule publish                          their status is <em>inprogress</em>.</li>                      </ul>                      <br />                    </li>                    <li><strong>Critical bug fixes identified in NSX 6.2.3</strong>:                      NSX 6.2.4 delivers a security patch for CVE-2016-2079                      which is a critical input validation vulnerability for                      sites that uses NSX SSL VPN. For customers who use SSL                      VPN, VMware strongly recommends a review of CVE-2016-2079                      and an upgrade to NSX 6.2.4 or later.</li>                  </ul>                  <p></p>                  <!-- <p>Changes introduced in NSX vSphere 6.2.4:<br /></p> -->                  <!--==================================Important Info about 6.2.3=================================================--->                  <!--==================================Important Info about 6.2.3=================================================--->                  <!--==================================Important Info about 6.2.3=================================================--->                  <a name="important623"></a>                  <h3>Important Information about NSX for vSphere 6.2.3</h3>                  <p>For customers who have installed NSX 6.2.3 or 6.2.3a,                    VMware recommends installing NSX 6.2.4 or later to address                    critical bug fixes.</p>                  <!--==================================New in 6.2.3==============================================================--->                  <!--==================================New in 6.2.3==============================================================--->                  <!--==================================New in 6.2.3==============================================================--->                  <a name="newin623"></a>                  <h3>New in 6.2.3</h3>                  <p>The 6.2.3 release delivers a security patch to address                    CVE-2016-2079. CVE-2016-2079 is a critical input validation                    vulnerability affecting sites that use NSX SSL-VPN. The                    release also includes a number of bug fixes documented in                    the <a href="#fixedin62x">Resolved Issues</a> section.<br />                  </p>                  <p>Changes introduced in NSX vSphere 6.2.3:</p>                  <ul>                    <li>                      <p><strong>Logical Switching and Routing</strong></p>                    </li>                    <ul>                      <li>                        <p><strong>NSX Hardware Layer 2 Gateway Integration</strong>:                          expands physical connectivity options by integrating                          3rd-party hardware gateway switches into the NSX                          logical network</p>                      </li>                      <li>                        <p><strong>New VXLAN Port 4789 in NSX 6.2.3 and later</strong>:                          Before version 6.2.3, the default VXLAN UDP port                          number was 8472. See the NSX Upgrade Guide for                          details.</p>                      </li>                    </ul>                    <li>                      <p><strong>Networking and Edge Services</strong></p>                    </li>                    <ul>                      <li>                        <p><strong>New Edge DHCP Options</strong>: DHCP Option                          121 supports static route option, which is used for                          DHCP server to publish static routes to DHCP client;                          DHCP Options 66, 67, 150 supports DHCP options for PXE                          Boot; and DHCP Option 26 supports configuration of                          DHCP client network interface MTU by DHCP server.</p>                      </li>                      <li>                        <p><strong>Increase in DHCP Pool, static binding limits</strong>:                          The following are the new limit numbers for various                          form factors: Compact: 2048; Large: 4096; Quad large:                          4096; and X-large: 8192.</p>                      </li>                      <li>                        <p><strong>Edge Firewall adds SYN flood protection</strong>:                          Avoid service disruptions by enabling SYN flood                          protection for transit traffic. Feature is disabled by                          default, use the NSX REST API to enable it.</p>                      </li>                      <li>                        <p><strong>NSX Edge — On Demand Failover</strong>:                          Enables users to initiate on-demand failover when                          needed.</p>                      </li>                      <li>                        <p><strong>NSX Edge — Default memory for Quad Large NSX                            Edge</strong>: Has increased from 1GB to 2GB.</p>                      </li>                      <li>                        <p><strong>NSX Edge — Resource Reservation</strong>:                          Reserves CPU/Memory for NSX Edge during creation. The                          CPU/Memory reserved is based on the Edge appliance                          form factor. You can change the default CPU and                          memory resource reservation percentages using this                          API.                          The CPU/Memory percentage can be set to 0 percent each                          to disable resource reservation. </p>                        <p><tt>PUT                            https://&lt;NSXManager&gt;/api/4.0/edgePublish/tuningConfiguration</tt></p>                        <p> </p>                        <pre>            &lt;tuningConfiguration&gt;               &lt;lockUpdatesOnEdge&gt;false&lt;/lockUpdatesOnEdge&gt;               &lt;aggregatePublishing&gt;true&lt;/aggregatePublishing&gt;               &lt;edgeVMHealthCheckIntervalInMin&gt;0&lt;/edgeVMHealthCheckIntervalInMin&gt;               &lt;healthCheckCommandTimeoutInMs&gt;120000&lt;/healthCheckCommandTimeoutInMs&gt;               &lt;maxParallelVixCallsForHealthCheck&gt;25&lt;/maxParallelVixCallsForHealthCheck&gt;               &lt;publishingTimeoutInMs&gt;1200000&lt;/publishingTimeoutInMs&gt;               &lt;edgeVCpuReservationPercentage&gt;0&lt;/edgeVCpuReservationPercentage&gt;               &lt;edgeMemoryReservationPercentage&gt;0&lt;/edgeMemoryReservationPercentage&gt;               &lt;megaHertzPerVCpu&gt;1000&lt;/megaHertzPerVCpu&gt;            &lt;/tuningConfiguration&gt;          </pre>                        <p></p>                      </li>                      <li>                        <p><strong>Change in NSX Edge Upgrade Behavior</strong>:                          Replacement NSX Edge VMs are deployed before upgrade                          or redeploy. The host must have sufficient resources                          for four NSX Edge VMs during the upgrade or redeploy                          of an Edge HA pair. Default value for TCP connection                          timeout is changed to 21600 seconds from the previous                          value of 3600 seconds. </p>                      </li>                      <p></p>                      <li>                        <p><strong>Cross VC NSX — Universal Distributed Logical                            Router (DLR) Upgrade</strong>: Auto upgrade of                          Universal DLR on secondary NSX Manager, once upgraded                          on primary NSX Manager.</p>                      </li>                      <li>                        <p><strong>Flexible SNAT / DNAT rule creation</strong>:                          <em>vnicId</em> no longer needed as an input                          parameter; removed requirement that the DNAT address                          must be the address of an NSX Edge VNIC.</p>                      </li>                      <!-- DOCNOTES: 1658019 6.2.3 -->                      <li>                        <p><strong>NSX Edge VM (ESG, DLR) now shows both Live                            Location and Desired Location.</strong> NSX Manager                          and NSX APIs including GET api/4.0/edges/<edge-id>/appliances                            now return configuredResourcePool and                            configuredDataStore in addition to current location.</edge-id></p>                      </li>                      <!-- DOCNOTES: 1667460 6.2.3 -->                      <li>                        <p><strong>Edge Firewall adds SYN flood protection:</strong>                          Avoid service disruptions by enabling SYN flood                          protection for transit traffic. Feature is disabled by                          default, use the NSX REST API to enable it.</p>                      </li>                      <!-- DOCNOTES: 1667460 6.2.3 -->                      <li>                        <p><strong>NSX Manager exposes the ESXi hostname </strong>                          on which the 3rd-party VM Series firewall SVM is                          running to improve operational manageability in                          large-scale environments.</p>                      </li>                      <!-- DOCNOTES: 1618365 6.2.3 -->                      <li>                        <p> <strong>NAT rule</strong> now can be applied to a                          VNIC interface and not only an IP address.</p>                      </li>                      <p></p>                      <!-- DOCNOTES: 1644630 6.2.3 -->                      <li>                        <p><strong>New configuration option to set the load                            balancer session aging time:</strong>                          This release delivers a new application rule command                          to set the session aging timeout value for both the                          server                          and client.                          If the pool is shared among multiple virtual servers,                          the maximum value will be set for it.                        </p>                      </li>                      <p></p>                      <!-- DOCNOTES: 1671077 6.2.3 -->                      <li>                        <p><strong>NSX API now returns XML output by default                            when "Accept" header is not provided: </strong>                          Beginning in NSX 6.2.3, if the "Accept:" header is not                          provided in a REST API call, then the default                          formatting of NSX API return values is XML. Previously                          the NSX API returned JSON-formatted output by default.                          To receive JSON-formatted output, the API user must                          explicitly set "application/json" in the "Accept:"                          header when calling the function.                        </p>                      </li>                      <p></p>                      <!--Doc bug 1689566 -->                      <li>                        <p><strong>New NSX API to change the autodraft setting                            for NSX distributed firewall</strong>: Starting with                          NSX 6.2.3, the following PUT API can be used to change                          the autodraft setting for the NSX distributed                          firewall:                        </p>                        <ul>                          <p></p>                          <li>Get the existing GlobalConfiguration: <br />                            GET <tt>https://NSX-Manager-IP-Address/api/4.0/firewall/config/globalconfiguration                              </tt><br /> </li>                          <strong>Note</strong>: GET will not show the                          autoDraftDisabled field.                          <p></p>                          <li>Add autoDraftDisabled config property to the                            global configuration and execute a PUT API call:<br />                            PUT <tt>https://NSX-Manager-IP-Address/api/4.0/firewall/config/globalconfiguration</tt>                            <br />                          </li>                          Request body:<br />                          <pre>  &lt;globalConfiguration&gt;    &lt;layer3RuleOptimize&gt;...&lt;/layer3RuleOptimize&gt;    &lt;layer2RuleOptimize&gt;...&lt;/layer2RuleOptimize&gt;    &lt;tcpStrictOption&gt;...&lt;/tcpStrictOption&gt;    &lt;autoDraftDisabled&gt;true&lt;/autoDraftDisabled&gt;  &lt;/globalConfiguration&gt;  </pre>                          <p></p>                        </ul>                      </li>                    </ul>                    <li>                      <p><strong>Security Services</strong></p>                    </li>                    <ul>                      <li>                        <p><strong>Distributed Firewall — TFTP ALG</strong>:                          Enables use cases such as network boot for VMs.</p>                      </li>                      <li>                        <p><strong>Firewall — Granular Rule Filtering</strong>:                          Simplifies troubleshooting by providing granular rule                          filters in UI, based on Source, Destination, Action,                          Enabled/Disabled, Logging, Name, Comments, Rule ID,                          Tag, Service, Protocol.</p>                      </li>                      <li>                        <p><strong>Guest Introspection — Windows 10 support</strong></p>                      </li>                      <li>                        <p><strong>SSL VPN Client — Mac OS El Capitan support</strong></p>                      </li>                      <li>                        <p><strong>Service Composer — Performance Improvements</strong>:                          Enables faster startup/reboot of NSX Manager by                          optimizing synchronization between security policy and                          firewall service, and disabling auto-save of firewall                          drafts by default.</p>                      </li>                      <li>                        <p><strong>Service Composer — Status Alarms</strong>:                          Raises system alarm if security policy is out-of-sync,                          and takes specific actions based on alarm code to                          resolve issue.</p>                      </li>                      <li>                        <p><strong>Reduction in firewall heap memory usage</strong>:                          Firewall usage of IP address sets has been optimized                          to reduce heap memory usage. </p>                      </li>                    </ul>                    <li>                      <p><strong>Operations and Troubleshooting</strong></p>                    </li>                    <ul>                      <li>                        <p><strong>NSX Dashboard</strong>: Simplifies                          troubleshooting by providing visibility into the                          overall health of NSX components in one central view.                        </p>                      </li>                      <li>                        <p><strong>Traceflow Enhancement — Network Introspection                            Services</strong>: Enhances ability to trace a                          packet from source to destination, by identifying                          whether packets were forwarded to 3rd-party network                          introspection services, and whether the packet comes                          back from the 3rd-party service VM or not.</p>                      </li>                      <li>                        <p><strong>SNMP Support</strong>: Configure SNMP traps                          for events from NSX Manager, NSX Controller, and Edge.</p>                      </li>                      <li>                        <p> <strong>Logging is now enabled by default</strong>                          for SSL VPN and L2 VPN. The default log level is                          notice. </p>                      </li>                      <li>                        <p> <strong>Logging for IPsec VPN is now enabled by                            default</strong> Default log level is set to                          warning. If you wish to disable logging or change the                          log level, see the section, <em>"Enable Logging for                            IPSec VPN"</em> in the NSX Administration Guide. </p>                      </li>                      <li>                        <p> <strong>Firewall rules UI</strong> now displays                          configured IP protocols and TCP/UDP port numbers                          associated with services. </p>                      </li>                      <li>                        <p> <strong>NSX Edge technical support logs</strong>                          have been enhanced to report memory consumption per                          process. </p>                        <p> </p>                      </li>                      <li><strong>Enhanced communication channel health status                          monitoring</strong> with new event log messages                        reported when the channel health status changes for a                        server or a cluster. </li>                      <p></p>                      <li>                        <p><strong>Central CLI Enhancements</strong></p>                      </li>                      <ul>                        <li>                          <p><strong>Central CLI for Host Health</strong>: Shows                            host health status, with 30+ checks in one command                            (including network config, VXLAN config, resource                            utilization, etc.)</p>                        </li>                        <li>                          <p><strong>Central CLI for Packet Capture</strong>:                            Provides ability to capture packet on the host and                            transfer the capture file to user’s remote server.                            This eliminates the need to open up hypervisor                            access to network administrators, when                            troubleshooting logical network issues. </p>                        </li>                      </ul>                      <li>                        <p><strong>Technical support bundle per host</strong>:                          Gathers per-host logs and creates a bundle that can be                          saved and submitted to VMware technical support for                          assistance.</p>                      </li>                    </ul>                    <li>                      <p><strong>Licensing Enhancements</strong></p>                    </li>                    <ul>                      <li>                        <p><strong>Change in default license &amp; evaluation                            key distribution</strong>: default license upon                          install is "NSX for vShield Endpoint", which enables                          use of NSX for deploying and managing vShield Endpoint                          for anti-virus offload capability only. Evaluation                          license keys can be requested through VMware sales.</p>                      </li>                      <li>                        <p><strong>License usage reporting</strong>: NSX license                          usage counts are displayed on NSX Manager's Summary UI                          and also retrievable via API. NSX license usage counts                          will no longer be reported through vCenter licensing                          service.</p>                      </li>                    </ul>                    <li>                      <p><strong>Load Balancer (LB) Enhancements</strong></p>                    </li>                    <ul>                      <li>                        <p><strong>Configurable session timeout for VIP                            configured without acceleration</strong>: Ability to                          configure Load Balancing L7 engine (no acceleration)                          VIP timeout above 5 minutes using the Application Rule                          "timeout client 3600s".</p>                      </li>                      <li>                        <p><strong>Statistics enhancement on CLI</strong>:                          Global statistics are now available through CLI.                          Specific VIP and pool statistics are also available.</p>                      </li>                      <li>                        <p><strong>LB with acceleration enhancement </strong>:                          Load Balancing L4 engine (acceleration enabled) will                          now always honor the health checks UDP, TCP source IP                          hash, and invalidate persistent entry.</p>                      </li>                      <li>                        <p><strong>Log refinements</strong>: Load Balancer log                          improvements.</p>                      </li>                      <li>                        <p><strong>Configurable SSL authentication</strong>:                          Ability to configure SSL server authentication in case                          of VIP with end-to-end SSL.</p>                      </li>                      <li>                        <p><strong>Source IP Persistent table enhancement</strong>:                          Even after a configuration change, the Source IP                          Persistence table remains available.</p>                      </li>                      <li>                        <p><strong>NSX Edge load balancer system control                            (sysctl) sysctl.net.ipv4.vs.expire_nodest_conn                            parameter added to NSX Manager whitelist</strong>:                          The sysctl.net.ipv4.vs.expire_nodest_conn to change                          the persistent connection status.</p>                      </li>                    </ul>                    <li>                      <p><strong>Solution Interoperability</strong></p>                    </li>                    <ul>                      <!-- DOCNOTES: 1658021  6.2.3 -->                      <li>                        <p><strong>Customer Experience Improvement Program</strong>:                          NSX supports reporting system statistics via the                          VMware Customer Experience Improvement Program (CEIP).                          Participation is optional and is configured in the                          vSphere Web Client.</p>                      </li>                      <li>                        <p><strong>VMware vRealize Log Insight 3.3.2 for NSX</strong>                          provides intelligent log analytics for NSX, with                          monitoring and troubleshooting capabilities and                          customizable dashboards for network virtualization,                          flow analysis and alerts. This version accepts NSX                          Standard/Advanced/Enterprise edition license keys                          issued for NSX 6.2.2+.​</p>                      </li>                      <li> <strong>vShield Endpoint Management Support</strong>:                        NSX supports management of vShield Endpoint anti-virus                        offload capabilitie. Customers who purchased vSphere                        with vShield Endpoint (Essentials Plus and above) can                        download NSX from the vSphere download site. For more                        information, refer to <a target="_blank" href="http://kb.vmware.com/kb/2110078">VMware                          knowledge base article 2110078</a> and <a target="_blank"                          href="http://kb.vmware.com/kb/2105558">VMware                          knowledge base article 2105558</a>. </li>                    </ul>                  </ul>                  <!--==================================New in 6.2.2==============================================================--->                  <!--==================================New in 6.2.2==============================================================--->                  <!--==================================New in 6.2.2==============================================================--->                  <a name="newin622"></a>                  <h3>New in 6.2.2</h3>                  <p>The 6.2.2 release delivers a security patch to address the                    glibc                    vulnerability and includes a number of bug fixes documented                    in                    the <a href="#resolvedissues">Resolved Issues</a> section.                    This release includes the same critical bug fixes that were                    provided                    in all the 6.1.4-based and 6.1.5-based patches. For NSX                    6.1.x users,                    this same set of patch fixes is available in the NSX 6.1.6                    release.                  </p>                  <p>The main features of this release are:</p>                  <ul>                    <li>                      <p><strong>CVE-2015-7547 (glibc) security patch</strong>:                        This patch                        addresses <a target="_blank" href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2015-7547">CVE-2015-7547</a>,                        also known as the glibc vulnerability.</p>                    </li>                    <!-- DOCNOTES: 1600484 -->                    <li>                      <p><strong>Issue 1600484: Removal of constraint                          validations on DHCP domain name                          configurations</strong> NSX 6.2.2 re-enables support                        for DHCP pools                        with ".local" domains. See <a target="_blank" href="http://kb.vmware.com/kb/2144097">VMware                          knowledge base article 2144097</a>.                      </p>                    </li>                    <!-- DOCNOTES: 1586149 -->                    <li>                      <p><strong>Issue 1586149: DFW UI Enhancements for better                          user experience.</strong>                        In the previous implementation, the table used to scroll                        the grid to                        the very first item of grid when a user made a change.                        In the fixed                        implementation, whenever a rule is added, the grid                        scrolls to the newly                        added rule. Now, when refreshing the grid data for any                        reason (e.g. after                        publishing or reverting changes), the vertical scroll                        position of                        the grid is maintained. <br />                      </p>                    </li>                    <!-- DOCNOTES: 1601304 -->                    <li>                      <p><strong>Issue 1592562: Behavior change when a new Edge                          Service is configured.</strong>                        Prior to 6.2.2, when a new Edge Service is configured,                        it is <strong>enabled</strong> by default. In 6.2.2,                        this behavior has changed.                        Now, if the current license supports the feature,                        then by default the feature is <strong>enabled</strong>.                        Otherwise, the feature is <strong>disabled</strong>. <br />                      </p>                    </li>                  </ul>                  <a name="newin621"></a>                  <h3>New in 6.2.1</h3>                  <p>The 6.2.1 release delivers a number of bug fixes that have                    been                    documented in the <a href="#resolvedissues">Resolved Issues</a>                    section.                  </p>                  <ul>                    <li>                      <p><strong>6.1.5 fixes</strong>: Release includes the same                        critical fixes as NSX-vSphere 6.1.5 content.</p>                    </li>                    <li>                      <p><strong>Introduced new 'show control-cluster network                          ipsec status' command</strong> that allows uses to                        inspect the Internet Protocol Security (IPsec) state.                      </p>                    </li>                    <li>                      <p><strong> Connectivity status</strong>: NSX Manager user                        interface now shows the connectivity status of the NSX                        Controller cluster.                      </p>                    </li>                    <li>                      <p><strong>Support for vRealize Orchestrator Plug-in for                          NSX 1.0.3</strong>:                        With NSX 6.2.1 release, NSX-vRO plugin version 1.0.3 is                        introduced for                        use with vRealize Automation 7.0.0. This plugin includes                        fixes that                        improve performance when vRealize Automation 7.0 uses                        NSX for vSphere                        6.2.1 as a networking and security end point.                      </p>                    </li>                    <li>                      <p><strong>Starting in 6.2.1, NSX Manager queries each                          Controller node in the                          cluster to get the connection information between that                          controller and                          the other controllers in the cluster.</strong><br />                        This is provided in the output of the NSX REST API ("GET                        https://[NSX-MANAGER-IP-ADDRESS]/api/2.0/vdn/controller"                        command),                        which now shows the peer connection status among the                        controller nodes. If NSX Manager finds the connection                        between any two                        controller nodes is broken, a system event is generated                        to alert the                        user. <br />                      </p>                    </li>                    <li>                      <p><strong>Service Composer now exposes an API that                          enables users                          to configure auto creation of Firewall drafts for                          Service Composer                          workflows.</strong><br />                        This setting can be turned on/off using REST API and the                        changes can                        be saved across reboot. When disabled, no draft is                        created in the                        Distributed Firewall (DFW) for policy workflows. This                        limits the                        number of drafts that are auto-created in the system and                        provides                        better performance. <br />                      </p>                    </li>                  </ul>                  <a name="newin620"></a>                  <h3>New in 6.2.0</h3>                  <p>NSX vSphere 6.2.0 included the following new and changed                    features:</p>                  <ul>                    <li>                      <p><strong>Cross vCenter Networking and Security</strong>                      </p>                      <ul>                        <li>                          <p><strong>NSX 6.2 with vSphere 6.0 supports Cross                              vCenter NSX</strong>                            where logical switches (LS), distributed logical                            routers (DLR) and                            distributed firewalls (DFW) can be deployed across                            multiple vCenters,                            thereby enabling logical networking and security for                            applications with                            workloads (VMs) that span multiple vCenters or                            multiple physical                            locations.</p>                        </li>                        <li>                          <p><strong>Consistent firewall policy across multiple                              vCenters</strong>: Firewall Rule Sections in NSX                            can now be marked as                            "Universal" whereby the rules defined in these                            sections get replicated                            across multiple NSX managers. This simplifies the                            workflows involving                            defining consistent firewall policy spanning                            multiple NSX                            installations</p>                        </li>                        <li>                          <p><strong>Cross vCenter vMotion with DFW</strong>:                            Virtual Machines that have                            policies defined in the "Universal" sections can be                            moved across hosts                            that belong to different vCenters with consistent                            security policy                            enforcement.</p>                        </li>                        <li>                          <p><strong>Universal Security Groups</strong>:                            Security Groups in NSX 6.2 that are                            based on IP Address, IP Set, MAC Address and MAC Set                            can now be used                            in Universal rules, whereby the groups and group                            memberships are synced                            up across multiple NSX managers. This improves the                            consistency in                            object group definitions across multiple NSX                            managers, and enables                            consistent policy enforcement</p>                        </li>                        <li>                          <p><strong>Universal Logical Switch (ULS)</strong>:                            This new functionality                            introduced in NSX 6.2 as a part of Cross vCenter NSX                            allows creation                            of logical switches that can span multiple vCenters,                            allowing the                            network administrator to create a contiguous L2                            domain for an                            application or tenant.</p>                        </li>                        <li>                          <p><strong>Universal Distributed Logical Router (UDLR)</strong>:                            This new functionality introduced in NSX 6.2 as a                            part of Cross                            vCenter NSX allows creation of distributed logical                            routers that can                            span multiple vCenters. The universal distributed                            logical routers                            enable routing across the universal logical switches                            described                            earlier. In addition, NSX UDLR is capable of                            localized north-south                            routing based on the physical location of the                            workloads.</p>                        </li>                      </ul>                    </li>                  </ul>                  <ul>                    <li>                      <p><strong>Operations and Troubleshooting Enhancements</strong></p>                      <ul>                        <li>                          <p><strong>New traceflow troubleshooting tool</strong>:                            Traceflow is a                            troubleshooting tool that helps identify if the                            problem is in the                            virtual or physical network. It provides the ability                            to trace a packet                            from source to destination and helps observe how                            that packet passes                            through the various network functions in the virtual                            network.</p>                        </li>                        <li>                          <p><strong>Flow monitoring and IPFIX separation</strong>:                            In NSX                            6.1.x, NSX supported IPFIX reporting, but IPFIX                            reporting could be                            enabled only if flow reporting to NSX Manager was                            also enabled.                            Starting in NSX 6.2.0, these features are decoupled.                            In NSX 6.2.0 and                            later, you can enable IPFIX independent of flow                            monitoring on NSX                            Manager.</p>                        </li>                        <li>                          <p><strong>New CLI monitoring and troubleshooting                              commands in                              6.2</strong>: See <a target="_blank" href="http://kb.vmware.com/kb/2129062">knowledge                              base article 2129062</a> for more                            information.</p>                        </li>                        <li>                          <p><strong>Central CLI</strong>: Central CLI reduces                            troubleshooting time for distributed network                            functions. Commands are run from the command line on                            NSX Manager and retrieve information from                            controllers, hosts, and the NSX Manager. This allows                            you to quickly access and compare information                            from multiple sources. The central CLI provides                            information about logical switches, logical routers,                            distributed firewall and edges.</p>                        </li>                        <li>                          <p><strong>CLI ping command adds configurable packet                              size and do-not-fragment flag</strong>:                            Starting in NSX 6.2.0, the NSX CLI 'ping' command                            offers options to                            specify the data packet size (not including the ICMP                            header) and to                            set the do-not-fragment flag. See the <a target="_blank"                              href="http://pubs.vmware.com/NSX-62/topic/com.vmware.ICbase/PDF/nsx_62_cli.pdf">NSX                              CLI Reference</a> for details.</p>                        </li>                        <li>                          <p><strong>Show health of the communication channels</strong>:                            NSX 6.2.0 adds the ability to monitor communication                            channel health.                            The channel health status between NSX Manager and                            the firewall agent,                            between NSX Manager and the control plane agent, and                            between host and                            the NSX Controller can be seen from the NSX Manager                            UI. In addition,                            the host command channel offers greater fault                            tolerance.</p>                        </li>                        <li>                          <p><strong>Standalone Edge L2 VPN client CLI</strong>:                            Prior to NSX 6.2, a standalone NSX Edge L2 VPN                            client could be configured only by 'deploy OVF'                            settings provided to the virtual center. Commands                            specific to standalone NSX Edge have been added                            to allow configuration using the command line                            interface.</p>                        </li>                      </ul>                    </li>                  </ul>                  <ul>                    <li>                      <p><strong>Logical Networking and Routing</strong></p>                    </li>                    <ul>                      <li>                        <p><strong>L2 Bridging Interoperability with Distributed                            Logical                            Router</strong>: With VMware NSX for vSphere 6.2, L2                          bridging can now                          participate in distributed logical routing. The VXLAN                          network to which                          the bridge instance is connected, will be used to                          connect the routing                          instance and the bridge instance together.</p>                      </li>                      <li>                        <p><strong>Support of /31 prefixes on ESG and DLR                            interfaces per RFC 3021.</strong>                        </p>                      </li>                      <li>                        <p><strong>Enhanced support of relayed DHCP request on                            the ESG DHCP server.</strong>                        </p>                      </li>                      <li>                        <p><strong>Ability to preserve VLAN IDs/headers inside                            NSX virtual networks.</strong>                        </p>                      </li>                      <li>                        <p><strong><!--PR: 1501875-->Exact Match for                            redistribution                            filters</strong>: The redistribution filter has same                          matching                          algorithm as ACL, so exact prefix match by default                          (except if <code>le</code> or <code>ge</code>                          options are used).</p>                      </li>                      <li>                        <p><strong>Support of administrative distance for static                            route.</strong>                        </p>                      </li>                      <li>                        <p><strong>Ability to enable, relax, or disable check                            per interface on Edge.</strong>                        </p>                      </li>                      <li>                        <p><strong>Display AS path in CLI command <tt>show ip                              bgp</tt></strong>                        </p>                      </li>                      <li>                        <p><strong>HA interface exclusion</strong> from                          redistribution into routing                          protocols on the DLR control VM.</p>                      </li>                      <li>                        <p><strong>Distributed logical router (DLR) force-sync                            avoids data loss for east-west routing traffic                            across the DLR.</strong> North-south routing and                          bridging may continue experience an interruption.<!--(1498358)--></p>                      </li>                      <li>                        <p><strong>View active Edge in HA pair</strong>: In the                          NSX 6.2                          web client, you can find out if an NSX Edge appliance                          is the active or                          backup in an HA pair.</p>                      </li>                      <li>                        <p><strong>REST API supports reverse path filter                            (rp_filter) on Edge</strong>: Using the system                          control REST API, rp_filter sysctl can be configured                          through UI, and is also exposed through REST API for                          vNIC interfaces and sub-interfaces. See the <a target="_blank"                            href="http://pubs.vmware.com/NSX-62/topic/com.vmware.ICbase/PDF/nsx_62_api.pdf"><em>NSX                              API documentation</em></a> for more information.</p>                      </li>                      <li>                        <p><strong><!--Doc 1489707-->Behavior of the IP prefix <tt>GE</tt>                            and IP prefix <tt>LE</tt> BGP route filters</strong>:                          In NSX 6.2, the following enhancements have been made                          to BGP route filters:</p>                      </li>                      <ul>                        <li>                          <p>LE / GE keywords not allowed: For the null route                            network address (defined as ANY or in CIDR format                            0.0.0.0/0),                            less-than-or-equal-to (LE) and                            greater-than-or-equal-to (GE) keywords                            are no longer allowed. In previous releases, these                            keywords were                            allowed.</p>                        </li>                        <li>                          <p>LE and GE values in the range 0-7 are now treated                            as valid. In                            previous releases, this range was not valid.</p>                        </li>                        <li>                          <p>For a given route prefix, you can no longer specify                            a GE value                            that is greater than the specified LE value.</p>                        </li>                      </ul>                    </ul>                  </ul>                  <ul>                    <li>                      <p><strong>Networking and Edge Services</strong></p>                      <ul>                        <li>                          <p><strong>The management interface of the DLR has                              been renamed to                              HA interface</strong>. This has been done to                            highlight the fact that                            the HA keepalives travel through this interface and                            that interruptions                            in traffic on this interface can result in a                            split-brain                            condition.</p>                        </li>                        <li>                          <p><strong>Load balancer health monitoring                              improvements</strong>:                            Delivers granular health monitoring that reports                            information on                            failures, keeps track of last health check and                            status change, and                            reports failure reasons.</p>                        </li>                        <li>                          <p><strong>Support VIP and pool port range</strong>:                            Enables load balancer                            support for applications that require a range of                            ports.</p>                        </li>                        <li>                          <p><strong>Increased maximum number of virtual IP                              addresses                              (VIPs)</strong>: VIP support rises to 1024.</p>                        </li>                      </ul>                    </li>                  </ul>                  <ul>                    <li>                      <p><strong>Security Service Enhancements</strong></p>                      <ul>                        <li>                          <p><strong>New IP address discovery mechanisms for VMs</strong>:                            Authoritative enforcement of security policies based                            on VM names or                            other vCenter-based attributes requires that NSX                            know the IP address                            of the VM. In NSX 6.1 and earlier, IP address                            discovery for each VM                            relied on the presence of VMware Tools (vmtools) on                            that VM or the                            manual authorization of the IP address for that VM.                            NSX 6.2 introduces the option to discover the VM's                            IP address by doing discovery from the hypervisor.                            These new discovery mechanisms enable NSX to enforce                            object based distributed firewall rules on VMs that                            do not have VMware Tools installed.</p>                        </li>                      </ul>                    </li>                  </ul>                  <ul>                    <li>                      <p><strong>Solution Interoperability</strong></p>                    </li>                    <ul>                      <li>                        <p><strong>Support for vSphere 6.0 Platform Services                            Controller                            topologies</strong>: NSX now supports external                          Platform Services                          Controllers (PSC), in addition to the already                          supported embedded PSC                          configurations.</p>                      </li>                      <li>                        <p><strong>Support for vRealize Orchestrator Plug-in for                            NSX</strong>: NSX 6.2 supports the                          <a target="_blank" href="http://pubs.vmware.com/Release_Notes/en/nsx/suite/releasenotes_nsx_vro_103.html">NSX-vRO                            plug-in</a>                          for integration of NSX with vRealize Orchestrator.                        </p>                      </li>                    </ul>                  </ul>                  <a name="sysreqs"></a>                  <h2>Recommended Versions, System Requirements and Installation</h2>                  <h3><a name="reqssysreqs"></a>Recommended Versions and System                    Requirements</h3>                  <p>The table below lists recommended and required versions of                    VMware                    software. This information is current as of the publication                    date of                    this document. For the latest recommendations, please refer                    to                    <a target="_blank" href="http://kb.vmware.com/kb/2144295">VMware
knowledge                      base article 2144295</a></p>                  <table width="100%" cellspacing="0" cellpadding="10" border="0">                    <tbody>                      <tr bgcolor="#4BA142" style="color:white">                        <td style="width:200px">Product or component</td>                        <td>Minimum recommended version</td>                      </tr>                      <tr bgcolor="#D7D7D7">                        <td>NSX for vSphere</td>                        <td>6.2.2                          <p>Note: There is a known issue with SSL VPN. For more                            information, see CVE-2016-2079. Customers                            running 6.2.2 or earlier are strongly advised to                            contact VMware Support to request immediate                            assistance.                          </p>                          <p>To contact VMware support, see                             <a href="https://kb.vmware.com/kb/2006985">How to                              file a Support Request in My VMware</a>                             or <a href="https://www.vmware.com/support/file-sr/">How                              to Submit a Support Request</a>.                          </p>                        </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td>vSphere</td>                        <td>5.5U3, or 6.0U2                          <p>Note: There is a known issue with vSphere 6.0 and                            NSX objects. For more information, see                            <a href="https://kb.vmware.com/kb/2144605">VMware                              Knowledge base article 2144605</a>,                            "Duplicate VTEPs in ESXi hosts after rebooting                            vCenter Server".</p>                        </td>                      </tr>                      <tr bgcolor="#D7D7D7">                        <td valign="top">Guest Introspection</td>                        <td valign="top">                          Guest Introspection-based features in NSX are                          compatible with specific                          VMware Tools (VMTools) versions. To enable the                          optional Thin Agent                          Network Introspection Driver component packaged with                          VMware Tools, you                          must upgrade to one of:                          <ul>                            <li>                              <p>VMware Tools 10.0.8 and later to resolve Slow                                VMs after upgrading VMware Tools in NSX / vCloud                                Networking and Security (VMware knowledge base                                article <a target="_blank" href="http://kb.vmware.com/kb/2144236"><em>                                    2144236</em></a>)</p>                            </li>                            <li>                              <p>VMware Tools 10.0.9 and later for Windows 10                                support</p>                            </li>                          </ul>                        </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td>vRealize Orchestrator</td>                        <td>NSX-vRO plugin 1.0.3 or later</td>                      </tr>                    </tbody>                  </table>                  <h3><a name="reqsinstall"></a>Installation</h3>                  <p>                    For installation instructions, see the                    <a target="_blank" href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html"><em>NSX
Installation                        Guide</em></a> or the                    <a target="_blank" href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx-cross-vcenter-install.doc/GUID-803A5A77-74A0-483C-97C9-5B66A0B02240.html"><em>NSX
Cross-vCenter                        Installation Guide</em></a>. For the complete list                    of NSX installation prerequisites, see the                    <a target="_blank" href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-311BBB9F-32CC-4633-9F91-26A39296381A.html">System
Requirements                      for NSX</a> section in the <em>NSX Installation Guide</em>.</p>                  <a name="deprecs"></a>                  <h2>Deprecated and Discontinued Functionality</h2>                  <h3><a name="reqeol"></a>End of Life and End of Support                    Warnings</h3>                  <p>For information about NSX and other VMware products that                    must be upgraded soon, please consult the                    <a target="_blank" href="https://www.vmware.com/files/pdf/support/Product-Lifecycle-Matrix.pdf"><em>VMware                        Lifecycle Product Matrix</em></a>. Upcoming                    end-of-support dates include:</p>                  <ul>                    <li>vCloud Networking and Security will reach End of                      Availability (EOA) and End of General Support (EOGS) on                      September 19, 2016. (See also <a target="_blank" href="http://kb.vmware.com/kb/2144733"><em>VMware                          knowledge base article 2144733</em></a>.) (See also <a                        target="_blank"                        href="http://kb.vmware.com/kb/2144620"><em>VMware                          knowledge base article 2144620</em></a>.) </li>                    <li>NSX for vSphere 6.1.x will reach End of Availability                      (EOA) and End of General Support (EOGS) on January 15,                      2017. (See also <a target="_blank" href="http://kb.vmware.com/kb/2144769"><em>VMware                          knowledge base article 2144769</em></a>.) </li>                    <li>                      <p>As of NSX 6.2.3, the NSX Data Security feature has been                        deprecated. In NSX 6.2.3, you can continue to use this                        feature at your discretion, but be aware that this                        feature will be removed from NSX in a future release.</p>                    </li>                    <p></p>                    <li> Web Access Terminal (WAT) is being deprecated and will                      not be included in a future maintenance release. VMware                      recommends using the full access client with SSL VPN                      deployments for improved security.</li>                  </ul>                  <!-- DOCNOTES:  issue 1621019 6.2.3-->                  <h3>Unsupported controller commands are no longer shown</h3>                  <p>Please review the CLI guide for the complete list of                    supported commands. You should only use commands which are                    documented in this guide. The join control-cluster command                    is not a supported command on NSX for vSphere. See also <a                      target="_blank"                      href="http://kb.vmware.com/kb/2135280"><em>VMware                        knowledge base article 2135280.</em></a> </p>                  <h3><a name="tls010"></a>TLS 1.0 support has been deprecated                    as of NSX 6.2.3</h3>                  <p> In the NSX VPN and IPsec cipher suite, TLS 1.0 support has                    been deprecated as of NSX 6.2.3. There have been some                    changes in Cipher support as compared to the previous                    release. These changes are captured in the tables below. </p>                  <p><strong>SSLVPN Cipher suite support: Changes in 6.2.3</strong>                  </p>                  <table cellspacing="0" cellpadding="10" border="0">                    <tbody>                      <tr bgcolor="#4BA142" style="color:white">                        <td> 6.2.2 </td>                        <td> 6.2.3 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> TLS_RSA_WITH_AES_128_CBC_SHA </td>                        <td> TLS_RSA_WITH_AES_128_CBC_SHA </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> TLS_RSA_WITH_AES_256_CBC_SHA </td>                        <td> TLS_RSA_WITH_AES_256_CBC_SHA </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> TLS_RSA_WITH_3DES_EDE_CBC_SHA </td>                        <td> TLS_RSA_WITH_AES_128_GCM_SHA256 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td>  </td>                        <td> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td>  </td>                        <td> TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 </td>                      </tr>                    </tbody>                  </table>                  <p><strong>L2VPN Cipher suite support: Changes in 6.2.3</strong></p>                  <table cellspacing="0" cellpadding="10" border="0">                    <tbody>                      <tr bgcolor="#4BA142" style="color:white">                        <td> 6.2.2 </td>                        <td> 6.2.3 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> AES128-SHA </td>                        <td> AES128-GCM-SHA256 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> AES256-SHA </td>                        <td> ECDHE-RSA-AES128-GCM-SHA256 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> AES128-GCM-SHA256 </td>                        <td> ECDHE-RSA-AES256-GCM-SHA384 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> DES-CBC3-SHA </td>                        <td> NULL-SHA256 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> NULL-MD5 </td>                        <td> NULL-MD5 </td>                      </tr>                    </tbody>                  </table>                  <p><strong>IP-Sec Cipher suite: Changes in 6.2.3</strong></p>                  <table cellspacing="0" cellpadding="10" border="0">                    <tbody>                      <tr bgcolor="#4BA142" style="color:white">                        <td> 6.2.2 </td>                        <td> 6.2.3 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> AES_128-HMAC_SHA1 </td>                        <td> AES_128-HMAC_SHA1 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> AES(12)_256-SHA1(2)_000 </td>                        <td> AES(12)_256-SHA1(2)_000 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> 3DES(3)_000-SHA1(2)_000 </td>                        <td> 3DES(3)_000-SHA1(2)_000 </td>                      </tr>                      <tr bgcolor="#F4F4F4">                        <td> AES_GCM_C_160-NONE </td>                        <td> AES_GCM_C_160-NONE </td>                      </tr>                    </tbody>                  </table>                  <!--<p>NOTE: AS OF 2016-03-30-22:42 THIS SECTION HAS BEEN UPDATED FOR single-hop upgrades from vCNS.</p>-->                  <!--<p>NOTE: AS OF 2015-08-16-11:55 THIS SECTION HAS BEEN UPDATED FOR 6.2.</p>-->                  <a name="upgradenotes"></a>                  <h2>Upgrade Notes</h2>                  <ul>                    <li>                      <p><strong>Downgrades are not supported:</strong> </p>                      <ul>                        <li>                          <p>Always capture a backup of NSX Manager before                            proceeding with an upgrade. </p>                        </li>                        <li>                          <p>Once NSX has been upgraded successfully, NSX cannot                            be downgraded. </p>                        </li>                      </ul>                    </li>                    <li>                      <p><strong>To upgrade to NSX 6.2.4</strong>, you must                        perform a full NSX upgrade including host cluster                        upgrade (which upgrades the host VIBs to 6.2.4). For                        instructions, see the <a target="_blank" href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html"><em>NSX                            Upgrade Guide</em></a> including the <a target="_blank"                          href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.upgrade.doc/GUID-96DA2DF6-6B93-4FDE-9A52-55619FFB3F1A.html">Upgrade                          Host Clusters to NSX 6.2</a> section. </p>                    </li>                    <li> <strong>Controller disk layout</strong>: New                      installations of NSX 6.2.3 or later will deploy NSX                      Controller appliances with updated disk partitions to                      provide extra cluster resiliency. In previous releases,                      log overflow on the controller disk might impact                      controller stability. In addition to adding log management                      enhancements to prevent overflows, the NSX Controller                      appliance has separate disk partitions for data and logs                      to safeguard against these events. If you upgrade to NSX                      6.2.3 or later, the NSX Controller appliances will retain                      their original disk layout.</li>                    <li>                      <p><strong>Upgrade paths:</strong></p>                      <ul>                        <li>                          <p>Upgrade path from NSX 6.x: The <a target="_blank"                              href="http://partnerweb.vmware.com/comp_guide/sim/interop_matrix.php">VMware                              Product Interoperability Matrix</a> provides                            details about the upgrade paths from VMware NSX.                            Cross-vCenter NSX upgrade is covered in the <a target="_blank"                              href="http://pubs.vmware.com/NSX-62/index.jsp#com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html">NSX                              Upgrade Guide</a>.</p>                        </li>                        <li>                          <p>Upgrade path from vCNS 5.5.x: Using the NSX upgrade                            bundle posted on or after 09 June, 2016, you may                            upgrade directly from VMware vCloud Network and                            Security (vCNS) 5.5.x to NSX 6.2.4. For                            instructions, see the <em>NSX Upgrade Guide</em>,                            in the section, <a target="_blank" href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.upgrade.doc/GUID-D2CDB014-39D8-48CC-9733-981308249F52.html">                              vCloud Networking and Security to NSX Upgrade</a>.                            This section also includes instructions for                            upgrading vCNS 5.5.x to NSX in a vCloud Director                            environment. A separate guide, the <a target="_blank"                              href="http://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.upgrade.endpoint.doc%2FGUID-EC5B912F-0E12-4032-9ED0-113DE5C6850C.html">                              NSX Upgrade Guide for vShield Endpoint</a>,                            includes instructions for upgrading vCNS 5.5.x to                            NSX 6.2.4 if you are using vShield Endpoint for                            anti-virus protection only. </p>                        </li>                        <li>                          <p><!--Doc PR: 1661729-->There is no support for                            upgrades from NSX 6.1.6 to NSX 6.2.0, 6.2.1, or                            6.2.2.</p>                        </li>                        <li>                          <p><!--Doc PR: 1509725-->There is no support for                            upgrades from NSX 6.1.5 to NSX 6.2.0. VMware                            recommends upgrading from 6.1.5 to 6.2.4 or later to                            get the latest security updates.</p>                        </li>                      </ul>                    </li>                    <li>                      <p><strong>To validate</strong> that your upgrade to NSX                        6.2.x was successful see <a target="_blank" href="http://kb.vmware.com/kb/2134525">knowledge                          base article 2134525</a>.</p>                    </li>                    <li>                      <p><strong>Upgrading as part of a wider VMware product                          upgrade</strong>: When you are upgrading NSX in                        context with other                        VMware product upgrades, such as vCenter and ESXi, it is                        important to                        follow the supported upgrade sequence documented in                        <a target="_blank" href="http://kb.vmware.com/kb/2109760">knowledge                          base article 2109760</a>.</p>                    </li>                    <li>                      <strong> Partner services compatibility</strong>: If your                      site uses VMware partner services for guest introspection                      or network                      introspection, you must review the                       <a target="_blank" href="http://www.vmware.com/resources/compatibility/search.php?deviceCategory=security">VMware
Compatibility                        Guide</a> before you upgrade, to verify that your                      vendor's service is compatible with this release of NSX.                    </li>                    <li>                      <p><strong>Known issues affecting upgrades</strong>:                        See the section, <a href="#InstallUpgradeissues">Installation
and                          Upgrade Known Issues</a>, later in this document, for                        a list of                        known upgrade-related issues.</p>                    </li>                    <li>                      <p><!--Doc PR: 1495916/1495916--><strong>New system                          requirements</strong>: For the memory and CPU                        requirements while installing                        and upgrading NSX Manager, see the                        <a target="_blank" href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-311BBB9F-32CC-4633-9F91-26A39296381A.html">System
Requirements                          for NSX</a> section in the NSX 6.2 documentation.</p>                    </li>                    <li>                      <p><!--Doc PR: 1466854 6.2.1--><strong>Maximum number of                          NAT                          rules</strong>: For NSX Edge versions prior to 6.2, a                        user could configure 2048 SNAT and 2048 DNAT rules                        separately, giving a total limit of 4096 rules.                        Since NSX Edge version 6.2 onwards, a limit is enforced                        for the maximum allowed NAT rules, based on the NSX Edge                        appliance size:                      </p>                      <ul>                        <p>1024 SNAT and 1024 DNAT rules for a total limit of                          2048 rules for COMPACT edge. </p>                      </ul>                      <p>                      </p>                      <ul>                        <p>2048 SNAT and 2048 DNAT for a total limit of 4096                          rules for LARGE edge and QUADLARGE edge. </p>                      </ul>                      <p>                      </p>                      <ul>                        <p>4096 SNAT and 4096 DNAT rules for a total limit of                          8192 rules for XLARGE edge.</p>                      </ul>                      <p></p>                      <p>During an NSX Edge upgrade to version 6.2, any existing                        COMPACT edge                        whose total NAT rules (sum of SNAT and DNAT) exceeds the                        limit 2048                        will fail validation, resulting in an upgrade failure.                        In this scenario,                        the user will need to change the appliance size to                        LARGE, QUADLARGE                        and retry the upgrade.</p>                    </li>                    <li>                      <p><strong><!--Doc 1490557-->Behavior change in                          redistribution                          filters</strong> on distributed logical router and                        Edge Services                        Gateway: Starting in the 6.2 release, redistribution                        rules in the DLR                        and ESG work as ACLs only. That is, if a rule is an                        exact match, the                        respective action is taken.                      </p>                    </li>                    <li>                      <p><strong>VXLAN tunnel ID</strong>: Before upgrading to                        NSX                        6.2.x, you must make sure your installation is not using                        a VXLAN                        tunnel ID of 4094 on any tunnels. VXLAN tunnel ID 4094                        no longer                        available for use. To assess and address this follow                        these steps:                      </p>                      <ol>                        <li>In vCenter, navigate to <strong>Home</strong> &gt;                          <strong>Networking                            and Security</strong> &gt; <strong>Installation</strong>                          and select                          the <strong>Host Preparation</strong> tab.                        </li>                        <li>                          <p>Click <strong>Configure</strong> in the VXLAN                            column.</p>                        </li>                        <li>                          <p>In the Configure VXLAN Networking window, set the                            VLAN ID to a value between 1 and 4093.                          </p>                        </li>                      </ol>                      <p></p>                    </li>                    <li>                      <p><strong>Reset vSphere web client</strong>: After                        upgrading NSX                        Manager, you must reset the vSphere web client server as                        explained in                        the <a target="_blank" href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.upgrade.doc/GUID-E8BA0AB5-9948-40B3-8234-31B5105154FB.html"><em>NSX                            Upgrade documentation</em></a>. Until you do                        this, the <strong>Networking and Security</strong> tab                        may fail to                        appear in the vSphere web client. You also may need to                        clear your browser cache or history.</p>                    </li>                    <li>                      <p><strong>Stateless environments</strong>: NSX upgrades                        in a                        stateless host environment use new VIB URLs: In NSX                        upgrades in a                        stateless host environment, the new VIBs are pre-added                        to the Host                        Image profile during the NSX upgrade process. As a                        result, NSX on                        stateless hosts upgrade process follows this sequence:</p>                      <p>                      </p>                      <ol style="margin-left: 10px;">                        <li>                          <p>Manually download the latest NSX VIBs from NSX                            Manager from a fixed URL.</p>                        </li>                        <li>                          <p>Add the VIBs to the host image profile.</p>                        </li>                      </ol>                      <p></p>                      <p>Prior to NSX 6.2.0, there was a single URL on NSX                        Manager from                        which VIBs for a certain version of the ESX Host could                        be found.                        (Meaning the administrator only needed to know a single                        URL,                        regardless of NSX version.) In NSX 6.2.0 and later, the                        new NSX VIBs                        are available at different URLs. To find the correct                        VIBs, you must                        perform the following steps:</p>                      <p>                      </p>                      <ul>                        <li>Find the new VIB URL from <tt>https://&lt;NSX-Manager-IP&gt;/bin/vdn/nwfabric.properties</tt>.</li>                        <li>Fetch VIBs of required ESX host version from                          corresponding URL.</li>                        <li>Add them to host image profile.</li>                      </ul>                    </li>                    <p></p>                    <!-- doc PR 1673045 6.2.3-->                    <li>                      <p> <strong>Autosaved drafts and Service Composer</strong>:                        In NSX 6.2.3 and later, the default for autosaved drafts                        is OFF. This setting governs the automatic saving of                        firewall rules for NSX distributed firewall. Manually                        configured settings are maintained during the upgrade.                        To avoid performance issues, VMware recommends that you                        disable the autosaved drafts feature. You can use the                        following API call to change the autodraft setting for                        NSX distributed firewall: </p>                      <ol>                        <li>Get the existing global firewall configuration                          (GlobalConfiguration):<br />                          <code>GET                            https://NSX-Manager-IP-Address/api/4.0/firewall/config/globalconfiguration</code>                        </li>                        <li>Use a PUT call to set the property <tt>autoDraftDisabled</tt>                          to true in the global configuration:<br />                          <code>PUT                            https://NSX-Manager-IP-Address/api/4.0/firewall/config/globalconfiguration</code><br />                          with a request body that includes:<br />                          <pre>&lt;globalConfiguration&gt;    &lt;layer3RuleOptimize&gt;...&lt;/layer3RuleOptimize&gt;    &lt;layer2RuleOptimize&gt;...&lt;/layer2RuleOptimize&gt;    &lt;tcpStrictOption&gt;...&lt;/tcpStrictOption&gt;    &lt;autoDraftDisabled&gt;true&lt;/autoDraftDisabled&gt;&lt;/globalConfiguration&gt;</pre> <br />                          Note that a GET will not show the autoDraftDisabled                          field. </li>                      </ol>                    </li>                    <!-- doc PR 1670314 6.2.3-->                    <li>                      <p> <strong>Host may become stuck in the installing state</strong>:                        During large NSX upgrades, a host may become stuck in                        the installing state for a long time. This can occur due                        to issues uninstalling old NSX VIBs. In this case the                        EAM thread associated with this host will be reported in                        the VI Client Tasks list as stuck.<br />                        <em>Workaround </em>: Log into vCenter using the VI                        Client. Right click on the stuck EAM task and cancel it.                        From the vSphere Web Client, issue a Resolve on the                        cluster. The stuck host may now show as in progress. Log                        into the host and issue a reboot to force completion of                        the upgrade on that host.                      </p>                    </li>                  </ul>                  <a name="knownissues"></a>                  <h2>Known Issues</h2>                  <p>Known issues are grouped as follows:</p>                  <ul>                    <li><a href="#Generalissues" name="&amp;lpos=apps_scode : 10">General                        Known Issues</a></li>                    <li><a href="#InstallUpgradeissues" name="&amp;lpos=apps_scode : 10">Installation                        and Upgrade Known Issues</a></li>                    <!-- absorb "#NSXendpointissues" Service Deployment Known Issues into 'Install + Upgrade' -->                    <li><a href="#NSXmanagerissues" name="&amp;lpos=apps_scode : 10">NSX                        Manager Known Issues</a></li>                    <!-- <li><a name="&amp;lpos=apps_scode : 13" href="#NSXlogical">Logical Networking Known Issues</a></li> -->                    <!-- was logical switch issues -->                    <li><a href="#NSXnetworkissues" name="&amp;lpos=apps_scode : 10">Logical                        Networking Known Issues and NSX Edge Known Issues</a></li>                    <!-- LB, VPN, DNS, NAT -->                    <!-- was "NSX Edge and Logical Routing Known Issues" -->                    <li><a href="#securityservicesissues" name="&amp;lpos=apps_scode : 10">Security                        Services Known Issues</a></li>                    <!-- FW, DFW, certs, service composer, data security network intro, guest intro -->                    <li><a href="#monitoringissues" name="&amp;lpos=apps_scode : 10">Monitoring                        Services Known Issues</a></li>                    <li><a href="#interopissues" name="&amp;lpos=apps_scode : 13">Solution                        Interoperability Known Issues</a></li>                    <!--<li><a name="&amp;lpos=apps_scode : 13" href="#NSXsiissues">Service Insertion Known Issues</a></li>-->                    <li><a href="#controllerissues" name="&amp;lpos=apps_scode : 10">NSX                        Controller Known Issues</a></li>                    <!--<li><a name="&amp;lpos=apps_scode : 13" href="#controllerissues">Controller Known Issues</a></li>-->                  </ul>                  <!--==================================General Known Issues==============================================================--->                  <!--==================================General Known Issues==============================================================--->                  <!--==================================General Known Issues==============================================================--->                  <p></p>                  <h3><a name="Generalissues"></a>General Known Issues</h3>                  <!-- DOCNOTES: 1734208  6.2.4 -->                  <p><strong>Issue 1708769: Increased latency on SVM (Service                      VM) after snapshot in NSX </strong> </p>                  <p>This issue occurs because running a snapshot of an Service                    VM (SVM) can cause added network latency. Snapshot is                    sometimes invoked by backup applications running in the                    environment. </p>                  <p><em>Workaround</em>: Refer to <a target="_blank" href="https://kb.vmware.com/kb/2146769">VMware                      knowledge base article 2146769</a>.</p>                  <p></p>                  <!-- DOCNOTES: 1716064  6.2.4 wont fix<p><strong>Issue 1661243: Applying a security filter to a host fails on a cluster  with a name having one or more spaces</strong>  <p>The space in the cluster name splits the agent name with space into two agents instead of one,     which results the slowpath connection to fail. </p><p><em>Workaround</em>: Rename the cluster with no space, may be add a '_' to  keep the name consistent with other cluster names.</p></p>-->                  <!-- DOCNOTES: 1702122  6.2.4 -->                  <p><strong>Issue 1700980: For security patch CVE-2016-2775, a                      query name which is too long can                      cause a segmentation fault in lwresd</strong> </p>                  <p>NSX 6.2.4 has BIND 9.10.4 installed with the product, but                    it does not use lwres option in <em>named.conf</em>, hence                    the product is not vulnerable. </p>                  <p><em>Workaround</em>: As the product is not vulnerable, no                    workaround is required.</p>                  <p></p>                  <!-- DOCNOTES: 1720236  6.2.4 -->                  <p><strong>Issue 1718726: Cannot force-sync Service Composer                      after a user has manually deleted the Service Composer's                      policy section using DFW REST API</strong> </p>                  <p> In a cross-vCenter NSX environment, a user's attempt to                    force sync NSX Service Composer configuration will fail if                    there was only one policy section and that policy section                    (the Service Composer-managed policy section) was deleted                    earlier via a REST API call. </p>                  <p><em>Workaround</em>: Do not delete the Service                    Composer-managed policy section via a REST API call. (Note                    that the UI already prevents deletion of this section.)</p>                  <p></p>                  <!-- DOCNOTES: 1698652  6.2.4 -->                  <p><strong>Issue 1685375: Remote MAC is missing from VXLAN                      gateway</strong> </p>                  <p> Remote MAC addresses are not sent after a switch reload.                    In rare circumstances, the NSX controller may not populate                    the ovsdb MAC address tables again,                    when a HW VTEP gateway reboots. </p>                  <p><em>Workaround</em>: You can perform any one of the                    following workaround that will cause the controller to                    populate the ovsdb remote MAC address table again in the HW                    VTEP:                  </p>                  <ol>                    <li>On a VM connected to the VXLAN, reset the appropriate                      network interface with the following commands:<br />                      <ul>                        <li>ifconfig eth1 down</li>                        <li>ifconfig eth1 up</li>                      </ul>                    </li>                    <li>                      From the NSX Manager UI, detach the hardware VXLAN gateway                      port, and attach the port again.</li>                    <p></p>                    <p></p>                  </ol>                  <!-- DOCNOTES:  1715204 6.2.4 -->                  <p><strong>Issue 1710624: Windows 2008 event log server is                      added as "TYPE" of "WIN2K3", if serverType is not                      specified in REST API request body</strong> </p>                  <p>If you create EventLog server API request, the server will                    be added as "TYPE" of "WIN2K3". If you use EventLog server                    only for IDFW, IDFW may not work correctly. </p>                  <p><em>Workaround</em>: Add serverType to REST API request                    body. For example: </p>                  <p>                  </p>                  <pre>&lt;EventlogServer&gt;  &lt;domainId&gt;1&lt;/domainId&gt;  &lt;hostName&gt;AD_server_IP&lt;/hostName&gt;  &lt;enabled&gt;true&lt;/enabled&gt;  &lt;serverType&gt;WIN2k8&lt;/serverType&gt;&lt;/EventlogServer&gt;</pre>                  <p></p>                  <p></p>                  <p></p>                  <!-- DOCNOTES: 1716969  6.2.4 -->                  <p><strong>Issue 1716328: Removing host that is in maintenance                      mode can                      result in later cluster preparation failure</strong> </p>                  <p> If an administrator places an NSX-enabled ESXi host in                    maintenance                    mode and removes it from an NSX-prepared cluster, NSX fails                    to delete                    its record of the ID number of the removed host. After the                    installation is in this state, if there is another host with                    same ID                    in another cluster or if this host is being added to another                    cluster,                    the cluster preparation process will fail for that cluster.                  </p>                  <p><em>Workaround</em>: Restart NSX Manager or run the                    following API to get rid of                    the extra entry. Perform a PUT of the API method,<br />                    <code>https://nsx-manager-address/api/internal/firewall/updatestatus</code>                  </p>                  <p></p>                  <!-- DOCNOTES: 1678883  6.2.3 -->                  <p><strong>Issue 1659043: Service Status for Guest                      Introspection reported as "Not Ready" when NSX Manager to                      USVM communication times out</strong> </p>                  <p> An error message similar to "PLAIN login refused: user                    'usvm-admin-host-14' - invalid credentials" may be reported                    for the Guest Introspection Universal SVM when the expected                    password change process with NSX Manager on the internal                    message bus (rabbit MQ) does not succeed. </p>                  <p><em>Workaround</em>: To re-establish connectivity between                    the USVM and NSX Manager, restart the USVM or manually                    delete it and then select the Resolve button on the Service                    Composer UI to prompt a redeploy of the USVM for the                    affected host only. </p>                  <p></p>                  <!-- DOCNOTES: 1671146 6.2.3 -->                  <p><strong>Issue 1662842: Guest Introspection: Connectivity                      lost between MUX and USVM when trying to resolve                      unresolvable Windows SIDs</strong> <br />                    Guest Introspection service will go into a warning state,                    with each Guest Introspection going in and out of a warning                    state.                    Until the Guest Introspection VM reconnects, network events                    will not be delivered to the NSX Manager.                    This will affect both Activity Monitoring and ID Firewall in                    the case that logon events are detected through the Guest                    Introspection path.                    <br />                  </p>                  <p><em>Workaround</em>: To return Guest Introspection to a                    stable state, Guest Introspection VMs must be configured to                    ignore lookups for these well-known SIDs. This is achieved                    by updating a configuration file on each Guest Introspection                    VM and then restarting the service. In addition, Active                    Directory log scraping can be used as a workaround for                    detecting logon events for ID Firewall.</p>                  Steps to ignore SID lookups for unresolvable SIDs:<br />                  <ol>                    <li>Login to Guest Introspection VM.</li>                    <li>Edit the file at                      /usr/local/usvmmgmt/config/ignore-sids.lst.</li>                    <li>Append the following 2 lines:<br />                      S-1-18-1<br />                      S-1-18-2</li>                    <li>Save and close the file.</li>                    <li>Restart the Guest Introspection service with command:<br />                      rcusvm restart. </li>                    <p></p>                  </ol>                  <!-- DOCNOTES:  1657995 6.2.3 -->                  <p><strong>Issue 1558285: Deleting cluster with Guest                      Introspection from Virtual Center results in null pointer                      exception</strong> <br />                    Services such as Guest Introspection must be removed first                    before a cluster is removed from VC <br />                  </p>                  <p><em>Workaround</em>: Delete the EAM Agency for the service                    deployment with no associated cluster.</p>                  <p></p>                  <!-- DOCNOTES: 1629030 6.2.3 -->                  <p><strong>Issue 1629030: The packet capture central CLI                      (debug packet capture and show packet capture) requires                      vSphere 5.5U3 or higher</strong> <br />                    These commands are not supported on earlier vSphere 5.5                    releases.                    <br />                  </p>                  <p><em>Workaround</em>: VMware advises all NSX customers to                    run vSphere 5.5U3 or higher.</p>                  <p></p>                  <!-- DOCNOTES: 1661793 6.2.3 -->                  <p><strong>Issue 1568180: Feature list incorrect for NSX when                      using vCenter Server Appliance (vCSA) 5.5</strong> <br />                    You can view the features of a license in the vSphere Web                    Client by selecting                    the license and clicking <strong> Actions &gt; View                      Features</strong>.                    If you upgrade to NSX 6.2.3, your license is upgraded to an                    Enterprise license, which enables all features. However, if                    NSX Manager is registered with vCenter Server Appliance                    (vCSA) 5.5, selecting <strong>View Features</strong> will                    display the list of features for the license used before the                    upgrade, not the new Enterprise license.<br />                  </p>                  <p><em>Workaround</em>: All Enterprise licenses have the same                    features, even if they are not                    displayed correctly in the vSphere Web Client.                    See the <a target="_blank" href="http://www.vmware.com/products/nsx/compare.html">NSX                      Licensing Page</a> for more information.                  </p>                  <!-- DOCNOTES: 1537301 6.2.3 -->                  <p><strong>Issue 1477280: Cannot create hardware gateway                      instances when no controller is deployed </strong> <br />                    Controllers must be deployed before any hardware gateway                    instances are configured. If controllers are not deployed                    first, the error message "Failed to do the Operation on the                    Controller" is shown. </p>                  <p><em>Workaround</em>: None.</p>                  <p></p>                  <!-- DOCNOTES: 1491275-->                  <p><strong>Issue 1491275: NSX API returns JSON instead of XML                      in certain circumstances</strong><br />                    On occasion, an API request will result in JSON, not XML,                    being returned to the user.                  </p>                  <p><em>Workaround</em>: Add <tt>Accept: application/xml</tt>                    to the request header.</p>                  <p></p>                  <!--=================================================Installation and Upgrade Issues ====================================================== -->                  <!--=================================================Installation and Upgrade Issues ====================================================== -->                  <!--=================================================Installation and Upgrade Issues ====================================================== -->                  <!--Installation and Upgrade Issues-->                  <h3><a name="InstallUpgradeissues"></a>Installation and                    Upgrade Known Issues</h3>                  <p>Before upgrading, please read the section <a href="#upgradenotes">Upgrade                      Notes</a>, earlier in this document.</p>                  <!-- DOCNOTES: 1730017 6.2.4 -->                  <p><strong>Issue 1730017: Upgrades from 6.2.3 to 6.2.4 do not                      show a version change for Guest Introspection</strong>                  </p>                  <p>As the 6.2.3 Guest Introspection module is the latest                    version available, the version after a 6.2.4 upgrade remains                    unchanged. Note that upgrades from earlier NSX releases may                    show a version change to 6.2.4 </p>                  <p><em>Workaround</em>: This does not affect any                    functionality.</p>                  <!-- DOCNOTES: 1691897 6.2.3 -->                  <p><strong>Issue 1685894: VMs migrated (with DRS) from hosts                      with installed NSX 6.2.3 VIBs to hosts with older release                      VIBs lose network connectivity</strong>                    <br />                    vMotion of virtual machines from a host running a newer                    version of NSX (the NSX VIB shows                    a higher export_version), to another host with lower version                    of NSX VIB is not supported. </p>                  <p><em>Workaround</em>: This is a known issue affecting NSX                    for vSphere 6.2.4 releases. See <a target="_blank" href="http://kb.vmware.com/kb/2146171">VMware                      knowledge                      base article 2146171</a> for more information.<br />                  </p>                  <!-- DOCNOTES:  1680322 6.2.3-->                  <p><strong>Issue 1683879: Upgrade to NSX 6.2.3 may fail on                      hosts with less than 8 GB of memory​</strong>                  </p>                  <p>NSX 6.2.3 requires a minimum of 8 GB of memory on prepared                    hosts running networking and security services. The minimum                    ESXi 6.0 memory requirement of 4 GB is not sufficient to run                    NSX.</p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1673626  6.2.3-->                  <p><strong>Issue 1673626: ​Using a database server alias name                      to create a DSN may cause the installation of vCenter                      Server to fail</strong>                  </p>                  <p>After upgrading from vCloud Networking and Security to NSX,                    you will see an error if you try to modify the tcpLoose                    setting in this API request: /api/3.0/edges.                  </p>                  <p><em>Workaround</em>: Use tcpPickOngoingConnections setting                    in the globalConfig section in the API request                    /api/4.0/firewall/config instead.</p>                  <!-- DOCNOTES:  1659558 6.2.3-->                  <p><strong>Issue 1658720: Enabling DFW for a given cluster                      would fail for a VCNS to NSX upgrade scenario where the                      cluster has VXLAN installed and vShield App not installed                      (or removed before upgrade) in VCNS deployment</strong>                  </p>                  <p>This issue occurs because the cluster sync status is not                    invoked when the hosts are upgraded.                  </p>                  <p><em>Workaround</em>: Restart NSX Manager.​</p>                  <!-- DOCNOTES: 1600281  6.2.3-->                  <p><strong>Issue 1600281: USVM Installation Status for Guest                      Introspection shows as Failed in the Service Deployments                      tab                    </strong>                  </p>                  <p>If the backing datastore for the Guest Introspection                    Universal SVM goes offline or becomes inaccessible, the USVM                    may need to be rebooted or re-deployed to recover.                  </p>                  <p><em>Workaround</em>:Reboot or re-deploy USVM to recover.</p>                  <!-- DOCNOTES: 1660373 6.2.3-->                  <p><strong>Issue 1660373: vCenter enforces expired NSX license</strong>                  </p>                  <p>As of vSphere 5.5 update 3 or vSphere 6.0.x vSphere                    Distributed Switch is included in the NSX license. However,                    vCenter does not allow ESX hosts to be added to a vSphere                    Distributed Switch if the NSX license is expired.                  </p>                  <p><em>Workaround</em>: Your NSX license must be active in                    order to                    add a host to a vSphere Distributed Switch.​</p>                  <!-- DOCNOTES: 1569010 6.2.3-->                  <p><strong>Issue 1569010/1645525: When upgrading from 6.1.x to                      NSX for vSphere 6.2.3 on a system connected to Virtual                      Center 5.5, the Product field in the "Assign License Key"                      window displays the NSX license as a generic value of "NSX                      for vSphere" and not a more specific version such as "NSX                      for vSphere - Enterprise."</strong>                  </p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1465249 6.2.3  -->                  <p><strong>Issue 1465249: Installation status for Guest                      Introspection shows Succeeded even though the host is                      offline</strong><br />                    After installing Guest Introspection on the cluster that has                    one host offline, the host that is offline shows                    Installation Status as Succeeded and Status Unknown.                  </p>                  <p><em>Workaround</em>: None.</p>                  <p></p>                  <!-- DOCNOTES: 1667454 6.2.3  -->                  <p><strong>Issue 1636916: In a vCloud Air environment, when                      the NSX Edge version is upgraded from vCNS 5.5.x to NSX                      6.x, Edge firewall rules with a source protocol value of                      "any" are changed to "tcp:any, udp:any" </strong><br />                    As a result, ICMP traffic is blocked, and packet drops may                    be seen.                  </p>                  <p><em>Workaround</em>: Before upgrading your NSX Edge                    version, create more specific Edge firewall rules and                    replace "any" with specific source port values.</p>                  <p></p>                  <!-- DOCNOTES: 1660949 6.2.3  -->                  <p><strong>Issue 1660355: VMs which are migrated from 6.1.5 to                      6.2.3 will not have support for TFTP ALG </strong><br />                    Even though the host is enabled, VMs which are migrated from                    6.1.5 to 6.2.3 will not have support for TFTP ALG. </p>                  <p><em>Workaround</em>: Add and remove the VM from the                    exclusion list or restart the VM, so that new 6.2.3 filter                    gets created which will have support for TFTP ALG.</p>                  <p></p>                  <!-- DOCNOTES:  1645477 6.2.3  -->                  <p><strong>Issue 1394287: Adding or removing VMs from a                      virtual wire does not update IP address set in vShieldApp                      rules </strong><br />                    If an existing vCNS vShield App firewall installation is not                    upgraded to the NSX distributed firewall in enhanced mode,                    new VMs with firewall rules based on virtual wires will not                    have an updated IP address. As a result, these VMs are not                    protected by the NSX firewall This issue is only seen in the                    following scenarios:                  </p>                  <ul>                    <li>After upgrading the Manager from vCNS to NSX and not                      switching to DFW Enhanced mode.</li>                    <li>If you add new VMs to a virtualWire with vshield App                      rules consuming those virtualwires, these rules will not                      have the new IP Address set for the new VMs. </li>                    This will cause the new VMs not protected by vShieldApp.                  </ul>                  <p><em>Workaround</em>: Publish the rule again which will set                    the new address.</p>                  <p></p>                  <!-- DOCNOTES: 1474238/1472872 -->                  <p><strong>Issue 1474238: After vCenter upgrade, vCenter might                      lose connectivity with NSX </strong><br />                    If you are using vCenter embedded SSO and you are upgrading                    vCenter 5.5 to vCenter 6.0, vCenter might lose connectivity                    with NSX. This happens if vCenter 5.5 was registered with                    NSX using the root user name. In NSX 6.2, vCenter                    registration with root is deprecated.                    NOTE: If you are using external SSO, no change is necessary.                    You can retain the same user name, for example                    admin@mybusiness.mydomain, and vCenter connectivity will not                    be lost.                    <br />                  </p>                  <p><em>Workaround</em>: Reregister vCenter with NSX using the                    admininstrator@vsphere.local user name instead of root.</p>                  <p></p>                  <!-- DOCNOTES: 1467872 -->                  <p><strong>Issue 1332563: Shutdown Guest OS for agent VMs                      (SVA) before powering OFF</strong><br />                    When a host is put into maintenance mode, all service                    appliances are                    powered-off, instead of shutting down gracefully. This may                    lead to                    errors within third-party appliances.                  </p>                  <p><em>Workaround</em>: None.</p>                  <p></p>                  <!-- DOCNOTES: 1473537/1112628 -->                  <p><strong>Issue 1473537: Unable to power on the Service                      appliance that was deployed using the Service Deployments                      view</strong><br />                  </p>                  <p><em>Workaround</em>: Before you proceed, verify the                    following:                  </p>                  <ul>                    <li>                      <p>The deployment of the virtual machine is complete.</p>                    </li>                    <!-- DOCNOTES: 1513762 -->                    <li>                      <p>No tasks such as cloning, reconfiguring, and so on are                        in                        progress for the virtual machine displayed in VC task                        pane.                      </p>                    </li>                    <li>                      <p>In the VC events pane of the virtual machine, the                        following events                        are displayed after the deployment is initiated: <br />                        <tt>                           <br />                          Agent VM &lt;vm name&gt; has been provisioned. <br />                          Mark agent as available to proceed agent workflow.                          <br />                                                   </tt>                        <br />                        In such a case, delete the service virtual machine. In                        service                        deployment UI, the deployment is seen as Failed. Upon                        clicking the Red                        icon, an alarm for an unavailable Agent VM is displayed                        for the host.                        When you resolve the alarm, the virtual machine is                        redeployed and                        powered on.</p>                    </li>                  </ul>                  <p></p>                  <p></p>                  <!-- DOCNOTES: ?-->                  <p><strong>If not all clusters in your environment are                      prepared, the Upgrade message for Distributed Firewall                      does not appear on the Host Preparation tab of                      Installation page </strong><br />                    When you prepare clusters for network virtualization,                    distributed firewall is enabled on those clusters. If not                    all clusters in your environment are                    prepared, the upgrade message for Distributed Firewall does                    not appear on the Host Preparation tab. <br />                  </p>                  <p><em>Workaround</em>: Use the following REST call to upgrade                    Distributed Firewall:<br />                  </p>                  <pre>PUT https://&lt;nsxmgr-ip&gt;/api/4.0/firewall/globalroot-0/state/&lt;nsxmgr-ip&gt;</pre>                  <p></p>                  <!-- DOCNOTES: 1215460 / 1211302: Resolved Wont Fix. Keep in RN-->                  <p><strong>Issue 1215460: If a service group is modified after                      the upgrade to add or remove services, these changes are                      not reflected in the firewall table</strong><br />                    User created service groups are expanded in the Edge                    Firewall table during upgrade - i.e., the Service column in                    the firewall table displays all services within the service                    group. If the service group is modified after the upgrade to                    add or remove services, these changes are not reflected in                    the firewall table.<br />                  </p>                  <p><em>Workaround</em>: Create a new service group with a                    different name and then consume this service group in the                    firewall rule.</p>                  <!-- DOCNOTES: 1088913 / 1088871-->                  <p><strong>Issue 1088913: vSphere Distributed Switch MTU does                      not get updated</strong><br />                    If you specify an MTU value lower than the MTU of the                    vSphere distributed switch when preparing a cluster, the                    vSphere Distributed Switch does not get updated to this                    value. This is to ensure that existing traffic with the                    higher frame size isn't unintentionally dropped.<br />                  </p>                  <p><em>Workaround</em>: Ensure that the MTU you specify when                    preparing the cluster is higher than or matches the current                    MTU of the vSphere distributed switch. The minimum required                    MTU for VXLAN is 1550.</p>                  <!-- DOCNOTES:   1413125 / No dev bug-->                  <p><strong>Issue 1413125: SSO cannot be reconfigured after                      upgrade</strong><br />                    When the SSO server configured on NSX Manager is the one                    native on                    vCenter server, you cannot reconfigure SSO settings on NSX                    Manager                    after vCenter Server is upgraded to version 6.0 and NSX                    Manager is                    upgraded to version 6.x.<br />                  </p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1288506 / -->                  <p><strong>Issue 1288506: After upgrading from vCloud                      Networking and Security 5.5.3 to NSX vSphere 6.0.5 or                      later, NSX Manager does not start up if you are using an                      SSL certificate with DSA-1024 keysize</strong><br />                    SSL certificates with DSA-1024 keysize are not supported in                    NSX vSphere 6.0.5 onwards, so the upgrade is not successful.<br />                  </p>                  <p><em>Workaround</em>: Import a new SSL certificate with a                    supported keysize before starting the upgrade.</p>                  <!-- DOCNOTES: 1266433 / 1263858-->                  <p><strong>Issue 1266433: SSL VPN does not send upgrade                      notification to remote client</strong><br />                    SSL VPN gateway does not send an upgrade notification to                    users. The administrator has to manually communicate that                    the SSL VPN gateway (server) is                    updated to remote users and they must update their clients.<br />                  </p>                  <p><em>Workaround</em>: Users need to uninstall the older                    version of client and install the latest version manually. </p>                  <!-- DOCNOTES: 1402307 / No dev bug-->                  <!-- DONE. This remains for 6.2. 2015-07-07-11:51 -->                  <p><strong>Issue 1402307: If vCenter is rebooted during NSX                      vSphere upgrade process, incorrect Cluster Status is                      displayed</strong><br />                    When you do host prep in an environment with multiple NSX                    prepared clusters during an upgrade and the vCenter Server                    gets rebooted after at                    least one cluster has been prepared, the other clusters may                    show Cluster Status as Not Ready instead of showing an                    Update link. The hosts                    in vCenter may also show Reboot Required.<br />                  </p>                  <p><em>Workaround</em>: Do not reboot vCenter during host                    preparation.</p>                  <!-- DOCNOTES: 1487752/1483819-->                  <p><strong>Issue 1487752: Momentary loss of third-party                      anti-virus protection during upgrade</strong><br />                    When upgrading from NSX 6.0.x to NSX 6.1.x or 6.2.x, you                    might experience momentary loss of third-party anti-virus                    protection for VMs. This issue does not affect upgrades from                    NSX 6.1.x to NSX 6.2.<br />                  </p>                  <p><em>Workaround</em>: None.                  </p>                  <!-- DOCNOTES: 1498376-->                  <p><strong>Issue 1498376: Host error message appears while                      configuring distributed firewall</strong><br />                    While configuring distributed firewall, if you encounter an                    error message related to the host, check the status of                    fabric feature <ttcom.vmware.vshield.nsxmgr.messaginginfra<                      tt="">.                      If the status is Red, perform the following workaround.<br />                    </ttcom.vmware.vshield.nsxmgr.messaginginfra<></p>                  <p><em>Workaround</em>: Use the following REST API call to                    reset communication between NSX Manager and a single host or                    all hosts in a cluster.</p>                  <p><tt>POST                      https://&lt;nsxmgr-ip&gt;/api/2.0/nwfabric/configure?action=synchronize</tt></p>                  <p>                  </p>                  <pre>&lt;nwFabricFeatureConfig&gt;&lt;featureId&gt;com.vmware.vshield.nsxmgr.messagingInfra&lt;/featureId&gt;  &lt;resourceConfig&gt;    &lt;resourceId&gt;HOST/CLUSTER MOID&lt;/resourceId&gt;&lt;/resourceConfig&gt;&lt;/nwFabricFeatureConfig&gt;</pre>                  <p></p>                  <!-- DOCNOTES: 1491820/1486589-->                  <p><strong>Issue 1491820: NSX Manager log collects <tt>WARN                        messagingTaskExecutor-7</tt> messages after upgrade to                      NSX 6.2</strong><br />                    After upgrading from NSX 6.1.x to NSX 6.2, the NSX Manager                    log becomes flooded with messages similar to: <tt>WARN                      messagingTaskExecutor-7                      ControllerInfoHandler:48 - host is unknown: host-15 return                      empty list.</tt> There is no operational impact.                  </p>                  <p><em>Workaround</em>: None.                  </p>                  <!-- DOCNOTES: 1284735 / 1556055-->                  <p><strong>Issue 1284735: After upgrade from vCNS, cannot                      place new grouping objects into some upgraded grouping                      objects</strong><br />                    vCNS 5.x supported creation of grouping objects at scopes                    below GlobalRoot (below the NSX-wide scope). For example, in                    vCNS 5.x you could create a grouping object as the DC or PG                    level. By contrast, the NSX 6.x user interface creates such                    objects under the GlobalRoot, and these newly created                    grouping objects cannot be added to existing grouping                    objects that were created at a lower scope (DC or PG) in                    your pre-upgrade vCNS installation.                  </p>                  <p><em>Workaround</em>: See <a target="_blank" href="http://kb.vmware.com/kb/2117821">                      VMware knowledge base article 2117821.</a>                  </p>                  <!-- DOCNOTES: 1495969/1497101-->                  <p><strong>Issue 1495969: After upgrading from vCNS 5.5.4 to                      NSX 6.2.x, the firewall on the Host Preparation tab                      remains disabled</strong><br />                    After upgrading from vCNS 5.5.x to NSX 6.2.x and upgrading                    all the clusters, the firewall on the Host Preparation tab                    remains disabled. In addition, the option to upgrade the                    firewall does not appear in the UI. This happens only when                    there are hosts that are not part of any prepared clusters                    in the datacenter, because the VIBs will not be installed on                    those hosts.                  </p>                  <p><em>Workaround</em>: To resolve the issue, move the hosts                    into an NSX 6.2 prepared cluster.                  </p>                  <!-- DOCNOTES: 1495307/1496963-->                  <p><strong>Issue 1495307: During an upgrade, L2 and L3                      firewall rules do not get published to hosts</strong><br />                    After publishing a change to the distributed firewall                    configuration, the status remains <tt>in progress</tt> both                    in the UI and the API indefinitely, and no log for L2 or L3                    rules is written to the file vsfwd.log.                  </p>                  <p><em>Workaround</em>: During an NSX upgrade, do not publish                    changes to the distributed firewall configuration. To exit                    from the <tt>in progress</tt> state and resolve the issue,                    reboot the NSX Manager virtual appliance.</p>                  <!-- DOCNOTES: 1474066/1476351-->                  <p><strong>Issue 1474066: The NSX REST API call to enable or                      disable IP detection seems to have no effect</strong><br />                    If host cluster preparation is not yet complete, the NSX                    REST API call to enable or disable IP detection <tt>(https://&lt;nsxmgr-ip&gt;/api/2.0/xvs/networks/universalwire-5/features)</tt>                    has no effect.                  </p>                  <p><em>Workaround</em>: Before issuing this API call, make                    sure the host cluster preparation is complete.</p>                  <!-- DOCNOTES: 1479314-->                  <p><strong>Issue 1479314: NSX 6.0.7 SSL VPN clients cannot                      connect to NSX 6.2 SSL VPN gateways</strong><br />                    In NSX 6.2 SSL VPN gateways, the SSLv2 and SSLv3 protocols                    are disabled. This means the SSL VPN gateway only accepts                    the TLS protocol. The SSL VPN 6.2 clients have been upgraded                    to use the TLS protocol by default during connection                    establishment.                    In NSX 6.0.7, the SSL VPN client uses an older version of                    OpenSSL library and the SSLv3 protocol to establish a                    connection. When an NSX 6.0.x client tries to connect to an                    NSX 6.2 gateway, the connection establishment fails at the                    SSL handshake level.                  </p>                  <p><em>Workaround</em>: Upgrade your SSL VPN client to NSX 6.2                    after you have upgraded to NSX 6.2. For upgrade                    instructions,                    see the <a target="_blank" href="http://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html">NSX                      Upgrade documentation</a>.</p>                  <!-- DOCNOTES: 1434909-->                  <p><strong>Issue 1434909: Must create a segment ID pool for                      new or upgraded logical routers</strong><br />                    In NSX 6.2, a segment ID pool with available segment IDs                    must be present before you can upgrade a logical router to                    6.2 or create a new 6.2 logical router. This is true even if                    you have no plans to use NSX logical switches in your                    deployment.                  </p>                  <p><em>Workaround</em>: If your NSX deployment does not have a                    local segment ID pool, create one as a prerequisite to NSX                    logical router upgrade or installation.</p>                  <!-- DOCNOTES: 1459032-->                  <p><strong>Issue 1459032: Error configuring VXLAN gateway</strong><br />                    When configuring VXLAN using a static IP pool (at <strong>Networking&amp;                      Security</strong>&gt;<strong>Installation</strong>&gt;                    <strong>Host Preparation</strong>&gt;<strong>Configure                      VXLAN</strong>) and the configuration fails to set an IP                    pool gateway IP on the VTEP(because the gateway is not                    properly configured or is not reachable), the VXLAN                    configuration status enters the Error (RED) state at for the                    host cluster.<br />                  </p>                  <p>The error message is <tt>VXLAN Gateway cannot be set on                      host</tt> and the error status is <tt>VXLAN_GATEWAY_SETUP_FAILURE</tt>.                    In the REST API call,                    <tt>GET                      https://&lt;nsxmgr-ip&gt;/api/2.0/nwfabric/status?resource=&lt;cluster-moid&gt;</tt>,                    the status of VXLAN is as follows:</p>                  <p></p>                  <pre>&lt;nwFabricFeatureStatus&gt;  &lt;featureId&gt;com.vmware.vshield.nsxmgr.vxlan&lt;/featureId&gt;  &lt;featureVersion&gt;5.5&lt;/featureVersion&gt;  &lt;updateAvailable&gt;false&lt;/updateAvailable&gt;  &lt;status&gt;RED&lt;/status&gt;  &lt;message&gt;VXLAN Gateway cannot be set on host&lt;/message&gt;  &lt;installed&gt;true&lt;/installed&gt;  &lt;enabled&gt;true&lt;/enabled&gt;  &lt;errorStatus&gt;VXLAN_GATEWAY_SETUP_FAILURE&lt;/errorStatus&gt;&lt;/nwFabricFeatureStatus&gt;</pre>                  <p></p>                  <p><em>Workaround</em>: To fix the error, there are two                    options.                  </p>                  <ul>                    <li>                      <p>Option 1: Remove VXLAN configuration for the host                        cluster, fix                        the underlying gateway setup in the IP pool by making                        sure the gateway                        is properly configured and reachable, and then                        reconfigure VXLAN for                        the host cluster.</p>                    </li>                    <li>                      <p>Option 2: Perform the following steps.</p>                    </li>                    <ol>                      <li>                        <p>Fix the underlying gateway setup in the IP pool by                          making sure the gateway is properly configured and                          reachable.</p>                      </li>                      <li>                        <p>Put the host (or hosts) into maintenance mode to                          ensure no VM traffic is active on the host.</p>                      </li>                      <li>                        <p>Delete the VXLAN VTEPs from the host.</p>                      </li>                      <li>                        <p>Take the host out of maintenance mode. Taking hosts                          out of                          maintenance mode triggers the VXLAN VTEP creation                          process on NSX                          Manager. NSX Manager will try to re-create the                          required VTEPs on the                          host.</p>                      </li>                    </ol>                  </ul>                  <p></p>                  <!-- DOCNOTES: 1463767/1491181-->                  <p><strong>Issue 1463767: In a cross vCenter deployment, a                      universal                      firewall configuration section might be under (subordinate                      to) a local configuration section</strong><br />                    If you move a secondary NSX Manager to the standalone                    (transit)                    state and then change it back to the secondary state, any                    local                    configuration changes that you made while it was temporarily                    in the                    standalone state might be listed above the replicated                    universal                    configuration sections inherited from the primary NSX                    Manager. This                    produces the error condition <tt>universal section has to                      be on top of all other sections on secondary NSX Managers</tt>.                  </p>                  <p><em>Workaround</em>: Use the UI option to move sections up                    or down                    so that the local section is below the universal section.</p>                  <p></p>                  <!-- DOCNOTES: 1289348-->                  <p><strong>Issue 1289348: After an upgrade, firewall rules and                      network introspection services might be out of sync with                      NSX Manager</strong><br />                    After upgrading from NSX 6.0 to NSX 6.1 or 6.2, the NSX                    firewall configuration displays the error message: <tt>synchronization                      failed / out of sync</tt>. Using the <strong>Force Sync                      Services</strong>: <strong>Firewall</strong> action does                    not resolve the issue.                  </p>                  <p><em>Workaround</em>: In NSX 6.1 and NSX 6.2, either                    security groups or dvPortgroups can be bound to a service                    profile, but not both. To resolve the issue, modify your                    service profiles.</p>                  <!-- DOCNOTES: 1462319-->                  <p><strong>Issue 1462319: The esx-dvfilter-switch-security VIB                      is no longer present in the output of the "esxcli software                      vib list | grep esx" command.</strong><br />                    Starting in NSX 6.2, the esx-dvfilter-switch-security                    modules are included within the esx-vxlan VIB. The only NSX                    VIBs installed for 6.2 are esx-vsip and esx-vxlan. During an                    NSX upgrade to 6.2, the old esx-dvfilter-switch-security VIB                    gets removed from the ESXi hosts.<br />                    Starting in NSX 6.2.3, a third VIB, esx-vdpi, is provided                    along with the esx-vsip and esx-vxlan NSX VIBs.                    A successful installation will show all 3 VIBs.                  </p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1481083-->                  <p><strong>Issue 1481083: After the upgrade, logical routers                      with explicit failover teaming configured might fail to                      forward packets properly</strong><br />                    When the hosts are running ESXi 5.5, the explicit failover                    NSX 6.2 teaming policy does not support multiple active                    uplinks on distributed logical routers.                  </p>                  <p><em>Workaround</em>: Alter the explicit failover teaming                    policy so that there is only one active uplink and the other                    uplinks are in standby mode.</p>                  <!-- DOCNOTES: 1485862-->                  <p><strong>Issue 1485862: Uninstalling NSX from a host cluster                      sometimes results in an error condition</strong><br />                    When using the Uninstall action on                    the <strong>Installation</strong>: <strong>Host                      Preparation</strong>                    tab, an error might occur with the <tt>eam.issue.OrphanedAgency</tt>                    message appearing in the EAM logs for the hosts. After using                    the                    Resolve action and rebooting the hosts, the error state                    continues even                    though the NSX VIBs are successfully uninstalled.                  </p>                  <p><em>Workaround</em>: Delete the orphaned agency from the                    vSphere                    ESX Agent Manager (<strong>Administration</strong>: <strong>vCenter
Server                      Extensions</strong>: <strong>vSphere ESX Agent Manager</strong>).</p>                  <!-- DOCNOTES: 1479314-->                  <p><strong>Issue 1479314: SSLv2 and SSLv3 deprecated in NSX                      6.2</strong><br />                    Starting in NSX 6.2, the SSL VPN gateway only accepts the                    TLS protocol. After the NSX upgrade, any new NSX 6.2 clients                    that you create automatically use the TLS protocol during                    connection establishment. When an NSX 6.0.x client tries to                    connect to an NSX 6.2 gateway, the connection establishment                    fails at the SSL handshake step.                  </p>                  <p><em>Workaround</em>: After the upgrade to NSX 6.2,                    uninstall your old SSL VPN clients and install the NSX 6.2                    version of the SSL VPN clients.</p>                  <!-- DOCNOTES: 1411275 / 1500977 -->                  <p><strong>Issue 1411275: vSphere Web Client does not display                      Networking and Security tab after backup and restore in                      NSX vSphere 6.2</strong><br />                    When you perform a backup and restore operation after                    upgrading to NSX vSphere 6.2, the vSphere Web Client does                    not display the <strong>Networking and Security</strong>                    tab.                  </p>                  <p><em>Workaround</em>: When an NSX Manager backup is                    restored, you are logged out of the Appliance Manager. Wait                    a few minutes before logging in to the vSphere Web Client.</p>                  <p></p>                  <!-- DOCNOTES: 1493777-->                  <p><strong>Issue 1493777: After upgrade to NSX 6.2, NSX                      Manager has more than 100 percentage of physical memory                      allocated</strong><br />                    Starting in NSX 6.2, NSX Manager requires 16 GB of reserved                    memory. The former requirement was 12 GB.                  </p>                  <p><em>Workaround</em>: Increase the NSX Manager virtual                    appliance's reserved memory to 16 GB.</p>                  <!--===============================Service Deployment Issues==============================================================--->                  <!-- this section subsumed into install issues section -->                  <!-- <h3><a name="NSXendpointissues"></a>Service Deployment Issues</h3> -->                  <!-- DOCNOTES: doc 1406430 / dev 1393889-->                  <p><strong>Issue 1393889: Data Security service status is                      shown as UP even when IP connectivity is not established</strong><br />                    Data Security appliance may have not received the IP address                    from DHCP or is connected to an incorrect port group.<br />                  </p>                  <p><em>Workaround</em>: Ensure that the Data Security                    appliance gets the IP from DHCP/IP Pool and is reachable                    from the management network. Check if the ping to the Data                    Security appliance is successful from NSX/ESX.</p>                  <!-- DOCNOTES: -->                  <p><strong>Service virtual machine deployed using the Service                      Deployments tab on the Installation page does not get                      powered on</strong><br />                  </p>                  <p><em>Workaround</em>: Follow the steps below.</p>                  <ol>                    <li>Manually remove the service virtual machine from the <code>ESX                        Agents</code> resource pool in the cluster.</li>                    <li>Click <b>Networking and Security</b> and then click <b>Installation</b>.                    </li>                    <li>Click the <b>Service Deployments</b> tab.</li>                    <li>Select the appropriate service and click the <b>Resolve</b>                      icon.<br />                      The service virtual machine is redeployed.</li>                  </ol>                  <p></p>                  <!--=======================================NSX Manager Issues==============================================================--->                  <!--=======================================NSX Manager Issues==============================================================--->                  <!--=======================================NSX Manager Issues==============================================================--->                  <h3><a name="NSXmanagerissues"></a>NSX Manager Known Issues</h3>                  <!-- DOCNOTES:  1696750 6.2.4 -->                  <p><strong>Issue 1696750: Assigning an IPv6 address to NSX                      Manager via PUT API requires a reboot to take effect</strong><br />                    Changing the configured network settings for NSX Manager via                    <em> https://{NSX Manager IP                      address}/api/1.0/appliance-management/system/network </em>                    requires a system reboot to take effect. Until the reboot,                    pre-existing settings will be shown.                  </p>                  <p><em>Workaround</em>: None.</p>                  <p></p>                  <!-- DOCNOTES:1675504 6.2.3 -->                  <p><strong>Issue 1671067: NSX Plugin does not appear in                      vCenter Web Client while ESXTOP plugin is also installed</strong><br />                    After deployment of NSX and successful registration with                    vCenter, NSX plugin does not appear in the vCenter Web                    Client.                    This issue is caused by conflict between NSX plugin and                    ESXTOP plugin.</p>                  <p><em>Workaround</em>: Remove ESXTOP plugin with the                    following procedure:<br />                  </p>                  <ol>                    <li>Make sure there is a recent backup of vCenter Snapshot                      vCenter VM (without quiesce)</li>                    <li>Delete                      /usr/lib/vmware-vsphere-client/plugin-packages/esxtop-plugin<br />                      rm -R                      /usr/lib/vmware-vsphere-client/plugin-packages/esxtop-plugin</li>                    <li>Delete /usr/lib/vmware-vsphere-client/server/work <br />                      rm -R /usr/lib/vmware-vsphere-client/server/work</li>                    <li>Restart the web client<br />                      service vsphere-client restart </li>                    <li>(Optional) Ensure that there is no output from the                      following command: "tail -f                      /var/log/vmware/vsphere-client/logs/eventlog.log | grep                      esx" </li>                    <li>Make sure to consolidate vCenter snapshot if that is the                      preferred method of roll back option </li>                  </ol>                  <p></p>                  <p></p>                  <!-- DOCNOTES: 1487103-->                  <p><strong>Issue 1466790: NSX Manager does not accept DNS                      search strings with a space delimiter</strong><br />                    NSX Manager does not accept DNS search strings with a space                    delimiter.                    You may only use a comma as a delimiter. For example, if the                    DHCP                    server advertises <tt>eng.sample.com</tt> and <tt>sample.com</tt>                    for the DNS search                    list, NSX Manager is configured with <tt>eng.sample.com                      sample.com</tt>.                  </p>                  <p><em>Workaround</em>: Use comma separators. NSX Manager only                    accepts comma separator as DNS search strings.</p>                  <p></p>                  <!-- DOCNOTES: 1582809 6.2.3 -->                  <p><strong>Issue 1529178: Uploading a server certificate which                      does not include a common name returns an "internal server                      error" message </strong> </p>                  <p> If you upload a server certificate that does not have any                    common name, an "internal server error" message appears. </p>                  <p><em>Workaround</em>: Use a server certificate which has                    both a SubAltName and a common                    name, or at least a common name.</p>                  <p></p>                  <!-- DOCNOTES: 1655388 6.2.3 -->                  <p><strong>Issue 1655388: NSX Manager 6.2.3 UI displays                      English language instead of local language when using                      IE11/Edge browser on Windows 10 OS for JA, CN, and DE                      languages</strong> </p>                  <p> When you launch NSX Manager 6.2.3 with IE11/Edge browser                    on Windows 10 OS for JA, CN, and DE languages, English                    language is displayed. </p>                  <p><em>Workaround</em>: </p>                  <p>Perform the following steps:</p>                  <ol>                    <li>Launch the Microsoft Registry Editor (regedit.exe), and                      go to <strong>Computer &gt; HKEY_CURRENT_USER &gt;                        SOFTWARE &gt; Microsoft &gt; Internet Explorer &gt;                        International</strong>. </li>                    <li>Modify the value of <em>AcceptLanguage</em> file to                      native language. For example, If you want to change                      language to <strong>DE</strong>, change value and make                      the <strong>DE</strong> show the first position. </li>                    <li>Restart the browser, and log in to the NSX Manager                      again. Appropriate language is displayed.</li>                  </ol>                  <p></p>                  <!-- DOCNOTES: 1542709 6.2.3 -->                  <p><strong>Issue 1446649/1445281: Change in Secondary NSX                      Manager IP / Thumbprint results into Replication errors of                      Universal objects in a Cross-vCenter setup</strong> </p>                  <p> If there is any change in the Secondary NSX Manager IP /                    Thumbprint, it would result into Replication errors of                    Universal objects in a Cross-vCenter setup as the Primary                    NSX Manager would not be aware of the latest IP/Thumbprint                    of the Secondary NSX Manager.</p>                  <p><em>Workaround</em>: If you click the Universal sync status                    of an Universal object and see exception like "Peer not                    authenticated;nested exception is                    javax.net.ssl.SSLPeerUniverifiedException", you can realize                    that the IP/Thumbprint has been changed. </p>                  <p></p>                  <!-- DOCNOTES:  1660510 6.2.3 -->                  <p><strong>Issue 1660718: Service Composer policy status is                      shown as "In Progress" at the UI and "Pending" in the API                      output </strong></p>                  <p> <em>Workaround</em>: None. </p>                  <!-- DOCNOTES: 1658836  6.2.3 -->                  <p><strong>Issue 1620491: Policy-level Sync status in Service                      Composer does not show publishing status of the rules                      within a policy </strong></p>                  When a policy is created or modified, Service Composer will                  display a success status which indicates only the persistence                  state. It does not reflect whether the rules were published to                  the host successfully.                  <p> <em>Workaround</em>: Use the firewall UI to view publish                    status.</p>                  <!-- DOCNOTES: 1435996/1485135-->                  <p><strong>Issue 1435996: Log files exported as CSV from NSX                      Manager are timestamped with epoch not datetime </strong><br />                    Log files exported as CSV from NSX Manager using the vSphere                    Web Client are timestamped with the epoch time in                    milliseconds, instead of with the appropriate time based on                    the time zone. <br />                  </p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1467871-->                  <p><strong>Issue 1466790: Unable to choose VMs on bridged                      network using the NSX traceflow tool</strong><br />                    Using the NSX traceflow tool, you cannot select VMs that are                    not attached to a logical switch. This means that VMs on an                    L2 bridged network cannot be chosen by VM name as the source                    or destination address for traceflow inspection.</p>                  <p><em>Workaround</em>: For VMs attached to L2 bridged                    networks, use the IP address or MAC address of the interface                    you wish to specify as destination in a traceflow                    inspection. You cannot choose VMs attached to L2 bridged                    networks as source. See the <a target="_blank" href="http://kb.vmware.com/kb/2129191">knowledge
                      base article 2129191</a> for more information.</p>                  <!-- DOCNOTES: 1641066  6.2.3-->                  <p><strong>Issue 1644297: Add/delete operation for any DFW                      section on the primary NSX creates two DFW saved                      configurations on the secondary NSX</strong><br />                    In a cross-vCenter setup, when an additional universal or                    local DFW section is added to the primary NSX Manager, two                    DFW configurations are saved on the secondary NSX Manager.                    While it does not affect any functionality, this issue will                    cause the saved configurations limit to be reached more                    quickly, possibly overwriting critical configurations.<br />                  </p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1534877 1535693 6.2.1-->                  <p><strong>Issue 1534877: NSX management service doesn't come                      up when the hostname's length is more than 64 characters                    </strong><br />                    Certificate creation via OpenSSL library requires a hostname                    less than or equal to 64 characters. <br />                    <!-- DOCNOTES: 1537258 6.2.1-->                  </p>                  <p><strong>Issue 1537258: NSX Manager list slow to display in                      Web Client</strong><br />                    In vSphere 6.0 environments with multiple NSX Managers, the                    vSphere                    web client may take up to two minutes to display the list of                    NSX                    Managers when the logged-in user is being validated with a                    large AD                    Group set. You may see a data service timeout error when                    attempting to                    display the NSX Manager list. There is no workaround. You                    must wait                    for the list to load/relogin to see the NSX Manager list.<br />                    <!-- DOCNOTES: 1534622 6.2.1-->                  </p>                  <p><strong>Issue 1534622: NSX controller shows as disconnected</strong><br />                    NSX Manager logs report disconnection to controllers via a                    message similar to "ERROR http-nio-127.0.0.1-7441-exec-16908                    BaseRestController:339 - Exception : 'I/O error: Read timed                    out; nested exception is java.net.SocketTimeoutException:                    Read timed out'". This condition occurs when an intermediate                    firewall on the network blocks the TCP/IP FIN message after                    the idle timeout value is reached. When this condition is                    occurring, the number of connections to the NSX Manager will                    increase. <br />                    <!-- DOCNOTES: 1534606 6.2.1-->                  </p>                  <p><strong>Issue 1534606: Host Preparation Page fails to load</strong><br />                    When running Virtual Center in linked mode, each VC must be                    connected to an NSX Manager on the same NSX version. If the                    NSX versions differ, the vSphere Web Client will only be                    able to communicate with the NSX Manager running the higher                    version of NSX. An error similar to "Could not establish                    communication with NSX Manager. Please contact                    administrator," will be displayed on the Host Preparation                    tab. <br />                  </p>                  <p><em>Workaround</em>: All NSX managers should be upgraded to                    the same NSX software version.</p>                  <!-- DOCNOTES: 1317814 / 1327964-->                  <p><strong>Issue 1317814: Service Composer goes out of sync                      when policy changes are made while one of the Service                      Managers is down</strong><br />                    When a policy changes is made when one of multiple Service                    Managers is down,                    the changes will fail, and Service Composer will fall out of                    sync.<br />                  </p>                  <p><em>Workaround</em>: Ensure the Service Manager is                    responding and then issue a force sync from Service                    Composer.</p>                  <!-- DOCNOTES: 1386874 / No dev bug-->                  <p><strong>Issue 1386874: Networking and Security Tab not                      displayed in vSphere Web Client</strong><br />                    After vSphere is upgraded to 6.0, you cannot see the                    Networking and Security Tab when you log in to the vSphere                    Web Client with the root user name. <br />                  </p>                  <p><em>Workaround</em>: Log in as administrator@vsphere.local                    or as any other vCenter user which existed on vCenter Server                    before the upgrade and whose role was defined in NSX                    Manager.</p>                  <!-- DOCNOTES: dev: 1415480-->                  <p><strong>Issue 1415480: After NSX Manager backup is                      restored, REST call shows status of fabric feature <tt>com.vmware.vshield.nsxmgr.messagingInfra</tt>                      as <tt>Red</tt></strong><br />                    When you restore the backup of an NSX Manager and check the                    status of fabric feature <tt>com.vmware.vshield.nsxmgr.messagingInfra</tt>                    using a REST API call, it is displayed as <tt>Red</tt>                    instead of <tt>Green</tt>. <br />                  </p>                  <p><em>Workaround</em>: Use the following REST API call to                    reset communication between NSX Manager and a single host or                    all hosts in a cluster.                    <br />                    <code>                      POST https://&lt;nsxmgr-ip&gt;/api/2.0/nwfabric/configure?                      action=synchronize<br />                      &lt;nwFabricFeatureConfig&gt;<br />&lt;featureId&gt;com.vmware.vshield.nsxmgr.messagingInfra&lt;/featureId&gt;<br />                      &lt;resourceConfig&gt;<br />                          &lt;resourceId&gt;HOST/CLUSTER                      MOID&lt;/resourceId&gt;<br />                      &lt;/resourceConfig&gt;<br />                      &lt;/nwFabricFeatureConfig&gt;                    </code>                  </p>                  <!-- DOCNOTES: 1070905: Base bug converted to RN bug. Send to KB team,     remove after KB created.-->                  <p><strong>Issue 1070905: Cannot remove and re-add a host to a                      cluster protected by Guest Introspection and third-party                      security solutions</strong><br />                    If you remove a host from a cluster protected by Guest                    Introspection and third-party security solutions by                    disconnecting it and then removing it from vCenter Server,                    you may experience problems if you try to re-add the same                    host to the same cluster.<br />                  </p>                  <p><em>Workaround</em>: To remove a host from a protected                    cluster, first put the host in maintenance mode. Next, move                    the host into an unprotected                    cluster or outside all clusters and then disconnect and                    remove the host.</p>                  <!-- DOCNOTES: 1027066: Resolved Wont Fix. Leave in RN-->                  <p><strong>Issue 1027066: vMotion of NSX Manager may display                      the error message, "<code>Virtual ethernet card Network                        adapter 1 is not supported</code>"</strong><br />                    You can ignore this error. Networking will work correctly                    after vMotion.<br />                    <!-- DOCNOTES: doc: 1406471/ dev: 1400350-->                  </p>                  <p><strong>Issue 1406471: Syslog shows host name of backed up                      NSX Manager on the restored NSX Manager</strong><br />                    If a second NSX Manager is installed with the same IP                    address and a unique hostname as the first NSX Manager,                    restoring the                    configuration will show the first NSX Manager’s hostname                    after a restore and the second NSX Manager’s hostname after                    reboot. <br />                  </p>                  <p><em>Workaround</em>: Host name of second NSX Manager should                    be configured to be same as the backed up NSX Manager.</p>                  <!-- DOCNOTES: 1477041/1489753-->                  <p><strong>Issue 1477041: NSX Manager virtual appliance                      summary page shows no DNS name</strong><br />                    When you log in to the NSX Manager virtual                    appliance, the Summary page has a field for the DNS name.                    This field                    remains blank even though a DNS name has been defined for                    the NSX                    Manager appliance.                  </p>                  <p><em>Workaround</em>: You can view the NSX Manager's                    hostname and                    the search domains on the Manage: Network page.</p>                  <p></p>                  <!-- DOCNOTES: 1492880-->                  <p><strong>Issue 1492880: NSX Manager UI do not automatically                      log out after changing password using NSX Command Line                      Interface</strong><br />                    If you are logged in to NSX Manager and recently changed                    your password using CLI, you might continue to stay logged                    in to the NSX Manager UI using your old password. Typically,                    NSX Manager client should automatically log you out if the                    session times out for being inactive.                  </p>                  <p><em>Workaround</em>: Log out from the NSX Manager UI and                    log back in with your new password.</p>                  <p></p>                  <!-- DOCNOTES: 1467866-->                  <p><strong>Issue 1467866: Standalone NSX Manager incorrectly                      allows import of universal firewall configuration </strong><br />                    On an NSX Manager running in stand-alone mode, universal                    firewall rules can be imported even though they do not                    apply.                    Once imported, these rules cannot be deleted via API or the                    Web Client. Instead, they are retained and treated as a                    local section.                  </p>                  <p><em>Workaround</em>: If you are running NSX Manager in                    standalone role, do not import a firewall configuration that                    contains universal rules. If you have already imported a                    universal firewall rule into a standalone NSX Manager, fix                    the issue by importing a saved firewall configuration file                    that does not contain universal rules, and publish that                    configuration file by loading it in the Firewall table. <br />                    Perform the following steps:                  </p>                  <ol>                    <li>                      <p>Log in to the vSphere Web Client.</p>                    </li>                    <li>                      <p>Click <strong>Networking &amp; Security</strong> and                        then click <strong>Firewall</strong>.</p>                    </li>                    <li>                      <p>Click the <strong>Firewall</strong> tab.</p>                    </li>                    <li>                      <p>Click the <strong>Saved Configurations</strong> tab.</p>                    </li>                    <li>                      <p>Click the <strong>Import configuration (import)</strong>                        icon.</p>                    </li>                    <li>                      <p>Click <strong>Browse</strong> and select the file                        containing the configuration that you want to import.                      </p>                      <p>Rules are imported based on the rule names. During the                        import, Firewall ensures that each object referenced in                        the rule exists in your environment. If an object is not                        found, the rule is marked as invalid. If a rule                        referenced a dynamic security group, the dynamic                        security group is created in NSX Manager during the                        import.</p>                    </li>                    <li>                      <p>Add the node back as a secondary node. The                        synchronizing across NSX Managers automatically syncs up                        the universal section correctly performing any required                        cleanup.</p>                      Once you have successfully published the configuration                      file, the rules are pushed down to the host and impact the                      datapath. The system functions as expected.                      <p></p>                    </li>                  </ol>                  <p></p>                  <p></p>                  <!-- DOCNOTES: 1468613-->                  <p><strong>Issue 1468613: Unable to edit a network host name</strong><br />                    After you login to NSX Manager virtual appliance and                    navigate to the Appliance Management, click Manage Appliance                    Settings, and click Network under Settings to edit the                    network host name, you might receive an invalid domain name                    list error. This happens when the domain names specified in                    the Search Domains field are separated with whitespace                    characters, instead of commas. NSX Manager only accepts                    domain names that are comma separated.                    <br />                    <em>Workaround</em>: Perform the following steps:                  </p>                  <ol>                    <li>                      <p>Log in to the NSX Manager virtual appliance.</p>                    </li>                    <li>                      <p>Under <strong>Appliance Management</strong>, click <strong>Manage                          Appliance Settings</strong>.</p>                    </li>                    <li>                      <p>From the Settings panel, click <strong>Network</strong>.</p>                    </li>                    <li>                      <p>Click <strong>Edit</strong> next to DNS Servers.</p>                    </li>                    <li>                      <p>In the Search Domains field, replace all whitespace                        characters with commas.</p>                    </li>                    <li>                      <p>Click <strong>OK</strong> to save the changes.</p>                    </li>                  </ol>                  <p></p>                  <!-- DOCNOTES: 1436953-->                  <p><strong>Issue 1436953: False system event is generated even                      after successfully restoring NSX Manager from a backup</strong><br />                    After successfully restoring NSX Manager from a backup, the                    following                    system events appear in the vSphere Web Client when you                    navigate                    to <strong>Networking &amp; Security</strong>: <strong>NSX                      Managers</strong>: <strong>Monitor</strong>: <strong>System                      Events</strong>.                  </p>                  <ul>                    <li>                      <p><tt>Restore of NSX Manager from backup failed (with                          Severity=Critical)</tt>.</p>                    </li>                    <li>                      <p><tt>Restore of NSX Manager successfully completed (with                          Severity=Informational)</tt>.</p>                    </li>                  </ul>                  <em>Workaround</em>: If the final system event message shows                  as successful, you can ignore the system generated event                  messages.                  <p></p>                  <!-- DOCNOTES: 1489768/1471685-->                  <p><strong>Issue 1489768: Change in behavior of NSX REST API                      call to add a namespace in a datacenter</strong><br />                    In NSX 6.2, the                    <tt>POST                      https://&lt;nsxmgr-ip&gt;/api/2.0/namespace/datacenter/<datacenter-id></datacenter-id></tt>                    REST API call returns a URL with an absolute path, for                    example                    <tt>http://198.51.100.3/api/2.0/namespace/api/2.0/namespace/datacenter/datacenter-1628/2</tt>.                    In previous releases of NSX, this API call returned a URL                    with a                    relative path, for example:                    <tt>/api/2.0/namespace/datacenter/datacenter-1628/2</tt>.</p>                  <p>                    <em>Workaround</em>: None.</p>                  <p></p>                  <!--===============================================Logical Networking Issues=====================================-->                  <!--===============================================Logical Networking Issues=====================================-->                  <!--===============================================Logical Networking Issues=====================================<!--<h3><a name="NSXlogical"></a>Logical Networking Issues</h3>-->                  <!--===============================================NSX Edge and Logical Routing Issues================================-->                  <!--===============================================NSX Edge and Logical Routing Issues================================-->                  <!--===============================================NSX Edge and Logical Routing Issues================================-->                  <h3><a name="NSXnetworkissues"></a>Logical Networking Known                    Issues and NSX Edge Known Issues</h3>                  <!-- DOCNOTES: 1738683  6.2.4 -->                  <p><strong>Issue 1733146: Under certain conditions, creating                      or modifying LIFs for a Universal DLR fails when no                      control VM exists                    </strong> </p>                  <p>This issue is known to manifest under the following                    conditions:                  </p>                  <ul>                    <li>ECMP with two static default routes</li>                    <li>Static routes with local egress flag</li>                  </ul>                  This issue results from a full synchronization being requested                  instead of a delta update,                  resulting in the rejection of duplicate entities and a failed                  operation.                  A message similar to the following will be seen:<br />                  <em>2016-09-22 20:18:58.080 GMT ERROR TaskFrameworkExecutor-24                    NvpRestClientManagerImpl:774 - NVP API returns error:                    [409] Route with the same prefix and priority already exist                    on router dc5e541a-d7a6-4cb9-8d8a-9334a9c51127</em>                  <p></p>                  <p><em>Workaround</em>:                  </p>                  <ol>                    <li>Delete the universal logical router.</li>                    <li>Deploy a new universal logical router. Enable local                      egress and uncheck “Deploy Edge Appliance”. Configure two                      uplink interfaces and a default gateway IP address via the                      first uplink using the locale-id on the primary DLR (for                      example, 1111xxxx). </li>                    <li>Do not add a static route of 0.0.0.0/0 with the locale                      id used on the secondary site (for example, 2222xxxx).</li>                    <li>Add the following two static routes with the expected                      next-hop IP address and locale id of the secondary site                      (for example, 222xxxx). </li>                    Route #1: 0.0.0.0/1<br />                    Route #2: 128.0.0.0/1                  </ol>                  <p></p>                  <!-- DOCNOTES: 1739494  6.2.4 -->                  <p><strong>Issue 1716545: Changing appliance size of Edge does                      not affect standby Edge's CPU and Memory reservation                    </strong> </p>                  <p>Only the first Edge VM created as part of an HA pair is                    assigned the reservation settings.<br />                    To configure the same CPU/Memory reservation on both Edge                    VMs:                  </p>                  <ul>                    <li>Use the PUT API https://<nsxmanager>/api/4.0/edgePublish/tuningConfiguration                        to set explicit values for both Edge VMs.</nsxmanager></li>                    or                    <li>Disable and re-enable Edge HA, which will delete the                      second Edge VM and redeploy a new one with the default                      reservations.</li>                  </ul>                  <p></p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1738662  6.2.4 -->                  <p><strong>Issue 1717369: When configured in HA mode, both                      active and standby Edge VMs may be deployed on the same                      host</strong> </p>                  <p> This issue results from anti-affinity rules not being                    created and applied on the vSphere hosts automatically                    during redeploy and upgrade operations. This issue will not                    be seen when HA is being enabled on existing Edge.                    In NSX releases with a fix for this issue, the following                    will be the expected behavior: <br />                  </p>                  <ul>                    <li>When vSphere HA is enabled, anti-affinity rules for Edge                      VMs of an HA pair will be created during redeploy,                      upgrade.</li>                    <li> When vSphere HA is disabled, anti-affinity rules for                      Edge VMs of an HA pair will not be created.</li>                  </ul>                  <p></p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1510724  6.2.4 -->                  <p><strong>Issue 1510724: Default routes do not populate on                      the hosts after creating a new Universal Distributed                      Logical Router (UDLR)</strong> </p>                  <p> After changing NSX Manager from Standalone to Primary mode                    for the purpose of configuring Cross-vCenter in NSX for                    vSphere 6.2.x, you may experience these symptoms: <br />                  </p>                  <ul>                    <li>When you create a new UDLR, the default routes are not                      populated on the host instance.</li>                    <li>Routes are populated on the UDLR Control VM but not on                      the host instance.</li>                    <li>Running the <em>show logical-router host host-ID dlr                        Edge-ID route</em> command fails to show default routes.</li>                  </ul>                  <p></p>                  <p><em>Workaround</em>: To recover from this issue, refer to <a                      target="_blank"                      href="https://kb.vmware.com/kb/2145959">VMware                      knowledge base article 2145959</a>.</p>                  <!-- DOCNOTES:  1738529 6.2.4 -->                  <p><strong>Issue 1704540: High volume of MAC learning table                      updates with NSX L2 bridge and LACP may lead to out of                      memory condition</strong> </p>                  <p> When an NSX L2 bridge sees a MAC address on a different                    uplink, it reports a MAC learning table change to                    controllers via the netcpa process. Networking environments                    with LACP will learn the same MAC address on multiple                    interfaces, resulting in a very high volume of table updates                    and potentially exhausting the memory needed by the netcpa                    process to do the reporting.</p>                  <p><em>Workaround</em>: Avoid setting a flow-based hashing                    algorithm on the physical switch when using LACP. Instead,                    pin MAC addresses to the same uplinks or change the policy                    to source-MAC.</p>                  <!-- DOCNOTES: 1709004  6.2.4 -->                  <p><strong>Issue 1703247: VMs lose network connectivity in NSX                      with DLR HA</strong> </p>                  <p>In NSX 6.2.3 environment using dynamic routing with High                    Availability (HA) configured on a DLR Control VM, virtual                    machines lose network connectivity when the DLR control VMs                    recover from a split-brain condition. </p>                  <p><em>Workaround</em>: To recover from this networking issue,                    refer to <a target="_blank" href="https://kb.vmware.com/kb/2146413">VMware                      knowledge base article 2146413</a>.</p>                  <!-- DOCNOTES: 1712105 - 6.2.0 -->                  <p><strong>Issue 1492547: Extended convergence time seen when                      NSX-based OSPF area border router with                      highest IP address is shut down or rebooted</strong><br />                    If an NSSA area border router which does not have the                    highest IP address is                    shut down or rebooted, traffic converges rapidly to another                    path. If an NSSA                    area border router with the highest IP address is shut down                    or rebooted, a                    multi-minute re-convergence time is seen. The OSPF process                    can be cleared                    manually to reduce the convergence time.                  </p>                  <p><em>Workaround:</em> See <a target="_blank" href="https://kb.vmware.com/kb/2127369">VMware                      knowledge base article 2127369</a>.                    <!-- DOCNOTES: 1711253 - 6.2.1 -->                  </p>                  <p><strong>Issue 1542416: Data path not working for 5 min                      after edge re-deploy and HA failover                      with sub-interfaces</strong>                    <br />                    Redeploy or HA failover operation will see a five minute                    outage                    if sub-interfaces are used. Issue is not observed on                    interfaces.                  </p>                  <p><em>Workaround: </em>No workaround.                    <!--DOCNOTES: 1709811 - 6.2.4 -->                  </p>                  <p><strong>Issue 1706429: Communication issues when enabling                      high availability                      (HA) after initial logical (distributed) router deployment                      might cause both                      logical router appliances to be active.</strong><br />                    If you deploy a logical router without HA and then later                    enable HA (deploying                    a new logical router appliance), or if you disable and then                    reenable HA,                    sometimes one of the logical router appliances is missing a                    connected route to                    the HA interface. This causes both appliances to be in the                    active state.                  </p>                  <p><em>Workaround:</em> On the logical router appliance that                    is missing                    the connected route for the HA interface, either disconnect                    and then reconnect the vNIC of the                    logical router appliance, or reboot the logical router                    appliance.                    <!-- DOCNOTES: 1626940  6.2.3 -->                  </p>                  <p><strong>Issue 1461421: "show ip bgp neighbor" command                      output for NSX Edge retains the historical count of                      previously established connections</strong> </p>                  <p>The “show ip bgp neighbor” command displays the number of                    times that the BGP state machine transitioned into the                    Established state for a given peer. Changing the password                    used with MD5 authentication causes the peer connection to                    be destroyed and re-created, which in turn will clear the                    counters. This issue does not occur with an Edge DLR. </p>                  <p><em>Workaround</em>: To clear the counters, execute the                    "clear ip bgp neighbor" command.</p>                  <!-- DOCNOTES: 1676328  6.2.3 -->                  <p><strong>Issue 1676085: Enabling Edge HA will fail if                      resource reservation fails</strong> </p>                  <p>Starting with NSX for vSphere 6.2.3, enabling high                    availability on an existing Edge will fail when sufficient                    resources cannot be reserved for the second Edge VM                    appliance. The configuration will roll back to the last                    known good configuration. In previous releases, if HA is                    enabled after Edge deployment and resource reservation                    fails, the Edge VM still is created.</p>                  <p><em>Workaround</em>: This is an expected change in                    behavior.</p>                  <!-- DOCNOTES: 1646454 6.2.3 -->                  <p><strong>Issue 1656713: IPsec Security Policies (SPs)                      missing on the NSX Edge after HA failover, traffic cannot                      flow over tunnel</strong> </p>                  <p> The <strong> Standby &gt; Active </strong> switchover                    will not work for traffic flowing on IPsec tunnels.                  </p>                  <p><em>Workaround</em>: Disable/Enable IPsec after the NSX                    Edge switchover. </p>                  <p></p>                  <!-- DOCNOTES: 1635240  6.2.3 -->                  <p><strong>Issue 1588450: NSX Edge virtual machine do not                      failover during a vSphere HA event</strong> </p>                  <p> The issue occurs when the NSX edge virtual machine is                    configured after vSphere High Availability (HA) has been                    configured.                    When the NSX Edge virtual machine is configured, it is added                    to the ESXi Auto Shutdown/Start up configuration.                    The NSX Edge virtual machine is then removed from the                    vSphere HA protected list when a power off event is received                    from the ESXi host. </p>                  <p><em>Workaround</em>: Refer to the <a target="_blank" href="http://kb.vmware.com/kb/2143998">VMware                      knowledge base article 2143998. </a></p>                  <!-- DOCNOTES:  1675705 6.2.3 -->                  <p><strong>Issue 1624663: After clicking "Configure Advanced                      Debugging" refreshes the VC UI and the change does not                      persist</strong> </p>                  <p> After clicking the specific edge ID &gt; Configuration                    &gt; Action &gt; Configure Advanced Debugging causes the VC                    UI to refresh and the change does not persist</p>                  <p><em>Workaround</em>: Go directly to the Edge list menu,                    highlight the edge, and click Action &gt; Configure Advanced                    Debugging to continue with the changes.</p>                  <!-- DOCNOTES:  1626939 6.2.3 -->                  <p><strong>Issue 1354824: When an Edge VM becomes corrupted or                      becomes otherwise unreachable due to such reasons as a                      power failure, system events are raised when the health                      check from NSX Manager fails</strong> </p>                  <p> The system events tab will report "Edge Unreachability"                    events. The NSX Edges list may continue to report a Status                    of Deployed. </p>                  <p><em>Workaround</em>: Use the <em>https://NSX-Manager-IP-Address/api/4.0/edges/edgeId/status</em>                    API with <em>detailedStatus=true</em>. </p>                  <!-- DOCNOTES: 1656642 6.2.3 -->                  <p><strong>Issue 1556924: L3 connectivity loss with VXLAN                      would block error</strong> </p>                  <p> When DLR LIF's are configured on the host but underlying                    VXLAN layer is not fully prepared, connectivity through some                    of DLR LIF's may be affected. Some of the VMs belonging to                    DLR are not reachable. There might be <em>"Failed to Create                      VXLAN trunk status: Would block"</em> logs in <em>/var/log/vmkernel.log                      </em> file.</p>                  <p><em>Workaround</em>: You may delete the LIF's and recreate                    them. Another option is rebooting the affected ESX hosts. </p>                  <!-- DOCNOTES: 1647657 6.2.3 -->                  <p><strong>Issue 1647657: Show commands on an ESXi host with                      VDR display no more than 2000 routes per VDR instance</strong>                  </p>                  <p>Show commands on an ESXi host with VDR enabled will not                    show more than 2000 routes per VDR instance, although more                    than this maximum may be running. This issue is a display                    issue, and the data path will work as expected for all                    routes. </p>                  <p><em>Workaround</em>: No workaround. </p>                  <!-- DOCNOTES: 1642472 6.2.3 -->                  <p><strong>Issue 1634215: OSPF CLI commands output does not                      indicate whether routing is disabled</strong> </p>                  <p> When OSPF is disabled, routing CLI commands output does                    not show any message saying <em>"OSPF is disabled"</em>.                    The output is empty. </p>                  <p><em>Workaround</em>: The <em>show ip ospf</em> command                    will display the correct status.</p>                  <p></p>                  <!-- DOCNOTES: 1664138 6.2.3 -->                  <p><strong>Issue 1663902: Renaming an NSX Edge VM disrupts                      traffic flowing through the Edge</strong> </p>                  <!-- DOCNOTES: 1663142 6.2.3 -->                  <p><strong>Issue 1647739: Redeploying an Edge VM after a                      vMotion operation will cause the Edge or DLR VM to be                      placed back on the original cluster.                      </strong>                  </p>                  <p><em>Workaround</em>: To place the Edge VM in a different                    resource pool or cluster, use the NSX Manager UI to                    configure the desired location.                  </p>                  <!-- DOCNOTES: 1463856 6.2.3 -->                  <p><strong>Issue 1463856: When NSX Edge Firewall is enabled,                      existing TCP connections are blocked </strong> <br />                    TCP connections are blocked through the Edge statefull                    firewall as the initial three-way handshake cannot be seen.                  </p>                  <p><em>Workaround:</em>To handle such existing flows, do the                    following. Use the NSX REST API to enable the flag                    "tcpPickOngoingConnections" in the firewall global                    configuration. This switches the firewall from strict mode                    to lenient mode. Next, enable the firewall. Once existing                    connections have been picked up (this may take a few minutes                    after you enable the firewall), set the flag                    "tcpPickOngoingConnections" back to false to return the                    firewall to strict mode. (This setting is persistent.)                  </p>                  <p></p>                  <pre>PUT /api/4.0/edges/{edgeId}/firewall/config/global    &lt;globalConfig&gt;        &lt;tcpPickOngoingConnections&gt;true&lt;/tcpPickOngoingConnections&gt;    &lt;/globalConfig&gt;<p></p></pre>                  <p></p>                  <!-- DOCNOTES: 1658010 6.2.3-->                  <p><strong>Issue 1374523: Reboot ESXi, or run <em>[services.sh                        restart]</em> after installation of VXLAN VIB to make                      the VXLAN commands available using esxcli</strong> </p>                  <p> After installation of VXLAN VIB, you must reboot ESXi or                    run the <em>[services.sh restart] </em> command, so that                    the VXLAN commands becomes available using esxcli.</p>                  <p><em>Workaround</em>: Instead of using esxcli, use localcli.                  </p>                  <!-- DOCNOTES: 1656648 6.2.3 -->                  <p><strong>Issue 1604514: Editing/Configuring default gateway                      on an unmanaged DLR fails after clicking Publish</strong>                  </p>                  <p>When a default gateway is added to an unmanaged DLR, the                    publish will fail with error "Routing Distance is support                    only on NSX Edge version 6.2.0 and later with NSX Edge VMs                    deployed". This is due to the default admin distance "1"                    populated on the UI. </p>                  <p><em>Workaround</em>: Remove the admin distance "1" which is                    populated by default. </p>                  <p></p>                  <!-- DOCNOTES: 1656701 6.2.3 -->                  <p><strong>Issue 1642087: After modifying the                      securelocaltrafficbyip parameter value in the IPsec VPN                      Extension, forwarding to destination networks fails</strong>                  </p>                  <p>When using an NSX Edge Services Gateway, you experience                    this symptom:<br />                  </p>                  <ul>                    <li>After changing the securelocaltrafficbyip value to 0 in                      the NSX UI (Edit IPsec VPN screen), forwarding to a remote                      subnet of the IPsec VPN tunnel no longer works</li>                    <li>After changing this parameter, you no longer see the                      correct information for a remote subnet in the IP routing                      table</li>                  </ul>                  <p></p>                  <p><em>Workaround</em>: Disable and re-enable the IPSec VPN                    service. Then validate that the expected routing information                    is shown in the CLI and the UI.</p>                  <!-- DOCNOTES: 1643499 6.2.3-->                  <p><strong>Issue 1606785: NSX Edge load balancer may fill the                      /var/log/partition with the nagios.log file messages</strong><br />                    The <em>nagio.log</em> file for the NSX Edge load balancer                    may fill the /var/log/partition if the daily                    log rotation rate is not sufficient to reset the logs in                    time.                    <br />                  </p>                  <p><em>Workaround</em>: Write the <em>Nagios.log</em>                    messages to syslog.</p>                  <!-- DOCNOTES: 1525003 6.2.3-->                  <p><strong>Issue 1525003: Restoring an NSX Manager backup with                      an incorrect passphrase will silently fail as critical                      root folders cannot be accessed</strong><br />                  </p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1640921 6.2.3-->                  <p><strong>Issue 1637639: When using the Windows 8 SSL VPN                      PHAT client, the virtual IP is not assigned from the IP                      pool</strong><br />                    On Windows 8, the virtual IP address is not assigned as                    expected from the IP pool when a new IP address is assigned                    by the Edge Services Gateway or when the IP pool changes to                    use a different IP range.                  </p>                  <p><em>Workaround</em>: This issue occurs only on Windows 8.                    Use a different Windows OS to avoid experiencing this issue.</p>                  <!-- DOCNOTES: 1628769 6.2.3-->                  <p><strong>Issue 1628220: DFW or NetX observations are not                      seen on receiver side </strong><br />                    Traceflow may not show DFW and NetX observations on receiver                    side if switch port associated with the destination VNIC                    changed. It will not be fixed for vSphere 5.5 releases. For                    vSphere 6.0 and up, there is no such issue. </p>                  <p><em>Workaround</em>: Do not disable VNIC. Reboot VM. </p>                  <!-- DOCNOTES: 1534603 6.2.1-->                  <p><strong>Issue 1534603: IPsec and L2 VPN service status                      shows as down even when the service is not enabled </strong><br />                    Under the Settings tab in the UI, the L2 service status is                    displayed as down, however the API shows the L2 status as                    up.                    L2 VPN and IPsec service always shows as down in the                    Settings tab unless the UI page is refreshed.                    <br />                  </p>                  <p><em>Workaround</em>: Refresh the page.</p>                  <!-- DOCNOTES: 1562767 / 1529669 / 1505993 / 1529665-->                  <p><strong>Issue 1562767: Delays in connecting to NSX load                      balancer do not provide                      consistent connections over multiple VIPs</strong><br />                    When the load balancer is configured to use Source IP Hash                    load                    balancing, a connected client session receives a consistent                    connection to a backend server. The load balancer should                    also be                    able to provide, for a given connected client, consistent                    connections over multiple VIPs if those VIPs are backed by                    the                    same server pool. That is, when one backend server serves                    multiple VIPs, a given client's connection to one of that                    backend                    server's VIPs should guarantee that that client will use the                    same                    backend server when connecting to other VIPs served by that                    backend server. A known issue prevents the NSX load balancer                    from                    providing such consistent connections over multiple VIPs.                  </p>                  <!-- DOCNOTES: 1553600 -->                  <p><strong>Issue 1553600: Delays in connecting to RIB and FIB                      after assigning IP address to interface                    </strong><br />                    When you attempt to assign an IP address to an interface,                    typically, the interface information is updated immediately.                    However, when waiting for a polling event, you may observe a                    delay in seeing the assigned IP address. (The NSX logical                    router polls periodically to get changes in the interfaces.)                  </p>                  <!-- DOCNOTES: 1534799 / 1506174 6.2.1 -->                  <p><strong>Issue 1534799: Slow convergence when OSPF area                      border router with highest IP address is shut down </strong><br />                    Convergence takes a long time when the NSX-based, OSPF area                    border                    router (ABR) with highest IP address is shut down or                    rebooted.                    If an ABR that does not have the numerically highest IP                    address is                    shut down or rebooted, traffic converges quickly to another                    path.                    However, if the ABR with the highest IP address is shut down                    or                    rebooted, a multi-minute re-convergence time is seen. The                    OSPF process                    can be cleared manually to reduce the convergence time.                  </p>                  <!-- DOCNOTES: 1446327 / 1540097 6.2.1-->                  <p><strong>Issue 1446327: Some TCP-based applications may time                      out when connecting through NSX Edge</strong><br />                    The default TCP established connection inactivity timeout is                    3600                    seconds. The NSX Edge deletes any connections idle for more                    than                    the inactivity timeout and drops those connections.                  </p>                  <em>Workaround</em>:                  <ol>                    <li>If the application has a relatively long inactivity                      time, enable TCP keepalives on the hosts with                      keep_alive_interval set to less than 3600 seconds.</li>                    <li>Increase the Edge TCP inactivity timeout to greater than                      2 hours using the following NSX REST API. For example, to                      increase the inactivity timeout to 9000 seconds. NSX API                      URL:<br />                      <tt>/api/4.0/edges/{edgeId}/systemcontrol/config PUT                        Method                        &lt;systemControl&gt;&lt;property&gt;sysctl.net.netfilter.nf_conntrack_tcp_timeout_established=9000&lt;/property&gt;&lt;/systemControl&gt;</tt></li>                  </ol>                  <p></p>                  <!-- DOCNOTES: 1534602-->                  <p><strong>Issue 1534602: UI does not display Edge management                      plane mode                      (VIX/MSGBUS), and does not provide the option to change                      from VIX to                      MSGBUS</strong><br />                    When an Edge appliance is in VIX mode, it is not eligible to                    be                    selected for inclusion in DFW, and centralized CLI commands                    take much                    longer to run compared to MSGBUS mode<br />                    <em>Workaround</em>: Make sure that the cluster where the                    Edge is                    deployed is prepared for NSX and its "NSX Manager to                    Firewall Agent"                    is in "Up" state, and redeploy the Edge.</p>                  <!-- DOCNOTES: 1498243 -->                  <p><strong>Issue 1498243: Distributed logical router                      advertises incorrect                      next hop for default route when BGP neighbor filter is set                      to "DENY, ANY, OUT"</strong><br />                    With 'default originate' enabled on an NSX distributed                    logical router                    (DLR), setting a BGP neighbor filter of "DENY, ANY, OUT" on                    the DLR                    causes the DLR to advertise an incorrect next hop address                    for the default route.                    This error occurs only when a BGP neighbor filter is added                    with the following                    attributes:                  </p>                  <ul>                    <li>Action: DENY</li>                    <li>Network: ANY</li>                    <li>Direction: OUT</li>                  </ul>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1471561 / 1500711/1647824-->                  <p><strong>Issue 1471561: BGP/OSPF neighbor relationship is                      not established with directly connected routers                     </strong><br />                    Dynamic routing does not work as expected with directly                    connected routers when ECMP routes exist for that directly                    connected network.<br />                  </p>                  <p><em>Workaround</em>: Reboot Edge OR delete and re-create                    the associated vNIC interface.</p>                  <!-- DOCNOTES: 1089745 /1089238 base bug converted to RN bug, base issue Resolved Wont Fix-->                  <p><strong>Issue 1089745: Logical router LIF routes are                      advertised by upstream Edge Services Gateway even if                      logical router OSPF is disabled</strong><br />                    Upstream Edge Services Gateway will continue to advertise                    OSPF external LSAs learned from logical router connected                    interfaces even when logical router OSPF is disabled.<br />                  </p>                  <p><em>Workaround</em>: Disable redistribution of connected                    routes into OSPF manually and publish before disabling OSPF                    protocol. This ensures that routes are properly withdrawn.</p>                  <!-- DOCNOTES: 1498965-->                  <p><strong>Issue 1498965: Edge syslog messages do not reach                      remote syslog server </strong><br />                    Immediately after deployment, the Edge syslog server cannot                    resolve the hostnames for any configured remote syslog                    servers. <br />                  </p>                  <p><em>Workaround</em>: Configure remote syslog servers using                    their IP address, or use the UI to Force Sync the Edge.                  </p>                  <!-- DOCNOTES: 1494025-->                  <p><strong>Issue 1494025: Logical router DNS Client                      configuration settings are not fully applied after                      updating REST Edge API</strong><br />                  </p>                  <p><em>Workaround</em>: When you use REST API to configure DNS                    forwarder (resolver), perform the following steps:                  </p>                  <ol>                    <li>                      <p>Specify the DNS Client XML server's settings so that                        they match                        the DNS forwarder setting.</p>                    </li>                    <li>                      <p>Enable DNS forwarder, and make sure that the forwarder                        settings                        are same as the DNS Client server's settings specified                        in the XML                        configuration.</p>                    </li>                    <p></p>                  </ol>                  <!-- DOCNOTES: 1243112-->                  <p><strong>Issue 1243112: Validation and error message not                      present for invalid next hop in static route, ECMP enabled</strong><br />                    When trying to add a static route, with ECMP enabled, if the                    routing table does not contain a default route and there is                    an unreachable next hop in the static route configuration,                    no error message is displayed and the static route is not                    installed.                  </p>                  <p><em>Workaround</em>: None.</p>                  <p></p>                  <!-- DOCNOTES: 1288487 / 1281425-->                  <p><strong>Issue 1288487: If an NSX Edge virtual machine with                      one sub interface backed by a logical switch is deleted                      through the vCenter Web Client user interface, data path                      may not work for a new virtual machine that connects to                      the same port</strong><br />                    When the Edge virtual machine is deleted through the vCenter                    Web Client user interface (and not from NSX Manager), the                    VXLAN trunk configured on dvPort over opaque channel does                    not get reset. This is because trunk configuration is                    managed by NSX Manager.<br />                  </p>                  <p><em>Workaround</em>: Manually delete the VXLAN trunk                    configuration by following the steps below:<br />                  </p>                  <ol>                    <li>Navigate to the vCenter Managed Object Browser by typing                      the following in a browser window:<br />                      <code>https://<i>&lt;vc-ip&gt;</i>/mob?vmodl=1</code></li>                    <li>Click <strong>Content</strong>.</li>                    <li>Retrieve the dvsUuid value by following the steps below.</li>                    <ol type="a">                      <li>Click the rootFolder link (for example,                        group-d1(Datacenters)).</li>                      <li>Click the data center name link (for example,                        datacenter-1).</li>                      <li>Click the networkFolder link (for example, group-n6).</li>                      <li>Click the DVS name link (for example, dvs-1)</li>                      <li>Copy the value of uuid.</li>                    </ol>                    <li>Click <strong>DVSManager</strong> and then click <strong>updateOpaqueDataEx</strong>.                    </li>                    <li>In <i>selectionSet</i>, add the following XML.<br />                      <code>&lt;selectionSet xsi:type="DVPortSelection"&gt;<br />                            &lt;dvsUuid&gt;<i>value</i>&lt;/dvsUuid&gt; <br />                            &lt;portKey&gt;<i>value</i>&lt;/portKey&gt;                        &lt;!--port                        number of the DVPG where trunk vnic got connected--&gt;<br />                        &lt;/selectionSet&gt;</code></li>                    <li>In <i>opaqueDataSpec</i>, add the following XML<br />                      <code>&lt;opaqueDataSpec&gt;<br />                            &lt;operation&gt;remove&lt;/operation&gt;<br />                            &lt;opaqueData&gt;<br />      &lt;key&gt;com.vmware.net.vxlan.trunkcfg&lt;/key&gt;<br />                              &lt;opaqueData&gt;&lt;/opaqueData&gt;<br />                            &lt;/opaqueData&gt;<br />                        &lt;/opaqueDataSpec&gt; </code></li>                    <li>Set <strong>isRuntime</strong> to false.</li>                    <li>Click <strong>Invoke Method</strong>.</li>                    <li>Repeat steps 5 through 8 for each trunk port configured                      on the deleted Edge                      virtual machine.</li>                  </ol>                  <p></p>                  <!--=================================Security Services Issues==============================================================--->                  <!--=================================Security Services Issues==============================================================--->                  <!--=================================Security Services Issues==============================================================--->                  <h3><a name="securityservicesissues"></a>Security Services                    Known Issues</h3>                  <!-- DOCNOTES: 1741814 6.2.4 -->                  <p><strong>Issue 1704661: VMs lose network connectivity with                      the error: Failed to restore PF state : Limit exceeded</strong><br />                    After upgrading from NSX for vSphere 6.1.x to 6.2.4, you may                    experience these symptoms:<br />                  </p>                  <ul>                    <li>Some virtual machines lose network connectivity after                      vMotion.</li>                    <li>In the <em>/var/log/vmkernel.log</em> file of the ESXi                      host where the virtual machine is migrated to, you see                      entries similar to:</li>                    <ul>                      <li> 2016-07-28T09:07:00.764Z cpu21:33397)&lt;6&gt;host7:                        libfc: Link up on port ( 0)</li>                      <li>2016-07-28T09:07:00.766Z cpu11:1294844)Vmxnet3: 15253:                        Using default queue delivery for vmxnet3 for port                        0x2000065</li>                      <li>2016-07-28T09:07:00.767Z cpu11:1294844)PFImportState:                        unsupported version: 0</li>                      <li>2016-07-28T09:07:00.767Z cpu11:1294844)vsip                        VSIPDVFRestoreState:2059: Failed to restore PF state :                        Limit exceeded</li>                      <li>2016-07-28T09:07:00.767Z cpu11:1294844)WARNING:                        NetPort: 1579: failed to enable port 0x2000065: Failure</li>                      <li>2016-07-28T09:07:00.767Z cpu11:1294844)Vmxnet3: 16236:                        Port_Enable failed for port 0x2000065</li>                    </ul>                    <li>This issue occurs due to a known issue on the VSIP                      module where support for vMotion of virtual machines                      deployed in NSX for vSphere 6.1.x is broken.</li>                  </ul>                  <p></p>                  <p><em>Workaround</em>: This is a known issue affecting NSX                    for vSphere 6.2.4 releases. See <a target="_blank" href="http://kb.vmware.com/kb/2146171">VMware                      knowledge                      base article 2146171</a> for more information. </p>                  <p></p>                  <!-- DOCNOTES: 1732337 6.2.4 -->                  <p><strong>Issue 1732337/1724222: NSX Manager fails to push                      firewall rules to ESXi 6.0 P03 host</strong><br />                    NSX Manager fails to push firewall rules to ESXi 6.0 P03                    host, and NSX Edge health check fails as vsfwd connection is                    closed. This is a known issue affecting VMware NSX for                    vSphere 6.2.x with ESXi 6.0 P03 (Build 4192238). This issue                    occurs when /dev/random call is blocked which affects NSX                    operation on password generation.                  </p>                  <p><em>Workaround</em>: Contact VMware technical support. See                    <a target="_blank" href="http://kb.vmware.com/kb/2146873">VMware                      knowledge                      base article 2146873</a> for more information.</p>                  <p></p>                  <!-- DOCNOTES: 1720234 6.2.4 -->                  <p><strong>Issue 1620460: NSX fails to prevent users from                      creating rules in                      Service Composer rules section</strong><br />                    In the vSphere Web Client, the Networking and Security:                    Firewall                    interface fails to prevent users from adding rules to the                    Service                    Composer rules section. Users should be permitted to add                    rules above/below                    the Service Composer section, but not inside it.                  </p>                  <p><em>Workaround</em>: Do not use the "+" button at the                    global rule level to add                    rules to the Service Composer rules section.</p>                  <p></p>                  <!-- DOCNOTES: 1710415 6.2.4 -->                  <p><strong>Issue 1682552: Threshold events for CPU/Memory/CPS                      for Distributed Firewall (DFW) are not reported</strong><br />                    Even when the DFW thresholds for CPU/Memory/CPS are set for                    reporting, the threshold events are not                    reported when the thresholds are crossed.                  </p>                  <p><em>Workaround</em>: </p>                  <ul>                    <li>Login to each ESXi host and restart the DFW controlplane                      process by running the following command: </li>                    <em>/etc/init.d/vShield_Stateful_Firewall restart</em>                    <li>Verify the status using the following command: </li>                    <em>/etc/init.d/vShield_Stateful_Firewall status</em> <br />                    <li>The result similar to following is displayed:</li>                    <em>"vShield-Stateful-Firewall is running"</em>                    <p>                    </p>                  </ul>                  <strong>Note</strong>: You should be cautious while performing                  this operation as this will push all DFW rules to all the                  filters again. If there are lot of rules, it might take some                  time to enforce them on all the filters.                  <p></p>                  <p></p>                  <p></p>                  <!-- DOCNOTES:  1738698 6.2.4 -->                  <p><strong>Issue 1707931: Order of distributed firewall rules                      changes when service policies defined in Service Composer                      are present, and a firewall rule is modified or published                      with a filter applied in the Firewall UI</strong><br />                    Changing the order, adding or deleting service policies                    created in Service Composer after one or more                    publish operations are made from the Networking &amp;                    Security &gt; Firewall UI will cause the order of                    firewall rules to change and may result in unintended                    consequences. <br />                  </p>                  <p><em>Workaround</em>: The following workarounds are                    available: <br />                  </p>                  <ul>                    <li> Synchronize Service Composer rules with firewall rules                      by selecting Synchronize Firewall Rules from the Actions                      menu in the Security Policies tab.</li>                    <li>Use filters only to view a set of rules and not to                      update a rule set.</li>                    <li> Perform a full publish before using a filter via the                      REST API <em> /api/4.0/firewall/globalroot-0/config PUT</em>                      or via the UI by updating multiple sections (not a single                      section) to ensure that the global firewall configuration                      is changed.</li>                  </ul>                  <p></p>                  <!-- DOCNOTES: 1720224 6.2.4 -->                  <p><strong>Issue 1717635: Firewall configuration operation                      fails if more than one cluster is present in environment                      and changes are done in parallel </strong><br />                    In an environment with multiple clusters, if two                    or more users modify the firewall configuration continuously                    in a                    tight loop. (for example, Add/Delete sections or rules),                    some operations                    fail, and the user will see an API response similar to:<br />                    &lt;?xml version="1.0" encoding="UTF-8"?                    &gt; <br />                    neutron-server.log.1:70282:2016-08-23 17:58:23.429 30787                    ERROR vmware_nsx.plugins.nsx_v.plugin <br />                    &lt;error&gt; <br />                    &lt;details&gt;                    org.hibernate.exception.GenericJDBCException: Could not                    execute JDBC batch update; nested exception is                    javax.persistence.PersistenceException:                    org.hibernate.exception.GenericJDBCException: Could not                    execute JDBC                    batch update                    &lt;/details&gt; <br />                    &lt;errorCode&gt;258 <br />                    &lt;/errorCode&gt; <br />                    &lt;/error&gt; <br />                  </p>                  <p><em>Workaround</em>: Avoid concurrent modification of the                    firewall configuration.                  </p>                  <!-- DOCNOTES: 1717997 6.2.4 -->                  <p><strong>Issue 1717994: Distributed Firewall (DFW) Status                      API query reports 500 internal server error intermittently</strong><br />                    If the DFW status API query is issued while adding a new                    host into a host prepared cluster, the API query fails with                    500 internal server error for few attempts, and then returns                    correct response once the host starts to get VIBs installed.<br />                  </p>                  <p><em>Workaround: Do not use the DFW status API query until                      the new host is prepared successfully.</em>                  </p>                  <!-- DOCNOTES: 1686041 6.2.4 -->                  <p><strong>Issue 1686036: Firewall rules cannot be added,                      edited, or removed when default section is deleted </strong><br />                    If the default Layer2 or Layer3 section is deleted,                    publishing a firewall rule may fail.<br />                  </p>                  <p><em>Workaround</em>: Do not delete the default rule. If the                    configuration with default rule was saved                    in draft, perform the following steps:<br />                  </p>                  <ol>                    <li>Delete the complete firewall configuration using                      following DELETE API call.</li>                    <em>https://&lt;NSX Manager                      IP&gt;/api/4.0/firewall/globalroot-0/config</em> <br />                    This will restore the default section on the firewall.                    <li>Load the saved draft of firewall rules with default                      section to the firewall.</li>                  </ol>                  <p></p>                  <!-- DOCNOTES:1632235 6.2.3 -->                  <p><strong>Issue 1632235: During Guest Introspection                      installation, network drop down list displays "Specified                      on Host" only</strong><br />                    When installing Guest Introspection with the NSX                    anti-virus-only license and vSphere Essential or Standard                    license,                    the network drop down list will display only the existing                    list of DV port groups.                    This license does not support DVS creation.<br />                  </p>                  <p><em>Workaround: Before installing Guest Introspection on a                      vSphere host with one of these licenses, first specify the                      network in the "Agent VM Settings" window.</em>                  </p>                  <!-- DOCNOTES: 1678797  6.2.3 -->                  <p><strong>Issue 1652155: Creating or migrating firewall rules                      using REST APIs may fail under certain conditions and                      report HTTP 404 error</strong> </p>                  <p>Adding or migrating firewall rules using REST APIs is not                    supported under these conditions:<br />                  </p>                  <ul>                    <li>Creating firewall rules as a bulk operation when the                      autosavedraft=true is set.</li>                    <li>Adding firewall rules in sections concurrently. </li>                  </ul>                  <p></p>                  <p><em>Workaround</em>: Set the autoSaveDraft parameter to                    false in the API call when performing bulk firewall rule                    creation or migration.</p>                  <p></p>                  <!-- DOCNOTES: 1509687-->                  <p><strong>Issue 1509687: URL length supports up to 16000                      characters when assigning a single security tag to many                      VMs at a time in one API call</strong><br />                    A single security tag cannot be assigned to a large number                    of VMs simultaneously with a single API if the URL                    length is more than 16,000 characters.<br />                  </p>                  <p><em>Workaround:</em>To optimize performance, tag up to 500                    VMs in a single call.                  </p>                  <!-- DOCNOTES: 1662534 6.2.3 -->                  <p><strong>Issue 1662020: Publish operation may fail resulting                      in an error message "Last publish failed on host <em>host                        number</em>" on DFW UI in General and Partner Security                      Services sections</strong> </p>                  <p>After changing any rule, the UI displays "Last publish                    failed on host <em>host number</em>". The hosts listed on                    the UI may not have correct version of firewall rules,                    resulting in lack of security and/or network disruption. </p>                  <p>The problem is usually seen in the following scenarios:</p>                  <p> </p>                  <ul>                    <li>After upgrade from older to latest NSXv version. </li>                    <li>Move a host out of cluster and move it back in. </li>                    <li>Move a host from one cluster to another. </li>                  </ul>                  <p><em>Workaround</em>: To recover, you must force sync the                    affected clusters (firewall only). </p>                  <!-- DOCNOTES: 1635344 6.2.3 Base PR wont fix-->                  <p><strong>Issue 1481522: Migrating firewall rule drafts from                      6.1.x to 6.2.3 is not supported as the drafts are not                      compatible between the releases</strong></p>                  <p> <em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1644659 6.2.3 -->                  <p><strong>Issue 1491046: IPv4 IP address does not get auto                      approved when                      SpoofGuard policy is set to Trust On First Use (TOFU) in                      VMware NSX for vSphere 6.2.x </strong></p>                  <p> <em>Workaround</em>: See <a target="_blank" href="http://kb.vmware.com/kb/2144649">VMware
                      knowledge base article 2144649</a>.</p>                  <!-- DOCNOTES:  1656708 6.2.3 -->                  <p><strong>Issue 1628679: With identity-based firewall, the VM                      for removed users continues to be part of the security                      group</strong></p>                  <p>When a user is removed from a group on the AD server, the                    VM where the user is logged-in continues to be a part of the                    security-group. This retains firewall policies at the VM                    vnic on the hypervisor, thereby granting the user full                    access to services.</p>                  <p> <em>Workaround</em>: None. This behavior is expected by                    design.</p>                  <!-- DOCNOTES: 1662534 6.2.3 -->                  <p><strong>Issue 1662020: In a cross vCenter setup, an error                      message <em>"Last publish failed on host 10.156.221.88"</em>                      appears on DFW UI in General and Partner Security Services                      tabs</strong> </p>                  <p>The error message appears when the associated NIC for the                    rules is not present.                  </p>                  <p><em>Workaround</em>: None.                    <!-- DOCNOTES: 1637939 6.2.3 -->                  </p>                  <p><strong>Issue 1637939: MD5 certificates are not supported                      while deploying hardware gateways</strong> </p>                  <p> While deploying hardware gateway switches as VTEPs for                    logical L2 VLAN to VXLAN bridging, the physical switches                    support at minimum SHA1 SSL certificates for OVSDB                    connection between the NSX controller and OVSDB switch.</p>                  <p><em>Workaround</em>: None. </p>                  <!-- DOCNOTES: 1637943 6.2.3 -->                  <p><strong>Issue 1637943: No support for hybrid or multicast                      replication modes for VNIs that have a hardware gateway                      binding</strong> </p>                  <p>Hardware gateway switches when used as VTEPs for L2                    VXLAN-to-VLAN bridging support Unicast replication mode                    only.</p>                  <p><em>Workaround</em>: Use Unicast replication mode only. </p>                  <!-- DOCNOTES: 1462027/1477176-->                  <p><strong>Issue 1462027: In cross vCenter NSX deployments,                      multiple versions of                      saved firewall configurations get replicated to secondary                      NSX Managers</strong><br />                    Universal Sync saves multiple copies of universal                    configurations on                    secondary NSX Managers. The list of saved configurations                    contains                    multiple drafts created by the synchronizing across NSX                    Managers with the same name                    and at the same time or with a time difference of 1 second.                  </p>                  <p><em>Workaround</em>: Run the API call to delete duplicate                    drafts.                  </p>                  <p><tt>DELETE :                      https://&lt;nsxmgr-ip&gt;/api/4.0/firewall/config/drafts/<draft-id></draft-id></tt></p>                  <p>Find the drafts to be deleted by viewing all drafts:</p>                  <p><tt>GET:                      https://&lt;nsxmgr-ip&gt;/api/4.0/firewall/config/drafts</tt></p>                  <p>In the following sample output, drafts 143 and 144 have the                    same                    name and were created at the same time and are therefore                    duplicates.                    Likewise, drafts 127 and 128 have the same name are off by 1                    second and                    are also duplicates.</p>                  <p>                  </p>                  <pre>&lt;firewallDrafts&gt;    &lt;firewallDraft id="<strong>144</strong>" name="AutoSaved_Wednesday, August 5, 2015 11:08:40 PM GMT" timestamp="1438816120917"&gt;        &lt;description&gt;Auto saved configuration&lt;/description&gt;        &lt;preserve&gt;false&lt;/preserve&gt;        &lt;user&gt;replicator-1fd96022-db14-434d-811d-31912b1cb907&lt;/user&gt;        &lt;mode&gt;autosaved&lt;/mode&gt;    &lt;/firewallDraft&gt;    &lt;firewallDraft id="<strong>143</strong>" name="AutoSaved_Wednesday, August 5, 2015 11:08:40 PM GMT" timestamp="1438816120713"&gt;        &lt;description&gt;Auto saved configuration&lt;/description&gt;        &lt;preserve&gt;false&lt;/preserve&gt;        &lt;user&gt;replicator-1fd96022-db14-434d-811d-31912b1cb907&lt;/user&gt;        &lt;mode&gt;autosaved&lt;/mode&gt;    &lt;/firewallDraft&gt;   &lt;firewallDraft id="<strong>128</strong>" name="AutoSaved_Wednesday, August 5, 2015 9:08:02 PM GMT" timestamp="1438808882608"&gt;        &lt;description&gt;Auto saved configuration&lt;/description&gt;        &lt;preserve&gt;false&lt;/preserve&gt;        &lt;user&gt;replicator-1fd96022-db14-434d-811d-31912b1cb907&lt;/user&gt;        &lt;mode&gt;autosaved&lt;/mode&gt;    &lt;/firewallDraft&gt;    &lt;firewallDraft id="<strong>127</strong>" name="AutoSaved_Wednesday, August 5, 2015 9:08:01 PM GMT" timestamp="1438808881750"&gt;        &lt;description&gt;Auto saved configuration&lt;/description&gt;        &lt;preserve&gt;false&lt;/preserve&gt;        &lt;user&gt;replicator-1fd96022-db14-434d-811d-31912b1cb907&lt;/user&gt;        &lt;mode&gt;autosaved&lt;/mode&gt;    &lt;/firewallDraft&gt;&lt;/firewallDrafts&gt;</pre>                  <p></p>                  <!-- DOCNOTES: 1449611/1466909-->                  <p><strong>Issue 1449611: When a firewall policy in the                      Service Composer is out of                      sync due to a deleted security group, the firewall rule                      cannot be                      fixed in the UI </strong><br />                  </p>                  <p><em>Workaround</em>: In the UI, you can delete the invalid                    firewall                    rule and then add it again. Or, in the API, you can fix the                    firewall                    rule by deleting the invalid security group. Then                    synchronize the                    firewall configuration: Select <strong>Service                      Composer</strong>: <strong>Security Policies</strong>,                    and for each                    security policy that has associated firewall rules,                    click <strong>Actions</strong> and select <strong>Synchronize                      Firewall                      Config</strong>. To prevent this issue, modify firewall                    rules so that                    they do not refer to security groups before deleting the                    security                    groups.</p>                  <p></p>                  <!-- DOCNOTES: 1557880 6.2.1-->                  <p><strong>Issue 1557880: Layer 2 (L2) rules may be missing if                      the MAC address of a VM used in the rules is modified</strong><br />                    Because L2 rule optimization is ON by default, L2 rules with                    both source and destination fields specified (other than                    "any") will be applied to vNICs(or filters) only if the vNIC                    MAC address matches the source or destination MAC address                    list. Hosts with VMs not matching the source or destination                    MAC addresses will not have those L2 rules applied. <br />                  </p>                  <p><em>Workaround</em>: To have L2 rules applied to all                    vNICs(or filters), set one of the source or destination                    fields to "any".</p>                  <!-- DOCNOTES: 1505316 / 1616333 - 6.2.2 and 6.1.6 (added in glibc edition of RNs) -->                  <p><strong>Issue 1505316: NSX NetX rule not published to host                      when selected service                      is a Service Group</strong><br />                    When creating an L3 Redirection rule in the Partner Services                    tab in                    DFW, selecting a Service Group doesn't create the rule                    correctly.                  </p>                  <p><em>Workaround</em>: Use individual services when creating                    the rule                    instead of using a Service Group.                  </p>                  <!-- DOCNOTES: 1496273/1496686-->                  <p><strong>Issue 1496273: UI allows creation of in/out NSX                      firewall rules that cannot be applied to Edges</strong><br />                    The web client incorrectly allows creation of an NSX                    firewall rule applied to one or more NSX Edges when the rule                    has traffic                    traveling in the 'in' or 'out' direction and when PacketType                    is IPV4 or IPV6. The UI should not allow creation of such                    rules,                    as NSX cannot apply them to NSX Edges.<br />                  </p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1493611 6.2.1 (added in 2nd edition of RNs) -->                  <p><strong>Issue 1493611: No connectivity on VLAN ID 0 in L2                      VPN</strong><br />                    NSX L2 VPN configuration incorrectly allows the user to                    configure an                    L2 VPN with VLAN ID 0. Once configured, no traffic can flow                    on this                    VPN.                  </p>                  <p><em>Workaround</em>:                    Workaround: Use a valid VLAN ID in the range from 1 to 4094.                  </p>                  <!-- DOCNOTES: 1534574 6.2.1-->                  <p><strong>Issue 1534574: There is no support for Cipher 3C                      (SHA-256) encryption algorithms for SSLVPN-Plus </strong><br />                    <!-- DOCNOTES: 1557924 6.2.1 -->                  </p>                  <p><strong>Issue 1557924: Universal logical switch is allowed                      to be consumed in the appliedTo field of a local DFW rule</strong><br />                    When a universal logical switch is used as a security group                    member, the DFW rule can use that security group in                    AppliedTo field. This indirectly applies the rule on the                    universal logical switch, which should not be allowed                    because it may cause unknown behavior of those rules.</p>                  <p><em>Workaround</em>: None. </p>                  <p></p>                  <!-- DOCNOTES: 1559971  6.2.1 -->                  <p><strong>Issue 1559971: Cross-vCenter NSX firewall exclude                      list not published if firewall is                      disabled on one cluster</strong><br />                    In cross-vCenter NSX, firewall exclude list is not published                    to any                    cluster when the firewall is disabled on one of the                    clusters.</p>                  <p><em>Workaround</em>: Force sync the affected NSX Edges.</p>                  <!-- DOCNOTES: 1407920 / 1522282 6.2.1 -->                  <p><strong>Issue 1407920: Firewall rule republish fails after                      DELETE API is used</strong><br />                    If you delete the entire firewall configuration through the                    DELETE API                    method and then try to republish all the rules from a                    previously saved                    firewall rules draft, then the rule publish will fail. </p>                  <!-- DOCNOTES: 1534585-->                  <p><strong>Issue 1534585: Publishing Distributed Firewall                      (DFW) rules fails after                      referenced object is deleted in VMware NSX for vSphere                      6.1.x and                      6.2.x</strong><br />                  </p>                  <p><em>Workaround</em>: If this occurs,                    see <a target="_blank" href="http://kb.vmware.com/kb/2126275">knowledge                      base article 2126275.</a></p>                  <p></p>                  <!-- DOCNOTES: 1494718-->                  <p><strong>Issue 1494718: New universal rules cannot be                      created, and existing                      universal rules cannot be edited from the flow monitoring                      UI </strong><br />                  </p>                  <p><em>Workaround</em>: Universal rules cannot be added or                    edited from                    the flow monitoring UI. EditRule will be automatically                    disabled.</p>                  <p></p>                  <!-- DOCNOTES: 1442379/1467060-->                  <p><strong>Issue 1442379: Service composer firewall                      configuration out of sync</strong><br />                    In the NSX service composer, if any firewall policy is                    invalid (for                    example of you deleted a security group that was currently                    in use in a                    firewall rule), deleting or modifying another firewall                    policy causes                    the service composer to become out of sync with the error                    message <tt>Firewall configuration is not in sync</tt>.                  </p>                  <p><em>Workaround</em>: Delete any invalid firewall rules and                    then                    synchronize the firewall configuration. Select <strong>Service                      Composer</strong>: <strong>Security Policies</strong>,                    and for each                    security policy that has associated firewall rules,                    click <strong>Actions</strong> and select <strong>Synchronize                      Firewall                      Config</strong>. To prevent this issue, always fix or                    delete invalid                    firewall configurations before making further firewall                    configuration                    changes.</p>                  <p></p>                  <!-- DOCNOTES: 1301627 / 1066277 -->                  <p><strong>Issue 1301627: Security policy name does not allow                      more than 229 characters</strong><br />                    The security policy name field in the Security Policy tab of                    Service Composer can accept up to 229 characters. This is                    because policy names are prepended internally with a prefix.<br />                  </p>                  <p><em>Workaround</em>: None.</p>                  <!-- DOCNOTES: 1443344 -->                  <p><strong>Issue 1443344: Some versions of 3rd-party Networks                      VM-Series do not work with NSX Manager default settings</strong><br />                    Some NSX 6.1.4 components disable SSLv3 by default. Before                    you                    upgrade, please check that all third-party solutions                    integrated with                    your NSX deployment do <em>not</em> rely on SSLv3                    communication. For                    example, some versions of the Palo Alto Networks VM-series                    solution                    require support for SSLv3, so please check with your vendors                    for their                    version requirements.</p>                  <!-- DOCNOTES: 1438859 / 1435948-->                  <p><strong>Issue 1438859: In upgraded NSX installations,                      publishing a firewall rule may result in <tt>Null Pointer                        exception</tt> in Web Client</strong><br />                    In upgraded NSX installations, publishing a firewall rule                    may result in a <tt>Null Pointer exception</tt> in the UI.                    The rule changes are                    saved. This is a display issue only.</p>                  <!--======================================Monitoring Services Known Issues=============================================================--->                  <!--======================================Monitoring Services Known Issues=============================================================--->                  <!--======================================Monitoring Services Known Issues=============================================================--->                  <h3><a name="monitoringissues"></a>Monitoring Services Known                    Issues</h3>                  <!-- DOCNOTES: 1670261 6.2.3 -->                  <p><strong>Issue 1655593: Missing status on NSX Dashboard when                      logging in as Auditor or Security Admin roles</strong>                  </p>                  <p>When viewing NSX Dashboard as Auditor or Security Admin, a                    error message "User is not authorized to access object ...                    and feature ... Please check object access scope and feature                    permissions for the user" appears. For example, Auditor may                    not be able to see "Logical Switch Status" from the                    Dashboard.</p>                  <p><em>Workaround</em>: None.</p>                  <!--=====================================Solution Interoperability Issues==============================================================--->                  <!--=====================================Solution Interoperability Issues==============================================================--->                  <!--=====================================Solution Interoperability Issues==============================================================--->                  <h3><a name="interopissues"></a>Solution Interoperability                    Issues</h3>                  <!-- DOCNOTES: 1659559 6.2.3 -->                  <p><strong>Issue 1568861: The NSX Edge deployment fails during                      any edge deployment from a VCD cell that does not own the                      VC listener</strong>                  </p>                  <p> The NSX Edge deployment fails during any Edge deployment                    from a VCD cell that does not own the VC listener. Also, NSX                    Edge actions, including a redeploy, fail from VCD.                  </p>                  <p><em>Workaround</em>: Deploy an NSX Edge from the VCD cell                    which                    owns the VC listener.</p>                  <!-- DOCNOTES: 1656641 6.2.3 -->                  <p><strong>Issue 1530360: After an NSX Manager VM has failed                      over, Site Recovery Manager (SRM) incorrectly reports a                      timeout error</strong></p>                  <p>When a NSX Manager VM is failed over, SRM incorrectly                    reports a timeout error waiting for VMware Tools. In this                    case, VMware Tools actually is up and running within the 300                    second timeout. </p>                  <p><em>Workaround</em>: None.</p>                  <p></p>                  <!--=====================================Controller known Issues==============================================================--->                  <!--=====================================Controller known Issues==============================================================--->                  <!--=====================================Controller known Issues==============================================================--->                  <h3><a name="controllerissues"></a>NSX Controller Known Issues</h3>                  <!-- DOCNOTES: 1517108 6.2.3 -->                  <p><strong>Issue 1516207: Controller(s) may become isolated                      after                      IPsec communication is re-enabled on in NSX controller                      cluster</strong> </p>                  <p> If an NSX controller cluster is set to allow                    controller-to-controller communications in the clear (IPsec                    is                    disabled), and IPsec-based communication is later                    re-enabled, one or more controllers may become isolated from                    the                    cluster majority due to a mismatched pre-shared key ("PSK").                    When this                    occurs, the NSX API may become unable to change the IPsec                    settings of                    the controllers. </p>                  <p><em>Workaround</em>:                  </p>                  <p> Follow these steps to address this issue: </p>                  <ol>                    <li>                      <p>Disable IPSec using the NSX API. </p>                      <p><em>PUT: /2.0/vdn/controller/node                          &lt;controllerNodeConfig&gt;<br />                          &lt;ipSecEnabled&gt;false&lt;/ipSecEnabled&gt;<br />                          &lt;/controllerNodeConfig&gt;</em></p>                    </li>                    <li>                      <p>Re-enable IPsec using the NSX API.</p>                      <p><em>PUT: /2.0/vdn/controller/node                          &lt;controllerNodeConfig&gt;<br />                          &lt;ipSecEnabled&gt;true&lt;/ipSecEnabled&gt;<br />                          &lt;controllerNodeConfig&gt;</em></p>                    </li>                  </ol>                  <p>Follow these best practices to avoid this issue:</p>                  <ul>                    <li>Always use the NSX API to disable IPsec. Using the NSX                      Controller CLI to disable IPsec is not supported.</li>                    <li>Always verify that all controllers are active before you                      use the API to change the IPsec setting.</li>                  </ul>                  <!-- DOCNOTES: 1306408 / 1492179 / 1534590-->                  <p><strong>Issue 1306408: NSX Controller logs must be                      downloaded sequentially</strong><br />                    NSX Controller logs cannot be downloaded simultaneously.                    Even when downloading from multiple controllers, you must                    wait for the download from the current controller to finish                    before you start the download from the next controller. Note                    also that you cannot cancel a log download once it has                    started.<br />                  </p>                  <p><em>Workaround</em>: Wait for the current controller log                    download to finish before starting another log download.</p>                  <!--========================================Resolved Issues==============================================================--->                  <!--========================================Resolved Issues==============================================================--->                  <!--========================================Resolved Issues==============================================================--->                  <!--========================================Resolved Issues==============================================================--->                  <!--========================================Resolved Issues==============================================================--->                  <a name="resolvedissues"></a>                  <h2>Resolved Issues</h2>                  <p>See what's resolved in <a href="#fixedin625">6.2.5</a>, <a                      href="#fixedin624">6.2.4</a>,                    or in <a href="#fixedin62x">6.2.3 and earlier</a>.                  </p>                  <!--====================================6.2.5 Fixedbugs==============================================================--->                  <!--====================================6.2.5 Fixedbugs==============================================================--->                  <!--====================================6.2.5 Fixedbugs==============================================================--->                  <!--====================================6.2.5 Fixedbugs==============================================================--->                  <h3><a name="fixedin625"></a>Issues Resolved in NSX 6.2.5</h3>                  <ul>                    <i>{placeholder}</i>                    <li>Issue ...</li>                  </ul>                  <!--====================================6.2.4 Fixedbugs==============================================================--->                  <!--====================================6.2.4 Fixedbugs==============================================================--->                  <!--====================================6.2.4 Fixedbugs==============================================================--->                  <!--====================================6.2.4 Fixedbugs==============================================================--->                  <h3><a name="fixedin624"></a>Issues Resolved in NSX 6.2.4</h3>                  <p>6.2.4 resolved issues are grouped as follows:</p>                  <ul>                    <li><a href="#generalfixedbugs0624" name="&amp;lpos=apps_scode : 10">General                        Resolved Issues in NSX 6.2.4</a></li>                    <li><a href="#installupgradefixedbugs0624" name="&amp;lpos=apps_scode : 10">Installation                        and Upgrade Resolved Issues in NSX 6.2.4</a></li>                    <li><a href="#nsxmgrfixedbugs0624" name="&amp;lpos=apps_scode : 10">NSX                        Manager Resolved Issues in NSX 6.2.4</a></li>                    <li><a href="#nsxnetworkfixedbugs0624" name="&amp;lpos=apps_scode : 10">Logical                        Networking Resolved Issues in NSX 6.2.4</a></li>                    <li><a href="#nsxedgefixedbugs0624" name="&amp;lpos=apps_scode : 10">Networking                        and Edge Services Resolved Issues in NSX 6.2.4</a></li>                    <li><a href="#nsxsecurityfixedbugs0624" name="&amp;lpos=apps_scode : 10">Security                        Services Resolved Issues in NSX 6.2.4</a></li>                    <li><a href="#monitoringfixedbugs0624" name="&amp;lpos=apps_scode : 10">Monitoring                        Services Resolved Issues in NSX 6.2.4</a></li>                    <li><a href="#interopfixedbugs0624" name="&amp;lpos=apps_scode : 13">Solution                        Interoperability Resolved Issues in NSX 6.2.4</a></li>                  </ul>                  <!--====================================General Known Fixedbugs 0624 ==================================================--->                  <!--====================================General Known Fixedbugs==============================================================--->                  <!--====================================General Known Fixedbugs==============================================================--->                  <!--====================================General Fixedbugs -->                  <h3><a name="generalfixedbugs0624"></a>General Resolved Issues                    in 6.2.4</h3>                  <ul>                    <!-- DOCNOTES: 1702934 - 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1696192: NTP sync issues on NSX                          Manager</strong><br />                        A newer version of fcron was introduced in NSX 6.2.3.                        There are no environment variables defined in the                        fcrontab which means that the environment is not                        initialized for fcron run jobs. The script is unable to                        locate the ntpdate command because the $PATH is empty. <em>This                          has been fixed in 6.2.4.</em></p>                    </li>                  </ul>                  <!--====================================sectbreak_install  Fixedbugs  0624  ================================================--->                  <!--====================================sectbreak_install  Fixedbugs==============================================================--->                  <!--====================================sectbreak_install  Fixedbugs==============================================================--->                  <h3><a name="installupgradefixedbugs0624"></a>Install and                    Upgrade Resolved Issues in 6.2.4</h3>                  <ul>                    <li>                      <!-- DOCNOTES: 1709004 / 1710454 / 1703247 / fix 2016-08-23-23:11 -->                      <p><strong>Fixed issue 1710454: HA Dead Time inconsistency                          between newly deployed and upgraded DLRs</strong><br />                        This issue occurred because the newly upgraded DLRs HA                        Dead Time are explicitly being changed from 15 seconds                        to 6 seconds during upgrade. <br />                        <em>Workaround</em>: Refer to the <a target="_blank" href="http://kb.vmware.com/kb/2146714">                          VMware knowledge base article 2146714</a>.<em> This                          has been fixed in 6.2.4.</em>                      </p>                    </li>                  </ul>                  <!--====================================sectbreak_manager Fixedbugs  0624 ==============================================================--->                  <!--====================================sectbreak_manager Fixedbugs==============================================================--->                  <!--====================================sectbreak_manager Fixedbugs==============================================================--->                  <h3><a name="nsxmgrfixedbugs0624"></a>NSX Manager Resolved                    Issues in 6.2.4</h3>                  <ul>                    <!-- DOCNOTES: 1681470  6.2.3 -->                    <li>                      <p><strong>Fixed issue 1668519: High CPU utilization on                          NSX Manager</strong><br />                        NSX Manager may experience sustained high CPU,                        especially after a reboot,                        when the purgetask process must process or clean up a                        very large volume of job entries on the NSX Manager                        database.<br />                        <em>Workaround</em>: Contact VMware technical support.                        See <a target="_blank" href="https://kb.vmware.com/kb/2145934">VMware                          knowledge base article 2145934</a>.<em> This has been                          fixed in 6.2.4.</em></p>                    </li>                    <!-- DOCNOTES: 1697798 6.2.4 -->                    <li>                      <p><strong>Fixed issue 1603954: NSX Manager displays                          memory utilization at almost 100% constantly</strong><br />                        Reboot of NSX Manager drops the memory utilization to                        significantly lower than 100%, however over time the                        utilization value raises back up to 100% and the display                        remains at that level. <em> This has been fixed in                          6.2.4.</em>                      </p>                    </li>                  </ul>                  <!--====================================sectbreak_logical Fixedbugs 0624 ==============================================================--->                  <!--====================================sectbreak_logical Fixedbugs==============================================================--->                  <!--====================================sectbreak_logical Fixedbugs==============================================================--->                  <h3><a name="nsxnetworkfixedbugs0624"></a>Logical Networking                    Resolved Issues in 6.2.4</h3>                  <ul>                    <!-- DOCNOTES: 1702938 - 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1696887: VMs lose network                          connectivity north of logical distributed router</strong><br />                        If a VM learns the pMac of the logical router as the MAC                        address for default gateway instead of the generic                        logical router MAC address,                        it loses connectivity north of the logical router.                        <br />                        <em>Workaround: </em> See <a target="_blank" href="https://kb.vmware.com/kb/2146293">VMware                          knowledge base article 2146293</a>. <em>This has been                          fixed in 6.2.4</em></p>                    </li>                  </ul>                  <!--===============================In Fixedbugs==============================================================--->                  <!--====================================Networking and Edge Services Resolved Issues Fixedbugs 0624 -->                  <!--====================================Networking and Edge Services Resolved Issues Fixedbugs ===============-->                  <!--====================================Networking and Edge Services Resolved Issues Fixedbugs ===============-->                  <h3><a name="nsxedgefixedbugs0624"></a>NSX Edge Services                    Resolved Issues in 6.2.4</h3>                  <ul>                    <!-- DOCNOTES: 1703913 / 1716060 fix 2016-08-23-23:11 -->                    <li>                      <p><strong>Fixed issue 1703913: NSX DLR HA nodes remain in                          a split-brain state</strong><br />                        In an NSX 6.2.3 environment using dynamic routing with                        High Availability (HA) configured on a DLR Control VM,                        both the primary and secondary DLR HA nodes can enter                        and remain in <em>Active</em> state concurrently. <br />                        <em>Workaround</em>: Refer to the <a target="_blank" href="http://kb.vmware.com/kb/2146506">                          VMware knowledge base article 2146506</a>.<em> This                          has been fixed in 6.2.4.</em> </p>                    </li>                    <!-- DOCNOTES: 1686050  6.2.3 -->                    <li>                      <p><strong>Fixed issue 1674721: NSX Edge is unmanageable                          after upgrading to NSX 6.2.3</strong><br />                        This issue occurs when serverSsl or clientSsl is                        configured in load balancer, but cipher's value is set                        as NULL in the previous version.<br />                        <em>Workaround</em>: Refer to the <a target="_blank" href="http://kb.vmware.com/kb/2145887">VMware                          knowledge base article 2145887</a>. <em>This has been                          fixed in 6.2.4.</em></p>                    </li>                    <!-- DOCNOTES: 1702447 -->                    <li>                      <p><strong>Fixed issue 1698389: After changing certain                          routing configurations via the vSphere Web Client,                          routing configuration is incorrect</strong><br />                        Sorting then editing causes an incorrect configuration                        when editing BGP neighbors, OSPF Area to Interface                        mapping, Route Redistribution – IP Prefixes, or BGP                        Filters.                        When a large number of BGP neighbors are configured,                        scrolling through the list, then editing can cause an                        incorrect                        configuration. <br />                        <em>Workaround</em>: Refer to the <a target="_blank" href="http://kb.vmware.com/kb/2146363">VMware                          knowledge base article 2146363</a>.                        <em>This has been fixed in 6.2.4.</em></p>                    </li>                  </ul>                  <!--==============================sectbreak_security Fixedbugs 0624 ==============================================================--->                  <!--====================================sectbreak_security Fixedbugs==============================================================--->                  <!--====================================sectbreak_security Fixedbugs==============================================================--->                  <h3><a name="nsxsecurityfixedbugs0624"></a>Security Services                    Resolved Issues in 6.2.4</h3>                  <ul>                    <!-- DOCNOTES: 1694819 -->                    <li>                      <p><strong>Fixed issue 1694483: After installing or                          upgrading to NSX for vSphere 6.2.3 with Distributed                          Firewall (DFW) and Security Groups (SG) configured,                          you may encounter traffic disruption upon a vMotion                          operation on compute virtual machines</strong>                        <br />                        See <a target="_blank" href="https://kb.vmware.com/kb/2146227">VMware
                          knowledge base article 2146227</a>. <em>This has been                          fixed in 6.2.4.</em></p>                    </li>                    <!-- DOCNOTES: 1696237 -->                    <li>                      <p><strong>Fixed issue 1689356: Editing a security group                          via search removes all objects from the security group</strong><br />                        Editing a security group by searching for a statically                        included member, for example, a VM, and then removing                        that member causes all statically included members to be                        removed from the security group. <em>This has been                          fixed in 6.2.4.</em> </p>                    </li>                    <!--DOCNOTES: 1701514 /1685136 Adding base PR-->                    <li>                      <p><strong>Fixed issue 1675694: Distributed firewall drops                          packets when reusing the same IP and port after a                          disrupted connection</strong><br />                        Connections in the half-closed state do not disconnect,                        causing new connections to that IP and port to fail. <em>This                          has been fixed in 6.2.4.</em>                      </p>                    </li>                    <!--DOCNOTES: 1703021/1710753 -->                    <li>                      <p><strong>Fixed issue 1698863: Retransmission of the                          initial TFTP packet on an established TFTP session                          while distributed firewall is enabled might cause a                          purple diagnostic screen</strong><br />                        <em>This has been fixed in 6.2.4.</em></p>                    </li>                    <!--DOCNOTES: 1702969 -->                    <li>                      <p><strong>Fixed issue 1701195: Distributed firewall                          experiences heap exhaustion</strong><br />                        In larger deployments with high consolidation ratios                        (number of provisioned VMs per host), distributed                        firewall would experience exhaustion of heap memory as                        DFW in VMkernel has a limited amount available (up to                        1.5GB on large memory hosts). <em>This has been fixed                          in 6.2.4. The maximum heap size has been increased to                          3GB for ESXi 6.0 hosts that have memory of 96GB or                          more, enabling a higher consolidation ratio.</em> </p>                    </li>                    <!--DOCNOTES: 1712698 6.2.3b -->                    <li>                      <p><strong>Fixed issue 1712698: Service Composer Security                          Policy rules are deleted after attempting to modify                          Security Policy firewall rules</strong><br />                        <em>This has been fixed in 6.2.4.</em></p>                    </li>                  </ul>                  <!--====================================sectbreak_monitoring Fixedbugs 0624 ==============================================================--->                  <!--====================================sectbreak_monitoring Fixedbugs==============================================================--->                  <!--====================================sectbreak_monitoring Fixedbugs==============================================================--->                  <h3><a name="monitoringfixedbugs0624"></a>Monitoring Services                    Resolved Issues in 6.2.4</h3>                  <ul>                    <!-- DOCNOTES: 1701511 -->                    <li>                      <p><strong>Fixed issue 1697118: All IPFIX flows are tagged                          as new flows rather than updated flows, resulting in                          frequent updates to the IPFIX collector</strong><br />                        Also, the frequency for sending active flows does not                        honor the active flow timeout configured value.                        <em>This has been fixed in 6.2.4.</em>                      </p>                    </li>                  </ul>                  <!--====================================sectbreak_interop Fixedbugs 0624 ==============================================================--->                  <!--====================================sectbreak_interop Fixedbugs==============================================================--->                  <!--====================================sectbreak_interop Fixedbugs==============================================================--->                  <!--<h3><a name="interopfixedbugs0624"></a>Solution Interoperability Resolved Issues in 6.2.4</h3>-->                  <p></p>                  <a name="fixedin62x"></a>                  <h3>The following issues were resolved in the 6.2.3, 6.2.2,                    6.2.1, and 6.2.0 releases:</h3>                  <p>6.2.3, 6.2.2, 6.2.1, and 6.2.0 resolved issues are grouped                    as follows:</p>                  <ul>                    <li><a href="#generalfixedbugs062x" name="&amp;lpos=apps_scode : 10">General                        Resolved Issues in 6.2.3 and earlier</a></li>                    <li><a href="#installupgradefixedbugs062x" name="&amp;lpos=apps_scode : 10">Installation                        and Upgrade Resolved Issues in 6.2.3 and earlier</a></li>                    <li><a href="#nsxmgrfixedbugs062x" name="&amp;lpos=apps_scode : 10">NSX                        Manager Resolved Issues in 6.2.3 and earlier</a></li>                    <li><a href="#nsxnetworkfixedbugs062x" name="&amp;lpos=apps_scode : 10">Logical                        Networking and NSX Edge Routing Resolved Issues in 6.2.3                        and earlier</a></li>                    <!-- <li><a name="&amp;lpos=apps_scode : 10" href="#nsxlogicalfixedbugs062x">Logical Networking Resolved Issues in 6.2.3 and earlier</a></li> -->                    <li><a href="#nsxedgefixedbugs062x" name="&amp;lpos=apps_scode : 10">Edge                        Services Resolved Issues in 6.2.3 and earlier</a></li>                    <li><a href="#nsxsecurityfixedbugs062x" name="&amp;lpos=apps_scode : 10">Security                        Services Resolved Issues in 6.2.3 and earlier</a></li>                    <li><a href="#monitoringfixedbugs062x" name="&amp;lpos=apps_scode : 10">Monitoring                        Services Resolved Issues in 6.2.3 and earlier</a></li>                    <li><a href="#interopfixedbugs062x" name="&amp;lpos=apps_scode : 13">Solution                        Interoperability Resolved Issues in 6.2.3 and earlier</a></li>                  </ul>                  <!--====================================General Known Fixedbugs 0621 06210 ==================================================--->                  <!--====================================General Known Fixedbugs==============================================================--->                  <!--====================================General Known Fixedbugs==============================================================--->                  <!--====================================General Fixedbugs -->                  <h3><a name="generalfixedbugs062x"></a>General Resolved Issues                    in 6.2.3 and earlier</h3>                  <ul>                    <!-- DOCNOTES: 1682150  6.2.3-->                    <li>                      <p><strong>Fixed issue 1644529: Security patch to address                          the security vulnerability, CVE-2016-2079 </strong><br />                        The 6.2.3 release delivers a security patch to address <a                          target="_blank"                          href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2016-2079">CVE-2016-2079</a>.</p>                    </li>                    <!-- DOCNOTES:  1597644 6.2.3-->                    <li>                      <p><strong>Fixed issue 1571156: vCenter 6.0 restart/reboot                          may result in duplicate VTEPs on VXLAN prepared ESX                          hosts</strong><br />                        See <a target="_blank" href="http://kb.vmware.com/kb/2144605">VMware
                          knowledge base article 2144605</a>. <em>This has been                          fixed in NSX 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES:   1601794 6.2.1-->                    <li><strong>Fixed issue 1529665: The DaaS service is not                        working as the service using 2 different VIPs (one VIP                        for HTTP and another for PCoIP) that must have exactly                        the same persistency</strong><br />                      <em> This issue has been fixed in 6.2.1. </em> </li>                    <!-- DOCNOTES:  1647341   6.2.3-->                    <li>                      <p><strong>Fixed issue 1631261: IDFW is configured to work                          with Log Scraper and GI is also installed, after                          un-installing the GI, IDFW stops working</strong><br />                        <em> This has been fixed in NSX 6.2.2. </em></p>                    </li>                    <!-- DOCNOTES: 1646182 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1551773: Edge Security Gateway                          (ESG) HA vNIC dropdown selection is always empty in                          VMware NSX for vSphere 6.2.0 </strong><br />                        <em>This has been fixed in NSX 6.2.2.</em> See <a target="_blank"                          href="http://kb.vmware.com/kb/2138158">VMware
                          knowledge base article 2138158</a>. </p>                    </li>                    <!-- DOCNOTES: 1608608 -->                    <li>                      <p><strong>Fixed issue 1608608: Security patch to address                          the glibc vulnerability,                          CVE-2015-7547</strong><br />                        The 6.2.2 release delivers a security                        patch to address <a target="_blank" href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2015-7547">CVE-2015-7547</a>.                      </p>                    </li>                    <!-- DOCNOTES: 1480581 -->                    <li>                      <p><strong>Fixed issue 1480581: netcpa sockets are CLOSED                          and VM                          fails to communicate across VNIs, subnets</strong><br />                        This issue was fixed by fixing thread unsafe use of                        boost::asio in                        vmacore. <em>This has been fixed in NSX 6.2.2.</em>                        See <a target="_blank" href="http://kb.vmware.com/kb/2137011">VMware
knowledge                          base article 2137011</a>.                      </p>                    </li>                    <!-- DOCNOTES: 1583566 -->                    <li>                      <p><strong>Fixed issue 1583566: Rules not pushed to host</strong><br />                        DFW rule/ip list updates failed to be scheduled due to                        task framework                        resource limitations in NSX Manager. Error message                        showed a failure to                        queue tasks for Change Notification threads. <em>This                          has been fixed                          in NSX 6.2.2.</em></p>                    </li>                    <!-- DOCNOTES: 1573818 -->                    <li>                      <p><strong>Fixed issue 1573818: Traffic interrupted for 50                          seconds after HA failover on                          ESG</strong><br />                        This issue was caused when NSX failed to synchronize the                        static routes                        among the HA NSX Edge nodes. <em>This has been fixed in                          NSX                          6.2.2.</em></p>                    </li>                    <!-- DOCNOTES: 1570808 -->                    <li>                      <p><strong>Fixed issue 1570808: NSX load balancer IP_HASH                          health check issue</strong><br />                        In IPVS, when using the source-ip hash algorithm, if the                        selected                        backend server's weight equals 0, a "service                        unavailable" reply is                        sent even if there are healthy backend servers. <em>This                          has been                          fixed in NSX 6.2.2.</em></p>                    </li>                    <!-- DOCNOTES: 1564005 -->                    <li>                      <p><strong>Fixed issue 1564005: In NSX NetX, cannot add                          rules to redirect traffic to                          partner devices</strong><br />                        Customers were unable to add traffic redirection rules                        to their NetX                        rule sets. As a result, customers were unable to                        redirect traffic to                        partner devices. This affected rules that used IP                        address sets. This                        issue was caused by incorrect handling of IP ranges in                        the NetX                        rules. <em>This has been fixed in NSX 6.2.2.</em></p>                    </li>                    <!-- DOCNOTES: 1587660 / 1601800 -->                    <li>                      <p><strong>Fixed issue 1587660: NSX NetX error in                          DVFilterProcessSlowPathPackets</strong><br />                        Using NSX NetX without DFW resulted in an error in                        DVFilter. The full                        error message indicated NetX error PF (err=11,cr2=0x10)                        in                        DVFilterProcessSlowPathPackets:                        VSIPDVFProcessSlowPathPackets:                        PFFilterPacket. <em>This has been fixed in NSX 6.2.2.</em>                        See <a target="_blank" href="http://kb.vmware.com/kb/2144018">VMware
knowledge                          base article 2144018</a>.                      </p>                    </li>                    <!-- DOCNOTES:  1591673 / 1588580 / 1592974 -->                    <li>                      <p><strong>Fixed issue 1591673: Adding ESXi host to                          vSphere Distributed                          Switch fails with license error</strong><br />                        In NSX 6.2.1 only, adding an ESXi host to a vSphere                        Distributed Switch                        failed with license error: "Host <em>IP address</em> is                        not                        licensed for the VDS feature. Cannot add this host to                        dvSwitch."                        For details, see <a target="_blank" href="http://kb.vmware.com/kb/2143397">VMware                          knowledge base article 2143397</a>.                        <em>This has been fixed in NSX 6.2.2.</em>                      </p>                    </li>                    <!-- DOCNOTES: 1590563 -->                    <li>                      <p><strong>Fixed issue 1590563: Enterprise license error                          after upgrade</strong><br />                        The NSX 6.2.1 upgrade routine allowed you to upgrade to                        6.2.1 without                        a VMware Enterprise License, but after upgrade, the                        Enterprise License                        was required in orer to use NSX. <em>This has been                          fixed in NSX                          6.2.2.</em>                        See <a target="_blank" href="http://kb.vmware.com/kb/2135310">VMware                          knowledge base article 2135310</a>.                      </p>                    </li>                    <!-- DOCNOTES: 1589046 -->                    <li>                      <p><strong>Fixed issue 1589046: Packet sent to LIF without                          DHCP relay results in PSOD</strong><br />                        The ESXi host suffers a PSOD if a DHCP unicast packet is                        addressed                        to the IP of a LIF that is expected to have DHCP relay                        enabled but the                        actual receiving LIF does not have DHCP relay enabled. <em>This                          has                          been fixed in NSX 6.2.2.</em>                        See <a target="_blank" href="http://kb.vmware.com/kb/2144314">VMware                          knowledge base article 2144314</a>.                      </p>                    </li>                    <!-- DOCNOTES: 1593436 -->                    <li>                      <p><strong>Fixed issue 1593436: In VXLAN hybrid mode,                          controller disconnection                          incorrectly triggers fallback to multicast mode</strong><br />                        <em>This has been fixed in NSX 6.2.2.</em>                        See <a target="_blank" href="http://kb.vmware.com/kb/2144457">VMware                          knowledge base article 2144457</a>.                      </p>                    </li>                    <!-- DOCNOTES: 1574995 -->                    <li>                      <p><strong>Fixed issue 1574995: DFW Publishing error</strong><br />                        Modifying and saving DFW rules in filtered mode may                        result in rules                        not being saved and published. <em>This has been fixed                          in NSX                          6.2.2.</em>                        See <a target="_blank" href="http://kb.vmware.com/kb/2141155">VMware                          knowledge base article 2141155</a>.                      </p>                    </li>                    <!-- DOCNOTES: For 6.1.5, see PR 1515593. For 6.2.0, see PRs 1422110/1464439-->                    <li>                      <p><strong>Fixed issue 1422110: One of the NSX Controllers                          does not hand over master role to other controllers                          when it is shut down</strong><br />                        <em>This has been fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1483728 / 1521659-->                    <li>                      <p><strong>Fixed issue 1483728: Control plane connectivity                          fails for NSX Controller</strong><br />                        Control plane connectivity was seen to fail for a                        Controller, showing an error in netcpa related to <tt>txInProgress</tt>.                        <em>This has been fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1534608, 1288006 6.2.0  -->                    <li>                      <p><strong>Fixed issue 1487910: Upgrading Edge Services                          Gateway fails with "Timed out waiting for Edge vm"                          message</strong><br />                        Applying an IPv6 address to the NSX management interface                        causes NSX Manager to use the host name. The vsfwd proxy                        which connects the Edge VM to NSX Manager does not                        correctly handle a FQDN, resulting in a error similar to                        "ERROR TaskFrameworkExecutor-6                        AbstractEdgeApplianceManager:185 - Timed out waiting for                        Edge vm {}. Vm took too long to boot and respond                        com.vmware.vshield.edge.exception.VshieldEdgeException".                        <em>This has been fixed in NSX 6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1623347 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1571548: In NSX for vSphere release                          6.2.0                          and later, if a VTEP IP address is changed directly on                          a host or in                          VC, the old IP address of the VTEP is released                          automatically.                        </strong><br />                        <em>This has been fixed in NSX 6.2.0.</em> </p>                    </li>                    <!-- DOCNOTES: 1591667 6.2.3-->                    <li>                      <p><strong>Fixed issue 1551164: NSX User Interface (UI) is                          grayed out for several seconds and exhibits slow                          performance on NSX for vSphere 6.2.0 </strong><br />                        See <a target="_blank" href="http://kb.vmware.com/kb/2141919">VMware
                          knowledge base article 2141919</a>. <em>This has been                          fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1545840 6.2.1-->                    <li>                      <p><strong>Fixed issue 1545840: Cannot disable the NSX                          distributed firewall (DFW) on a host in VMware NSX for                          vSphere 6.x</strong><br />                        See also <a target="_blank" href="http://kb.vmware.com/kb/2141915">VMware                          knowledge base article 2141915</a>. <em>This has been                          fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1528680 /  1526495 / 1541116 -->                    <!-- DOCNOTES: 2134329 -->                    <li>                      <p><strong>Fixed issue 1528680: VMware ESXi 5.x and 6.x                          experiences a purple diagnostic screen when                          using IP discovery in VMware NSX for vSphere 6.2.0 (KB                          2134329)</strong><br />                        When using IP discovery on logical switches in VMware                        NSX for vSphere                        6.2.0, the ESXi 5.x and 6.x host fails with a purple                        diagnostic                        screen as explained in <a target="_blank" href="http://kb.vmware.com/kb/2134329">knowledge
base                          article 2134329</a>. <em>This has been fixed in NSX                          6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1545885 / 6.2.1-->                    <li>                      <p><strong>Fixed issue 1545885: Manage option on Security                          Tag portlet is grayed out by default</strong><br />                        On a Virtual Machine's summary page, the "Manage"                        hyperlink on the security tag portlet remains grayed out                        till the user creates a new security tag. <em>This has                          been fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1476087 / 6.2.1-->                    <li>                      <p><strong>Fixed issue 1476087: Some Controller logs not                          available for syslog export.</strong><br />                        Controller logs, including Zookeeper clustering logs,                        are not part of syslog export. <em>This has been fixed                          in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES:1545830 -->                    <li>                      <p><strong>Fixed issue 1545830: ESXi 6.0 PSOD on vdl2 when                          pinging with data size higher than available data size                          for the MTU</strong><br />                        Starting ping from NSX host switch attached vmknic will                        lead to host PSOD if data size if greater than MTU.<em>This                          has been fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1545873-->                    <li>                      <p><strong>Fixed issue 1545873: Users needed to configure                          same IP address and port number for both TCP and UDP                          protocol</strong>                        <br />                        This release resolves the following issues as well:                      </p>                      <ul>                        <li>UDP virtual server without pool configuration leads                          to configuration failure.</li>                        <li>Statistics shows incorrect data when UDP virtual                          server is not associated with any pool.</li>                      </ul>                      <br />                      <em>This has been fixed in NSX 6.2.1. With 6.2.1 release,                        users can use the same IP address and port number for                        both TCP and UDP with/without a pool associated. </em>                      <p></p>                    </li>                  </ul>                  <!--====================================sectbreak_install  Fixedbugs  0621 06210  ================================================--->                  <!--====================================sectbreak_install  Fixedbugs==============================================================--->                  <!--====================================sectbreak_install  Fixedbugs==============================================================--->                  <h3><a name="installupgradefixedbugs062x"></a>Install and                    Upgrade Resolved Issues in 6.2.3 and earlier</h3>                  <ul>                    <!-- DOCNOTES: 1656643  -->                    <li>                      <p><strong>Fixed issue 1578509: After EAM restart, Guest                          Introspection(GI) Service status is in warning state</strong><br />                        <em>This has been fixed in NSX 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES: 1543686 6.2.3-->                    <li>                      <p><strong>Fixed issue 1539203: After NSX upgrade, NSX                          plugin gets disconnected from Primary VC during a                          Cross-vCenter Upgrade</strong><br />                        <em>This has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1558017 6.2.1  -->                    <li>                      <p><strong>Fixed issue 1558017: After upgrading the NSX                          Edge from 6.1.x to 6.2.x, the NSX Manager vsm.log                          shows "INVALID DHCP CONFIG"</strong><br />                        If you have an interface with an IPv6 subnet, DHCP                        generates an empty shared subnet and treats it as an                        invalid operation. </p>                    </li>                    <!-- DOCNOTES: 1490496/1483819 / 1506622-->                    <li>                      <p><strong>Fixed issue 1490496: After NSX upgrade, Guest                          Introspection fails to communicate                          with NSX Manager</strong><br />                        After upgrading from NSX 6.0.x to NSX 6.1.x or from NSX                        6.0.x to NSX 6.2 and before the Guest Introspection                        service is upgraded, the NSX Manager cannot communicate                        with the Guest Introspection Universal Service Virtual                        Machine (USVM). <em>This has been fixed in NSX 6.1.5                          and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1536179/1539954/1543683 6.2.1-->                    <li>                      <p><strong>Fixed issue 1536179: SSL VPN-Plus client cannot                          be installed on Mac OS X Yosemite and higher</strong><br />                        Earlier versions of Mac OS X are supported.                        <em>This has been fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1393503 / No dev bug-->                    <li>                      <p><strong>Fixed issue 1393503: After upgrading NSX                          vSphere from 6.0.7 to 6.1.3, vSphere Web Client                          crashes on login screen</strong><br />                        After upgrading NSX Manager from 6.0.7 to 6.1.3, you                        will see                        exceptions displayed on the vSphere Web Client UI login                        screen. You                        will not be able to login and perform operations on                        either vCenter or                        NSX Manager. <em>This has been fixed in NSX 6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1088497 / 1087988: Base bug -1161083 -->                    <li>                      <p><strong>Fixed issue 1088497: Guest Introspection                          installation fails with error</strong><br />                        When installing Guest Introspection on a cluster, the                        install fails with the following error:<br />                        <code>Invalid format for VIB Module</code>. <em>This                          has been fixed in NSX 6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1328589 / 1457120 / 1456184-->                    <li>                      <p><strong>Fixed issue 1328589: DVPort fails to enable                          with "Would block" due to host prep issue</strong><br />                        On an NSX-enabled ESXi host, the DVPort fails to enable                        with "Would                        block" due to a host preparation issue. When this                        occurs, the error                        message first noticed varies (for example, this may be                        seen as a VTEP                        creation failure in VC/hostd.log, a DVPort connect                        failure in                        vmkernel.log, or a 'SIOCSIFFLAGS' error in the guest).                        This happens                        when VIBs are loaded after the vSphere Distributed                        Switch (vDS)                        properties are pushed by vCenter. This may happen during                        upgrade.                        See <a target="_blank" href="http://kb.vmware.com/kb/2107951">knowledge
base                          article 2107951</a>. <em>This has been fixed in NSX                          6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1446544 /1445066/1405519-->                    <li>                      <p><strong>Fixed issue 1446544: Attempts to delete                          existing NSX Edge Gateway fail in an environment                          upgraded to NSX 6.1.4</strong><br />                        In NSX installations upgraded from 6.1.3 to 6.1.4, the                        existing NSX Edge Gateways cannot be deleted after the                        upgrade to 6.1.4. This issue                        does not affect new Edge Gateways created after the                        upgrade. Installations that upgraded directly from 6.1.2                        or earlier are not                        affected by this issue. <em>This has been fixed in NSX                          6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1418836-->                    <li>                      <p><strong>Fixed issue 1418836: The AES encryption                          unavailable when performing an NSX backup using                          third-party secured FTP backup</strong>. <em>This has                          been fixed in NSX 6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1410153/1484867/1490422-->                    <li>                      <p><strong>Fixed issue 1410153: NSX Manager UI does not                          display user-friendly error messages during host                          reboot</strong><br />                        In this 6.2 release, NSX Manager UI is updated to                        display detail error messages that describe the problems                        you might encounter during host reboot and provide                        possible solution. <em>This has been fixed in NSX                          6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1412133-->                    <li>                      <p><strong>Fixed issue 1412133: Unable to install NSX VIB                          installation </strong><br />                        The installation of NSX VIB might not complete, as                        expected if the ixgbe driver fails to load from                        third-party module because it has been locked and                        prevents it from being used for installation. <em>This                          has been fixed in NSX 6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1467438-->                    <li>                      <p><strong>Fixed issue 1467438: Unable to start NSX                          Manager service after upgrading from vCloud Networking                          and Security (vCNS) 5.5.3</strong><br />                        After upgrading vCloud Networking and Security (vCNS)                        5.5.3 to NSX 6.1.3, the NSX Manager service hangs and is                        unable to start successfully. <em>This has been fixed                          in NSX 6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1440867-->                    <li>                      <p><strong>Fixed issue 1440867: The message bus randomly                          does not start after NSX Edge reboot</strong><br />                        After restarting an Edge VM, the message bus often does                        not start after powering on, and an additional reboot is                        required. <em>This has been fixed in NSX 6.2.0.</em></p>                    </li>                  </ul>                  <!--====================================sectbreak_manager Fixedbugs  0621 06210 ==============================================================--->                  <!--====================================sectbreak_manager Fixedbugs==============================================================--->                  <!--====================================sectbreak_manager Fixedbugs==============================================================--->                  <h3><a name="nsxmgrfixedbugs062x"></a>NSX Manager Resolved                    Issues in 6.2.3 and earlier</h3>                  <ul>                    <!-- DOCNOTES: 1709012 6.2.4-->                    <li>                      <p><strong>Fixed Issue 1540187: Users cannot login through                          vSphere Web Client and use NSX plugin, giving out                          error that user/group does not have permissions </strong><br />                        This issue had dependency on the timeout that happens                        when saml token is being generated. At times when the                        request operation does not complete when communicating                        with SSO service, NSX is not able to refresh solution                        registration provider internally. This causes null                        pointer exception for every other request once this                        happens.<br />                        <em>This has been fixed in NSX 6.2.3 by avoiding null                          pointer exception and reconnecting to the SSO service                          if required. </em></p>                    </li>                    <!-- DOCNOTES: 1653449-->                    <li>                      <p><strong>Fixed Issue 1640388: While uninstalling Guest                          Introspection from a cluster that does not contain any                          VM, an error message "Pre-Uninstall cleanup failed"                          appears, and the status is displayed as unresolved</strong><br />                        This was a known issue in the uninstall logic of Guest                        Introspection. <br />                        <em>This has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1534588-->                    <li>                      <p><strong>Fixed issue 1534588: Previous backups are not                          displayed in the NSX Manager UI</strong><br />                        Running a backup operation never shows a successful                        completion at the NSX Manager UI. Either one of these                        issues may manifest if a large number of backup files                        are stored in the destination folder. Each backup file                        has to be checked for compatibility before displaying                        the list on the same page. The current file list process                        can cause the page to timeout. <br />                        <em>This has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1656644 6.2.3-->                    <li>                      <p><strong>Fixed Issue 1593910: Duplicate NSX Manager IP                          address is not detected or prevented</strong> <br />                        If the NSX Manager IP address is assigned to another                        device on the network, an explicit error or event log is                        not generated. As a result, NSX controllers and hosts                        may respond to NSX Manager using an incorrect MAC                        address, causing a data path outage. <em>Workaround</em>:                        Attempt to determine and then remove the other network                        device from the network or assign it a different IP                        address. Due to presence of duplicate NSX manager IP in                        the network, hosts and Controllers are responding to NSX                        Manager/VM using the wrong MAC address. This affects                        communications between NSX Manager and ESX and between                        NSX Manager and NSX Controllers. This can result in a                        datapath outage. In this case applications are impacted                        until the duplicate IP is removed from the network and                        communication channels are restored. <br />                        <em>This has been fixed in NSX 6.2.3 </em> by adding a                        system event when a duplicate IP address is detected.                        <!-- DOCNOTES: 1591670 6.2.3--> </p>                    </li>                    <li>                      <p><strong>Fixed issue 1489648: NSX is unavailable from                          the vSphere Web Client Plug-in after taking a backup                          of NSX Manager with quiesced snapshot </strong><br />                        See <a target="_blank" href="http://kb.vmware.com/kb/2142263">VMware
                          knowledge base article 2142263</a>. <em>This has been                          fixed in NSX 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES: 1498642 / 1501526-->                    <li>                      <p><strong>Fixed issue 1440451: NSX Manager certificate                          replacement requires restart of NSX Manager and may                          require restart of vSphere Web Client</strong><br />                        After you replace the NSX Manager appliance certificate,                        you must always restart the NSX Manager appliance. In                        certain cases after a certificate replacement, the                        vSphere Web Client will not display the "Networking and                        Security" tab. </p>                    </li>                    <!-- DOCNOTES: 1543689 6.2.1-->                    <li>                      <p><strong>Fixed issue 1568861: Unable to add Secondary                          NSX Manager if GUI is Japanese language on Firefox                          browser </strong><br />                        When adding a secondary NSX Manager with German,                        Japanese, Korean, or French locale and a Firefox                        browser, the thumbprint dialog is not shown, blocking                        the configuration. </p>                    </li>                    <!-- DOCNOTES:1522092 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1482989 / 1522092: NSX Networking                          and Security shows all hosts as GREEN but Cluster                          status incorrectly shown as RED</strong><br />                        In NSX 6.1.4 and earlier, under rare conditions the NSX                        Networking and Security tab showed all hosts as GREEN                        but incorrectly showed the Cluster status as RED                        (incorrectly indicating an error condition).<em>This has                          been fixed in NSX 6.1.5.</em></p>                    </li>                    <!-- DOCNOTES: 1515656/1450767-->                    <li>                      <p><strong>Fixed issue 1515656: NSX Manager CPU                          utilization is high after adding it to Active                          Directory domain</strong><br />                        NSX Manager CPU utilization is high after adding it to                        Active Directory domain. In the system logs of the NSX                        Manager, multiple Postgres threads are seen as running.                        <em>This has been fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1484939 / 1521696-->                    <li>                      <p><strong>Fixed issue 1484939: Unable to register NSX                          Manager 6.1.4 with vCenter, gives error: NSX                          Management Service operation failed</strong> <br />                        <em>This has been fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1521710-->                    <li>                      <p><strong>Fixed issue 1521710: NSX Manager web client                          displays error: Code 301002</strong><br />                        Description: When you navigate to NSX manager &gt;                        Monitor &gt; System                        Events, the web client displays the following message: <tt>Filter
config                          not applied to vnic. Code 301002</tt>. <em>This has                          been fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES:1479665 / 1483014 / 1537332 -->                    <li>                      <p><strong>Fixed issue 1479665: Starting in 6.2.1, NSX                          Manager queries each controller node in the                          cluster to get the connection information between that                          controller and                          the other controllers in the cluster</strong><br />                        This is provided in the output of the NSX REST API ("GET                        https://[NSX-MANAGER-IP-ADDRESS]/api/2.0/vdn/controller"                        command),                        which now shows the peer connection status between among                        the                        controller nodes. If NSX Manager finds the connection                        between any two                        controller nodes is broken, a system event is generated                        to alert the                        user. <em>This has been fixed in NSX 6.2.1.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES:1525516/1534815 -->                    <li>                      <p><strong>Fixed issue 1525516: Force-sync of controller                          is broken if backup-restore of manager is done on                          another appliance</strong><br />                        If an NSX Manager appliance is cloned and/or restored                        from a backup, a force-sync operation to an NSX                        controller cluster will fail. This issue does not occur                        for an NSX Manager deployed from scratch. <em>This has                          been fixed in NSX 6.2.1.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES:1509454 / 1537374 -->                    <li>                      <p><strong>Fixed issue 1509454: NSX logging heartbeat                          failures for hosts that are not part of the NSX                          installation</strong><br />                        When an NSX-prepared host is directly removed from the                        vCenter                        inventory (without first unpreparing it in NSX), NSX                        receives an                        unexpected 'Host Connected' DCN which causes partial                        removal of                        messaging infrastructure components from the host. As a                        result, the                        messaging link between NSX and the host may remain                        active when it                        should have been removed, and NSX may raise false                        'Alert' SystemEvents                        for the host. This has been fixed in NSX 6.2.1. <em>This                          has been fixed in NSX 6.2.1.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1418655-->                    <li>                      <p><strong>Fixed issue 1418655: NSX Manager is                          non-functional after running the <tt>write erase</tt>                          command</strong><br />                        When you restart the NSX Manager after running the <tt>write                          erase</tt> command, you might notice that the NSX                        Manager is not working as expected, such as the password                        to access the Linux shell has been reset, the setup                        command is missing, and so on. <em>This has been fixed                          in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1366669/1497041-->                    <li>                      <p><strong>Fixed issue 1366669: Add Domain shows error at                          LDAP option with <tt>Use Domain Credentials</tt></strong><br />                        In NSX 6.1.x, the user when trying to add an LDAP                        domain, the web client gave a <tt>User Name was not                          specified</tt> error, even when Username was provided                        in UI. This has been fixed in NSX 6.2.0. <em>This has                          been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: dev: 1352169-->                    <li>                      <p><strong>Fixed issue 1352169: CA signed certificate                          import needs an NSX Manager reboot before becoming                          effective</strong><br />                        When you import an NSX Manager certificate signed by CA,                        the newly imported certificate does not become effective                        until NSX Manager is rebooted. <em>This has been fixed                          in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: doc 1497113/1472331-->                    <li>                      <p><strong>Fixed issue 1497113: Unable to import NSX                          Manager to LDAPS domain</strong><br />                        When you attempt to add NSX manager to LDAPS domain, the                        following error message appears.<br />                        <tt>Cannot connect to host &lt;Server FQDN&gt; <br />                          error message: simple bind failed: &lt;Server                          FQDN:Number&gt;</tt>. <em>This has been fixed in NSX                          6.2.0.</em></p>                      <p></p>                    </li>                  </ul>                  <!--====================================sectbreak_logical Fixedbugs 0621 06210 ==============================================================--->                  <!--====================================sectbreak_logical Fixedbugs==============================================================--->                  <!--====================================sectbreak_logical Fixedbugs==============================================================--->                  <h3><a name="nsxnetworkfixedbugs062x"></a>Logical Networking                    and NSX Edge Routing Resolved Issues in 6.2.3 and earlier</h3>                  <ul>                    <!-- DOCNOTES: Leslies field advisory  6.2.3-->                    <li>                      <p><strong>Fixed issue: Data path issues for VNIs with                          disconnected NSX Controller</strong><br />                        This issue occurs because IPSec re-keying is disabled in                        NSX-V 6.1.5, 6.1.6, 6.2, 6,2,1 and 6.2.2 releases to                        avoid hitting a another known IPSec issue. </p>                      See <a target="_blank" href="http://kb.vmware.com/kb/2146973">VMware
                        knowledge base article 2146973</a>. <em>This has been                        fixed in NSX 6.2.3.</em> </li>                    <!-- DOCNOTES: 1601803  6.2.3-->                    <li>                      <p><strong>Fixed issue 1591582: In some corner conditions                          ARP requests sent by VDR instance might get dropped</strong><br />                        VDR ARP requests for remote VMs located on other hosts                        may get dropped at VDR uplink output processing, causing                        slow connection establishment. </p>                    </li>                    <!-- DOCNOTES: 1644663 6.2.3-->                    <li>                      <p><strong>Fixed Issue 1501900: Edge OSPF router remains                          stuck in ExchangeStart state after changing OSPF                          interface IP address</strong><br />                        Due to a race condition, changing the IP address on an                        OSPF interface was causing the OSPF neighbors to remain                        stuck in ExchangeStart state on both sides. Under normal                        conditions, it is a supported operation to change the                        OSPF interface IP address. <br />                        <em>This has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES:  issue 1601291 6.2.3-->                    <li><strong>Fixed issue 1498251: IS-IS is not a supported                        routing protocol for the Edge Services Gateway router </strong></li>                    <p>The references to IS-IS are removed from the UI and APIs                      in NSX 6.2.3. </p>                    <p></p>                    <!-- DOCNOTES: 1492738-->                    <li>                      <p><strong>Fixed issue 1492738: Unable to add more than                          eight uplink interfaces during Distributed Logical                          Router (DLR) deployment using vSphere Web Client</strong><br />                        <em>This has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <!--  <em>Workaround</em>: Wait for the DLR deployment to complete and then add additional interfaces to Distributed Logical Router.--->                    <!-- DOCNOTES: 1681508 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1552038: Intermittent loss of                          connectivity from NSX Edge to DLR uplink interface</strong><br />                        This issue was caused by the NSX Edge having the DLR                        control VM’s MAC address in its ARP table rather than                        the local instance MAC address of the DLR. This release                        adds an outbound ARP filter to prevent the DLR control                        VM from generating ARPs related to the DLR IP address. </p>                    </li>                    <!-- DOCNOTES: 1667440 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1454161: Static routes with a next                          hop as a /31 IP address cannot be configured</strong><br />                        <em>This has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <p></p>                    <!-- DOCNOTES: 1656635 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1528443: VXLAN ARP cache on the                          hosts not being updated when an Edge VM sends a GARP                          during failover</strong><br />                        In certain deployments where the VM's and Edge are on                        the same VXLAN segment, VXLAN ARP cache on the host                        won't get updated after the Edge failover. <em>This has                          been fixed in NSX 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES: 1601308 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1600874: Stranded VMs are not                          removed when new Edge VM is being deployed</strong><br />                        When upgrading an Edge, if the publish operation and                        roll back operations both fail, the original Edge VM                        remains in the NSX Manager database, while VC retains                        the new Edge VM's ID number. Due to this mismatch,                        redeploying an Edge VM will fail. A force sync also will                        fail with a "VM not found" error. </p>                    </li>                    <!-- DOCNOTES: 1646923  6.2.3-->                    <li>                      <p><strong>Fixed issue 1467774: Incorrect value shown for                          admin distance field in "show ip bgp neighbor" command                        </strong><br />                        A route learned from a EBGP peer and advertised to an                        IBGP peer in the same AS incorrectly retains the                        previous admin distance. This issue has been fixed in                        6.2.3. </p>                    </li>                    <!-- DOCNOTES: 1635247 6.2.3-->                    <li>                      <p><strong>Fixed issue 1613383: For an NSX Edge Load                          Balancer running in L4 mode, the current connections                          value incorrectly used the total connections number</strong><br />                        This release fixes the issue by calculating the current                        connections using a sum of the active connections. This                        issue has been fixed in 6.2.3. </p>                    </li>                    <!-- DOCNOTES:  1635233 6.2.3-->                    <li>                      <p><strong>Fixed issue 1584664 : If load balancer pool VM                          members are removed manually from the vCenter                          inventory without first being  unconfigured in NSX,                          orphan database entries are left behind in the NSX                          Manager database.   An ObjectNotFoundException will be                          reported in the NSX Manager logs. </strong><br />                        This issue has been fixed in 6.2.3. </p>                    </li>                    <!-- DOCNOTES:  1667445 6.2.3-->                    <li>                      <p><strong>Fixed issue 1446809: NSX Edge can no longer be                          managed by vCloud Director if no health check recovery                          event is sent after an Edge reboot </strong><br />                        NSX Manager saves Edge connectivity status in memory.                        When an Edge VM fails to respond to a health check, a                        miss event is raised, and on recovery, a recovered event                        is raised. If NSX Manager is restarted, the recovery                        event may not be sent if no health checks were missed                        after the reboot. As vCloud Director depends on these                        events, a missed recovery event can lead to an                        unmanageable Edge VM from VCD. </p>                    </li>                    <!-- DOCNOTES: 1441319 -->                    <li>                      <p><strong>Fixed issue 1441319: Connectivity loss after                          removing a logical interface (LIF) in installations                          with dynamic routing</strong><br />                        A problem was identified in the NSX Logical Router                        (Edge/DLR)                        when using dynamic routing (OSPF &amp; BGP) that will                        cause network                        connectivity loss after removing a LIF. This affects NSX                        versions                        6.0.x through 6.1.4. <em>This has been fixed in NSX                          6.1.5 and NSX 6.2.1. </em></p>                    </li>                    <!-- DOCNOTES: 1445291 / 1534803 -->                    <li>                      <p><strong>Fixed issue 1445291: RADIUS authentication                          server configuration fails on NSX Edge</strong><br />                        In NSX 6.1.5 and earlier, the RADIUS server secret key                        string had a                        32-character limit; if the string exceeded this                        character limit, the                        RADIUS server failed to connect with the NSX Edge. The                        limit is now 64 characters. <em>This was fixed in NSX                          6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1534811 -->                    <li>                      <p><strong>Fixed issue 1534811: VIO Heat stack deployment                          fails intermittently for the VMware NSX for vSphere                          6.x Edge with the error: Cannot allocate memory</strong><br />                        Health monitoring memory usage increases over time,                        eventually causing edge failure. <em>This has been                          fixed in NSX 6.2.1.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1500624-->                    <li>                      <p><strong>Fixed issue 1500624: BGP filters are taking                          approximately 40 seconds to be effectively applied</strong><br />                        During this period all the redistribution policies are                        applied without filters. This delay applies only to NSX                        Distributed Logical Router (DLR) for OUT directions. <em>This                          has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1484758-->                    <li>                      <p><strong>Fixed issue 1484758: On NSX Edge subinterfaces,                          ICMP redirects are sent out, even                          when the <tt>Send ICMP redirect</tt> option is                          disabled</strong><br />                        By default, NSX Edge subinterfaces have <tt>Send ICMP                          redirect</tt>                        disabled. Although this option is disabled, ICMP                        redirects are sent                        out on edge subinterfaces. <em>This has been fixed in                          NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1265605 and 1265548 / 1235372-->                    <li>                      <p><strong>Fixed issue 1265605: Cannot add non-ASCII                          characters in bridge or tenant name for logical router</strong><br />                        NSX controller APIs do not support non-ASCII characters.                        <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: doc 1341784/1402993-->                    <li>                      <p><strong>Fixed issue 1341784: When a BGP neighbor filter                          rule is modified, the existing filters may not be                          applied for up to 40 seconds</strong><br />                        When BGP filters are applied to an NSX Edge running                        IBGP, it may take up to 40 seconds for the filters to be                        applied on the IBGP session. During this time, NSX Edge                        may advertise routes which are denied in the BGP filter                        for the IBGP peer. <em>This has been fixed in NSX                          6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1422110/1464439 [also added to 6.5.1 RN: PR 1515593-->                    <li>                      <p><strong>Fixed issue 1422110: One of the NSX Controllers                          does not hand over master role to other controllers                          when it is shut down</strong><br />                        Typically, when a controller assumes operations master                        role and is preparing to shut down, it automatically                        hands over the master role to other controllers. In this                        case, the controller fails to hand over the role to                        other controllers and the status becomes interrupted and                        then goes into disconnected mode. <em>This has been                          fixed in NSX 6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1440790-->                    <li>                      <p><strong>Fixed issue 1440790: Unable to pass VXLAN                          traffic between hosts with unicast or multicast</strong><br />                        When VMs are on the same host they can communicate                        across VXLAN with unicast or multicast, but cannot                        communicate when VMs are on different hosts. <em>This                          has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1432420/1497050-->                    <li>                      <p><strong>Fixed issue 1432420: Removing multiple BGP                          rules on NSX Edge/DLR at the same time causes web                          client to crash</strong>. <em>This has been fixed in                          NSX 6.2.0. You can now delete multiple BGP rules at a                          time.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1431716-->                    <li>                      <p><strong>Fixed issue 1431716: Protocol address is                          briefly displayed after adding Border Gateway Protocol                          (BGP) deny rule</strong><br />                        You might notice that the protocol address is briefly                        displayed after adding Border Gateway Protocol (BGP)                        deny rule in NSX Edge services gateway. <em>This has                          been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1441773-->                    <li>                      <p><strong>Fixed issue 1441773: VMs disconnect during                          vMotion</strong><br />                        You might notice that VMs disconnect during vMotion or                        you might receive alerts for VMs with disconnected NICs.                        <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1463579-->                    <li>                      <p><strong>Fixed issue 1463579: Unable to download                          controller snapshot</strong><br />                        When downloading controller snapshots, you might notice                        that you are unable to download snapshot for the last                        controller. For example, if you have three controllers,                        one can successfully download snapshots of the first two                        but you might fail to download snapshot of the third                        controller. <em>This has been fixed in NSX 6.2.0.</em></p>                    </li>                  </ul>                  <!--====================================In Fixedbugs==============================================================--->                  <!--====================================Networking and Edge Services Resolved Issues Fixedbugs 0622 0621 0620 -->                  <!--====================================Networking and Edge Services Resolved Issues Fixedbugs -->                  <!--====================================Networking and Edge Services Resolved Issues Fixedbugs -->                  <h3><a name="nsxedgefixedbugs062x"></a>Edge Services Resolved                    Issues in 6.2.3 and earlier</h3>                  <ul>                    <!-- DOCNOTES:  1658022 6.2.3 -->                    <li>                      <p><strong>Fixed issue 1633694: Storage failure may induce                          loss of VXLAN configuration in the NSX Manager                          database</strong><br />                        After a storage failure, Virtual Center may report that                        the DVS is deleted, and NSX Manager responds by removing                        the VXLAN configuration associated with the DVS. When                        this condition occurs, a message similar to <em> "INFO                          DCNPool-9 VcDriver:1077 - Deleting vmknic info from                          host tables [host-21843 : 319]"</em> will be printed                        in the NSX Manager logs.<br />                        <em> This has been fixed in NSX 6.2.3. </em> </p>                    </li>                    <p></p>                    <!-- DOCNOTES:  1646922   6.2.3-->                    <li>                      <p><strong>Fixed issue 1456172: NAT does not translate IP                          addresses when NSX Edge firewall is disabled </strong><br />                        When the Edge gateway firewall is disabled, all stateful                        services also are disabled if the Edge device is a 6.0                        Extra Large or 6.1 and 6.2 Edge device. <em> NSX 6.2.3                          release adds a warning at the UI that other stateful                          services also are disabled. </em></p>                    </li>                    <!-- DOCNOTES:   1646170  6.2.3-->                    <li>                      <p><strong>Fixed issue 1499601: Extended HA failover times                          for Edge Services Gateway (ESG) or DLR with Edge VM                          when using only static routes </strong><br />                        <em> This has been fixed in NSX 6.2.3. </em></p>                    </li>                    <li>                      <!-- DOCNOTES: Fixed issue 1618289 / 1656705 6.2.3-->                      <p><strong>Fixed issue 1618289: Unexpected TCP                          interruption on TCP sessions during Edge High                          Availability (HA) failover in VMware NSX for vSphere                          6.2.x </strong><br />                        This issue occurred due to outdated internal libraries                        that are used in VMware NSX for vSphere 6.2.x. <em>This                          has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1654438  6.2.3 -->                    <li>                      <p><strong>Fixed Issue 1653484: NSX Edge core dumps did                          not display function names</strong><br />                        NSX 6.2.3 enhances debugability by displaying memory                        address information in the core file. However, you must                        enable core dumps only when requested by VMware                        technical support. <br />                        <em>This has been fixed in NSX 6.2.3.</em>                        <!--<em>Workaround</em>: When no longer requested, core dumps should be disabled using the REST API POSThttps://<nsxmgr-ip>/api/4.0/edges/{edgeId}/coredump?enable=true|false. </p> -->                      </p>                      <p> </p>                    </li>                    <!-- DOCNOTES: 1656646 6.2.3-->                    <li>                      <p><strong>Fixed Issue 1604506: Cannot deploy DLR without                          NSX Edge VM if using default gateway for static                          routing use case</strong></p>                      <p> When deploying a new Distributed Logical Router (DLR)                        through the Web Client, by selecting the "Configure                        Default Gateway" option during configuration, the DLR                        fails to create and the following error appears as a                        pop-up window: <em>"[Routing] Admin Distance is                          supported only on NSX Edge version 6.2.0 and later                          with NSX Edge VMs deployed"</em>. </p>                      <p>See the <a target="_blank" href="http://kb.vmware.com/kb/2144551">                          VMware knowledge base article 2144551</a> for more                        information. <em>This has been fixed in NSX 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES:1445057 / 1451734 / 1545868 / 1537279 -->                    <li>                      <p><strong>Fixed issue 1445057: OSPF routes configured on                          NSX Edge Services Gateway (ESG) not honored in the                          logical router (DLR), and affected packets are dropped</strong><br />                        The problem occurs in cases when OSPF uses IP_HDRINCL                        option. On certain Linux kernels, when this option is                        present, it prevents the IP stack from fragmenting the                        packets. Hence, any packets greater than the interface                        MTU are dropped. <em>This has been fixed in NSX 6.1.5                          and NSX 6.2.1.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: doc: 1406471/ dev: 1400350-->                    <p><strong>Fixed issue 1406471: Syslog shows host name of                        backed up NSX Manager on the restored NSX Manager</strong><br />                      Suppose the host name of the first NSX Manager is A and a                      backup is created for that NSX Manager. Now a second NSX                      Manager is installed and configured to the same IP address                      as the old Manager according to backup-restore docs, but                      host name is B. Restore is run on this NSX Manager. The                      restored NSX Manager shows host name A just after restore                      and host name B again after reboot. <em>This has been                        fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    <!-- DOCNOTES: 1444581-->                    <li>                      <p><strong>Fixed issue 1444581: ESXi host might lose                          network connectivity</strong><br />                        An ESXi host might lose network connectivity and                        experience stability issues when multiple error messages                        similar to the following are logged in:                        <br />                        <tt>WARNING: Heartbeat: 785: PCPU 63 didn't have a                          heartbeat for 7 seconds; *may* be locked up</tt>. <em>This                          has been fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1444784/1521664-->                    <li>                      <p><strong>Fixed issue 1444784: VMs disconnect during                          vMotion </strong><br />                        VMs disconnect during vMotion on 6.0.8 with message, <tt>VISP                          heap depleted</tt>. <em>This has been fixed in NSX                          6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1462506 / 1522088-->                    <li>                      <p><strong>Fixed issue 1462506: Cannot redeploy NSX Edge                          with L2VPN Service configured with CA-signed                          certificate </strong><br />                        Cannot redeploy or change size of NSX Edge with L2VPN                        Service configured with CA-signed or self-signed                        certificate. <em>This has been fixed in NSX 6.1.5 and                          NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1440867/1515650-->                    <li>                      <p><strong>Fixed issue 1440867: The message bus randomly                          does not start after NSX Edge reboot</strong><br />                        After restarting an Edge VM, the message bus often does                        not start after powering on, and an additional reboot is                        required. <em>This has been fixed in NSX 6.1.5 and NSX                          6.2.1.</em></p>                    </li>                    <!-- </ul> -->                    <!--====================================sectbreak_edge Fixedbugs==============================================================--->                    <!--====================================sectbreak_edge Fixedbugs==============================================================--->                    <!--====================================sectbreak_edge Fixedbugs==============================================================--->                    <!-- <h3>This section subsumed into Logical Networking Resolved Issues and NSX Edge Resolved Issues</h3> -->                    <!-- <ul> -->                    <!-- DOCNOTES:1548939 -->                    <li>                      <p><strong>Fixed issue 1548939: When configuring a virtual                          server, the previously selected IP address is applied</strong><br />                        When creating a new virtual server, you might notice                        that the IP address is automatically applied from the                        list of previously selected IP pool. This happens when                        you have previously selected an IP pool to derive the                        Virtual Server IP. When you attempt to edit the virtual                        server IP Pool information, the information is not                        automatically sent to the backend from the UI and                        previous IP address derived from the IP Pool is                        automatically applied. <em>This has been fixed in NSX                          6.2.1.</em></p>                      <p></p>                    </li>                    <li>                      <!-- DOCNOTES: Fixed issue  1655754 6.2.3-->                      <p><strong>Fixed issue 1599706: SYN/ACK packet lost in                          communication over LDR between two VNIs</strong><br />                        <em>This has been fixed in NSX 6.2.2.</em>                      </p>                    </li>                    <!-- DOCNOTES: 1082549 / 1076762-->                    <li>                      <p><strong>Fixed issue 1082549: When HA is enabled on Edge                          Services Gateway, OSPF hello and dead interval                          configured to values other than 30 seconds and 120                          seconds                          respectively can cause some traffic loss during                          failover</strong><br />                        When the primary NSX Edge fails with OSPF running and HA                        enabled, the time required for standby to take over                        exceeds the graceful restart timeout and results in OSPF                        neighbors removing learned routes from their Forwarding                        Information Base (FIB) table. This results in dataplane                        outage until OSPF re-initiates converges. <em>This has                          been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1403594/1479964-->                    <li>                      <p><strong>Fixed issue 1403594: VMs are unable to receive                          ping from Edge DHCP server</strong><br />                        VM's can ping the Edge gateway but unable to receive                        DHCP ping from an Edge gateway trunk over an overlay                        network. The Edge DHCP server is setup as a trunk port                        and fails to pass or receive any traffic. However, when                        the Edge Gateway and the DHCP Edge are on the same host                        they are able to ping each other. When the DHCP Edge is                        moved to another host, the DHCP Edge is unable to                        receive ping from the Edge Gateway. <em>This has been                          fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1477176/1430364-->                    <li>                      <p><strong>Fixed issue 1477176: Edge Load Balancer stats                          not correctly displayed in the vSphere Web Client</strong><br />                        The Load Balancer does not display the number of                        concurrent connection statistics in the chart in vSphere                        Web Client UI. <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1399863 / 1392763-->                    <li>                      <p><strong>Fixed issue 1399863: When the direct aggregate                          network in local and remote subnet of an IPsec VPN                          channel is removed,                          the aggregate route to the indirect subnets of the                          peer Edge also disappears</strong><br />                        When there is no default gateway on Edge and you remove                        all of the direct connect                        subnets in local subnets and part of the remote subnets                        at the same time when configuring IPsec,                        the remaining peer subnets become unreachable by IPsec                        VPN. <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1484743 -->                    <li>                      <p><strong>Fixed issue 1484743: Unable to pass traffic                          through load balancer after upgrading to NSX 6.1.2 or                          later</strong><br />                        When using option Insert X-Forwarded-For on NSX Edge                        Load Balancer, traffic may not pass through the load                        balancer. <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1449461-->                    <li>                      <p><strong>Fixed issue 1449461: Running the clear ip ospf                          neighbor command returns a segmentation fault error</strong><br />                        <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1418264/1411857-->                    <li>                      <p><strong>Fixed issue 1418264: Unable to process Kerberos                          requests</strong><br />                        Certain Kerberos requests are failing when being                        balanced with an NSX Edge. <em>This has been fixed in                          NSX 6.2.0.</em></p>                      <p></p>                    </li>                  </ul>                  <!--====================================sectbreak_security Fixedbugs 0621 06210 ==============================================================--->                  <!--====================================sectbreak_security Fixedbugs==============================================================--->                  <!--====================================sectbreak_security Fixedbugs==============================================================--->                  <h3><a name="nsxsecurityfixedbugs062x"></a>Security Services                    Resolved Issues in 6.2.3 and earlier</h3>                  <ul>                    <!-- DOCNOTES: 1647664 6.2.3-->                    <li>                      <p><strong>Fixed Issue 1620109: Deployment of third-party                          service VMs may not complete as expected, and                          Installation Status will be reported as "Failed"</strong><br />                        For example, the SVM may not receive the expected IP                        address. An error message of "Value provided for                        parameter property.info.key was not correct" is seen in                        the NSX Manager logs.</p>                      See <a target="_blank" href="http://kb.vmware.com/kb/2145376">VMware
                        knowledge base article 2145376</a>.<em> This has been                        fixed in NSX 6.2.3.</em></li>                    <!-- DOCNOTES: 1647532  6.2.3-->                    <li>                      <p><strong>Fixed issue 1619570: In a large-scale DFW                          configuration with millions of rules and Service                          Composer, rule publishing may require several seconds                          to complete after a reboot. During this time, new                          rules cannot be published</strong><br />                        NSX 6.2.3 reduces the time to sync the firewall rules on                        reboot by re-syncing only those firewall policies for                        which the latest revision has not been processed due to                        reboot. </p>                    </li>                    <!-- DOCNOTES:   1635230 6.2.3-->                    <li>                      <p><strong>Fixed issue 1526781: Querying the                          getFirewallConfigLayer3SectionByName API does not                          return the responseHeaders field on NSX 6.2.x</strong><br />                        <em> This issue has been fixed in 6.2.3 and the ETag                          header information reinstated in the API output.</em></p>                    </li>                    <!-- DOCNOTES:  1610970 6.2.3-->                    <li>                      <p><strong>Fixed issue 1599576: An edited rule in a                          universal firewall section may fail to be published                          because a null value was being set for the Global                          Section ID field.                        </strong><br />                        No error message is reported. <em>This has been fixed                          in NSX 6.2.3.</em></p>                    </li>                    <!-- DOCNOTE:  1559866 6.2.3 -->                    <li>                      <p><strong>Fixed Issue 1558501: Guest Introspection                          installation may fail when the Universal SVM to NSX                          Manager connection fails</strong> <br />                        When NSX Manager is configured with a FQDN only, the                        messaging channel between NSX Manager and the Guest                        Introspection Service VM may fail.                        When this issue occurs, the Guest Introspection Service                        Status remains in "Warning" status.                        An "UnknownHostException" message is displayed in the                        eventmanager.log file on the USVM. <em>This has been                          fixed in NSX 6.2.3 by adding automatic DNS support.</em></p>                    </li>                    <!-- DOCNOTE: 1673068 -->                    <li>                      <p><strong>Fixed Issue 1673068: Editing firewall rules                          within Service Composer Policy section causes                          out-of-sync configuration</strong> <br />                        Service Composer goes out-of-sync when firewall rules                        are added or edited from within the Service Composer                        policy section of the firewall configuration screen.                        This has been fixed in NSX 6.2.3, by changing the                        Service Composer section of the firewall configuration                        to read-only. Rules created through Service Composer                        must be managed through Service Composer. <em>This has                          been fixed in NSX 6.2.3.</em></p>                    </li>                    <li>                      <!-- DOCNOTES: Fixed issue 1646900 6.2.3-->                      <p><strong>Fixed issue 1639612: MSRPC connectivity issues                          with Windows 2008 and later in NSX for vSphere 6.2.x</strong><br />                        In later versions of Windows which support 64-bit                        addressing, the DCE/EPM protocol negotiates NDR64 as the                        transfer encoding format, leading the firewall to not                        parse the EPM response packet, hence failing to detect                        the dynamic port to open. See <a target="_blank" href="http://kb.vmware.com/kb/2145135">VMware
                          knowledge base article 2145135</a>.<em> This has been                          fixed in NSX 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1567693  6.2.1 -->                    <li>                      <p><strong>Fixed issue 1567693: Using IPset as                          source/destination in NetX rule displays the error                          Invalid container type: IPSet</strong><br />                        <em>This has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1407920-->                    <li>                      <p><strong>Fixed issue 1407920: If you delete the firewall                          configuration using a REST API call, you cannot load                          and publish saved configurations </strong><br />                        When you delete the firewall configuration, a new                        default section is created with a new section ID. When                        you load a saved draft (that has the same section name                        but an older section ID), section names conflict and                        display the following error:<br />                        <code>Duplicate key value violates unique constraint <em>firewall_section_name_key.</em></code><br />                        <em>This has been fixed in NSX 6.2.3.</em> </p>                    </li>                    <li>                      <!-- DOCNOTES: 1498504, 1582806, 1514711 6.2.3-->                      <p><strong>Fixed issue 1498504: VM loses firewall                          protection when removed from one of two overlapping                          service groups</strong><br />                        NetX filters (created by firewall workflow) on the host                        are removed when another service created by Service                        Composer workflow is applied to the same VM. This could                        happen, for example, when one service profile was                        applied to two overlapping service groups. In this case                        if a VM is in both service groups and then is removed                        from one of the service groups, it loses protection. <em>This                          has been fixed in 6.2.3</em> by introducing the <strong>priority</strong>                        field in the service profile. If there are overlapping                        service groups for a VNIC on a host, the service profile                        with the highest priority is applied. </p>                    </li>                    <!-- DOCNOTES: 1591656 6.2.3-->                    <li>                      <p><strong>Fixed issue 1550370: Linux virtual machines                          with NFSv3 mounts experience an operating system hang                          after more than 15 minutes outage on the upstream                          datapath</strong><br />                        See <a target="_blank" href="http://kb.vmware.com/kb/2133815">VMware
knowledge                          base article 2133815</a>. <em>This has been fixed in                          NSX 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES: 1494366-->                    <li>                      <p><strong>Fixed issue 1494366: Copy and paste of a                          firewall rule with negate                          source/destination enabled will list a new rule with                          Negate option                          disabled</strong><br />                        When copying a firewall rule with the negate                        source/destination option enabled,                        a new firewall is created with this option disabled. <em>This                          has been fixed in NSX 6.2.3.</em><br />                      </p>                    </li>                    <p></p>                    <!-- DOCNOTES: 1473767 / 1410165-->                    <li>                      <p><strong>Fixed issue 1473767: Flow Monitoring drops                          flows that exceed a 2 million flows / 5 minutes limit</strong><br />                        NSX Flow Monitoring retains up to 2 million flow                        records. If hosts generate more than 2 million records                        in 5 minutes, new flows are dropped. <em>This has been                          fixed in NSX 6.2.3.</em><br />                        See <a target="_blank" href="http://kb.vmware.com/kb/2091376">VMware
                          knowledge base article 2091376</a>. </p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1658016 6.2.3-->                    <li>                      <p><strong>Fixed issue 1611238: In 6.2.x Edge Firewall                          only the Security Groups created at Edge Scope (SGs at                          Edge Scope can be created only through REST) used to                          appear </strong><br />                        In 6.2.3, SGs created at Global Scope (these can be                        created in UI) and SGs created at Edge scope for the                        corresponding Edge (these can be created only through                        REST) appear in the Edge Firewall in the Security Group                        listing under the Source/Destination columns. <br />                        <em>This has been fixed in NSX 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES: 1644668 6.2.3-->                    <li>                      <p><strong>Fixed issue 1516460: Firewall rules continued                          to be marked as valid even after the applied-to                          logical switch in the rule was deleted</strong><br />                        <em>This has been fixed in 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES:  1644994 6.2.3-->                    <li>                      <p><strong>Fixed issue 1542157: Loss of distributed                          firewall functionality after vMotion of protected VMs                          to destination host </strong><br />                        Removing an NSX-prepared host from the VC inventory                        removes the host entry in the internal firewall tables.                        Later adding that host back to the VC inventory was not                        re-creating the firewall table entries. <em>This has                          been fixed in 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES:  1601301 6.2.3-->                    <li>                      <p><strong>Fixed issue 1592439: Service Composer fails to                          translate virtual machines into security-groups </strong><br />                        This issue occurred due to a deadlock in EpSecLib on the                        Universal Service Virtual Machine (USVM). <em>This has                          been fixed in 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1534597 -->                    <li>                      <p><strong>Fixed issue 1534597: NSX for vSphere 6.x                          Controllers disconnect intermittently</strong> <br />                        Due to an IPSEC bug in the StrongSWAN package that was                        shipping in 6.1.4 and earlier releases, the tunnels                        between controllers weren’t established after IPSEC                        rekeying. This caused partial connectivity failures                        between controllers resulting in multiple different                        issues. For more information see <a target="_blank" href="http://kb.vmware.com/kb/2127655">                          knowledge base article 2127655.</a> <em>This has been                          fixed in NSX 6.1.5 and 6.2.1</em></p>                    </li>                    <!-- DOCNOTES: 1491042 / 1438044 / 1521682 -->                    <li>                      <p><strong>Fixed issue 1491042: LDAP Domain Objects take                          too long to return or fail to return in Security Group                          Object Selection screen</strong>. <em>This has been                          fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1468169 / 1425691 / 1482292 / 1521686 -->                    <li>                      <p><strong>Fixed issue 1468169: Delayed mouse movement                          when viewing FW rules</strong><br />                        In NSX Networking and Security section of vSphere Web                        Client, moving the mouse over rows in the Firewall Rules                        display results in a 3 second delay each time the mouse                        is moved. <em>This has been fixed in NSX 6.1.5 and NSX                          6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1476642 / 1521694-->                    <li>                      <p><strong>Fixed issue 1476642: Some IP Spoofguard rules                          in NSX-v are not applied correctly</strong><br />                        Some IP Spoofguard rules in NSX-v are not applied                        correctly. Instance is not present in the Security Group                        in NSX-v and needs to be manually added to the security                        group. <em>This has been fixed in NSX 6.1.5 and NSX                          6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1510350 / 1511008-->                    <li>                      <p><strong>Fixed issue 1510350: Bulk deletion in Service                          Composer user interface generates "<tt>between 0 to 0</tt>"                          message</strong><br />                        Bulk deletion of policies (~100) from the NSX Service                        Composer user interface generates a message, "<tt>It                          should be between 0 to 0</tt>". You may safely ignore                        this message. <em>This has been fixed in NSX 6.1.5 and                          NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1515656/1450767-->                    <li>                      <p><strong>Fixed issue 1515656: Background operation for                          Policy deletion may take long                          time with high CPU utilization</strong><br />                        Deletion of a policy                        reevaluates all the remaining policies in background.                        This may take                        more than an hour on setups having large number of                        policies, large                        number of security groups, and/or large number of rules                        per                        policy. <em>This has been fixed in NSX 6.1.5 and NSX                          6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: doc 1515630-->                    <li>                      <p><strong>Fixed issue 1515630: All queued publishable                          tasks are marked as failed after the default timeout                          of 20 minutes</strong><br />                        Queues are maintained per NSX Edge and can publish in                        parallel for different Edges. <em>This has been fixed                          in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1545879  -->                    <li>                      <p><strong>Fixed issue 1545879: If you rename an existing                          firewall draft, the operation will fail with the UI                          displaying "Internal Server Error"</strong><br />                        <em>This has been fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1545893 -->                    <li>                      <p><strong>Fixed issue 1545893: Some DFW central CLIs show                          "ERROR output 100" output</strong> <br />                        In some situations, where a Virtual Network Adapter                        (vNIC) is disconnected, a discrepancy can arise between                        the vNIC state information in NSX Manager and the Host                        leading to an "ERROR output 100" in the centralized CLI.                        <em>This has been fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1545853 -->                    <li>                      <p><strong>Fixed issue 1545853: Application Profile list                          is not sorted.</strong> <br />                        The list of Application Profile names in NSX Edge when                        Service Insertion is enabled, is presented in an                        unordered fashion. This release incorporates the fix to                        present the Application Profile list in a sorted manner.                        <em>This has been fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1545895 -->                    <li>                      <p><strong>Fixed issue 1545895: Central CLI commands that                          are run for a specific ESXi host time out on some                          setups</strong> <br />                        <em>This has been fixed in NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1491365-->                    <li>                      <p><strong>Fixed issue 1491365: The vsfwd.log gets                          overwritten quickly with a large                          number of container updates</strong><br />                        After the SpoofGuard policy                        is changed the NSX Manager promptly sends the change to                        host but host                        takes longer to process the change and update the state                        of the virtual                        machine's SpoofGuard state. <em>This has been fixed in                          NSX 6.2.0.</em></p>                    </li>                    <!-- DOCNOTES: 1113755 / 1114361-->                    <li>                      <p><strong>Fixed issue 1113755: Cannot configure NSX                          firewall using security groups or other grouping                          objects defined at global scope</strong><br />                        Administrator users defined at the NSX Edge scope cannot                        access objects defined at the global scope. For example,                        if user <i>abc</i> is defined at Edge scope and                        security group <i>sg-1</i> is defined at global scope,                        then <i>abc</i> will not be able to use <i>sg-1</i> in                        firewall configuration on the NSX Edge. <em>This has                          been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1425691-->                    <li>                      <p><strong>Fixed issue 1425691: Delayed mouse movement                          when viewing FW rules</strong><br />                        In the NSX Networking and Security section of vSphere                        Web Client, moving the mouse over rows in the Firewall                        Rules display results in a 3 second delay. <em>This has                          been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: dev 1352926-->                    <li>                      <p><strong>Fixed issue 1352926: UI shows error <code>Firewall                            Publish Failed</code> despite successful publish</strong><br />                        If Distributed Firewall is enabled on a subset of                        clusters in your environment and you update an                        application group that is used in one or more active                        firewall rules, any publish action on the UI will                        display an error message containing IDs of the hosts                        belonging to the clusters where NSX firewall is not                        enabled. <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: no doc bug / 1295384/1294092-->                    <li>                      <p><strong>Fixed issue 1295384: Deleting security rules                          via REST displays error</strong><br />                        If a REST API call is used to delete security rules                        created by Service Composer, the corresponding rule set                        is not actually deleted in the service                        profile cache resulting in an <code>ObjectNotFoundException</code>                        error. <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1412713/1467651-->                    <li>                      <p><strong>Fixed issue 1412713: Firewall rules do not                          reflect newly added virtual machine </strong><br />                        When new VMs were added to the logical switch, firewall                        rules are not updated correctly to include the newly                        added VMs. If you make a change to the firewall and                        publish changes the new objects are added to the policy.                        <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1448022-->                    <li>                      <p><strong>Fixed issue 1448022: Cannot select Active                          Directory objects when configuring security groups</strong><br />                        In NSX 6.1.x, AD/LDAP Domain Objects took a long time to                        return in the Security Group Object selection screen. <em>This                          has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1473585/1491906-->                    <li>                      <p><strong>Fixed issue 1473585: Cannot add firewall rule                          with source/destination as multiple comma-separated IP                          addresses</strong>. <em>This has been fixed in NSX                          6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1460351-->                    <li>                      <p><strong>Fixed issue 1460351: Unable to move NSX                          Distributed Firewall (DFW) section at the top of the                          list</strong><br />                        When using Service Composer to create a security group                        policy, the section created in the DFW table cannot be                        added to the top of the list.                        There is no way to move DFW section up or down. <em>This                          has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: dev 1398106/1501451 doc 1501451-->                    <li>                      <p><strong>Fixed issue 1501451: Security policy configured                          as a port range causes firewall to go out of sync</strong><br />                        Configuring security policies as a port range (for                        example, "5900-5964") will cause the firewall to go out                        of sync with a <code>NumberFormatException</code>                        error. <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                  </ul>                  <!--====================================sectbreak_monitoring Fixedbugs 0621 06210 ==============================================================--->                  <!--====================================sectbreak_monitoring Fixedbugs==============================================================--->                  <!--====================================sectbreak_monitoring Fixedbugs==============================================================--->                  <h3><a name="monitoringfixedbugs062x"></a>Monitoring Services                    Resolved Issues in 6.2.3 and earlier</h3>                  <ul>                    <!-- DOCNOTES: 1658018 6.2.3-->                    <li>                      <p><strong>Fixed issue 1617561: <em>vmkernel</em> log                          files floods with <em>"ALERT: vdrb: VdrArpInput:1015:                            CP:Malformed pkt"</em></strong><br />                        This happens when networking device such as server is                        sending ARP request in IEEE 802 Networks ARP format. <em>This                          has been fixed in 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES:  1644967 6.2.3-->                    <li>                      <p><strong>Fixed issue 1525620: The icmpCode value in a                          distributed firewall rule was not being sent to the                          host. The protocolName and subProtocolName values work                          as expected</strong><br />                        <em>This has been fixed in 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1601294  6.2.3-->                    <li>                      <p><strong>Fixed issue 1563830: Applying firewall rule on                          a DLR appliance with the source or destination                          as'mgmtInterface' fails</strong><br />                        A message similar to "vShield Edge:10014:Configuration                        failed on NSX Edge vm" is reported in the NSX Manager                        logs. <em>This has been fixed in 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1646927  6.2.3-->                    <li>                      <p><strong>Fixed issue 1474498: Importing draft firewall                          rules fails after existing firewall configuration is                          removed by a REST API request </strong><br />                        This issue occurs when drafts are created in VMware NSX                        for vSphere 6.1.x and 6.2.x containing <em>section id =                          null</em>. <em>This has been fixed in 6.2.3.</em> </p>                    </li>                    <!-- DOCNOTES: 1545888-->                    <li>                      <p><strong>Fixed issue 1545888: When reporting flow                          statistics, index 0 (bytes-in) and index 1 (bytes out)                          counts are sometimes reversed. </strong><br />                        Index 0 holds the counts for traffic for the origination                        direction and, and index 1 holds the counts for traffic                        in the reverse direction. <em>This has been fixed in                          NSX 6.2.1.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1460085-->                    <li>                      <p><strong>Fixed issue 1460085: The <tt>#show interface</tt>                          command does not display the bandwidth/speed of vNic_0                          interface </strong><br />                        After running the "#show interface" command, a full                        duplex, "0 M/s" speed is displayed but not the                        bandwidth/speed of NSX Edge vNic_0 interface. <em>This                          has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1288395 No doc bug / 1288395-->                    <li><strong>Fixed issue 1288395: When IPFIX configuration is                        enabled for Distributed Firewall, firewall ports in the                        ESXi management interface for NetFlow for vDS or SNMP                        may be removed</strong><br />                      When a collector IP and port is defined for IPFIX, the                      firewall for ESXi management interface is opened up in the                      outbound direction for the                      specified UDP collector ports. This operation may remove                      the dynamic ruleset configuration on ESXi management                      interface firewall for the following services if they were                      previously configured on the ESXi host:<br />                      <ul type="square">                        <li>Netflow collector port configuration on vDS</li>                        <li>SNMP target port configuration</li>                      </ul>                      <br />                      <em>This has been fixed in NSX 6.2.0.</em>                    </li>                    <!-- DOCNOTES: 1354728-->                    <li>                      <p><strong>Fixed issue 1354728: Unable to process <tt>Denied/Block</tt>                          events through IPFIX protocol</strong><br />                        Typically, vsfwd user process handles the collection of                        flows, including dropped/denied ones and processes them                        for IPFIX.                        This happens when the IPFIX Collector fails to see the <tt>Denied/Block</tt>                        events because the vSIP drop packet queue is either too                        narrow or is wrapped around by inactive flow events.                        In this release, the ability to send <tt>Denied/Block</tt>                        events using the IPFIX protocol is implemented. <em>This                          has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                  </ul>                  <!--====================================sectbreak_interop Fixedbugs 0621 06210 ==============================================================--->                  <!--====================================sectbreak_interop Fixedbugs==============================================================--->                  <!--====================================sectbreak_interop Fixedbugs==============================================================--->                  <h3><a name="interopfixedbugs062x"></a>Solution                    Interoperability Resolved Issues in 6.2.3 and earlier</h3>                  <ul>                    <!-- DOCNOTES: 1571170 6.2.1-->                    <li>                      <p><strong>Fixed issue 1571170: Some Log Insight reports                          are not supported in NSX 6.2 with some vRealize                          Content Pack versions</strong><br />                        <em>This has been fixed in the latest version of the Log                          Insight Content Pack.</em> Download and install the                        content pack from <a target="_blank" href="https://solutionexchange.vmware.com/store/products/nsx-for-vsphere-content-pack-v3">VMware
                          Solution Exchange</a>. <em>This has been fixed in NSX                          6.2.3.</em></p>                    </li>                    <!-- DOCNOTES: 1484506-->                    <li>                      <p><strong>Fixed Issue 1484506: Purple diagnostic screen                          during ESXi upgrade</strong><br />                        When you are upgrading an NSX-enabled vSphere 5.5U2 host                        to vSphere 6.0, some of the ESXi host upgrades might                        halt with a purple diagnostic screen. See <a target="_blank"                          href="http://kb.vmware.com/kb/2137826">VMware
                          knowledge base article 2137826</a>. <em>This has been                          fixed in 6.2.3.</em></p>                    </li>                    <!-- DOCNOTES: 1453802-->                    <li>                      <p><strong>Fixed issue 1453802: Copy of VM via vCloud                          Connector fails when route traverses NSX Load Balancer</strong>.                        <em>This has been fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1462006 / 1521691-->                    <li>                      <p><strong>Fixed issue 1462006: In VIO Deployment, some                          newly deployed VMs appear to have valid port and IPs                          assigned but do not have access to the network</strong>.                        <em>This has been fixed in NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1482665/1439714-->                    <li>                      <p><strong>Fixed issue 1482665: Slow login to NSX tab of                          vSphere web client with AD-backed SSO</strong><br />                        In NSX for vSphere installations that use SSO for AD                        authentication, the user's initial login to the NSX                        Networking and Security section of the vSphere Web                        Client takes a long time. <em>This has been fixed in                          NSX 6.1.5 and NSX 6.2.1.</em></p>                    </li>                    <!-- DOCNOTES: 1326669-->                    <li>                      <p><strong>Fixed issue 1326669: Unable to set up                          organizational network</strong><br />                        When attempting to set up an organization-wide network,                        vCloud                        Director fails with an error message. <em>This has been                          fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                    <!-- DOCNOTES: 1497044/1403116-->                    <li>                      <p><strong>Fixed issue 1497044: Unable to launch multiple                          VMs using VIO setup</strong><br />                        Users using VMware Integrated OpenStack were unable to                        launch large numbers of VMs or publish large numbers of                        firewall rules in a short period of time. This resulted                        in <tt>Error publishing ip for vnic</tt> messages in                        the log. <em>This has been fixed in NSX 6.2.0.</em></p>                      <p></p>                    </li>                  </ul>                  <p></p>                  <!--====================================End of resolved issues section==============================================================--->                  <!--====================================End of resolved issues section==============================================================--->                  <!--====================================End of resolved issues section==============================================================--->                  <!--====================================End of resolved issues section==============================================================--->                  <a name="revisionhistory"></a>                  <h2>Document Revision History</h2>                  <p>                    20 August 2015: First edition for NSX 6.2.0.<br />                    17 December 2015: First edition for NSX 6.2.1.<br />                    4 March 2016: First edition for NSX 6.2.2. Security patch to                    address glibc vulnerability.<br />                    9 June 2016: First edition for NSX 6.2.3.<br />                    25 August 2016: First edition for NSX 6.2.4.<br />                    02 September 2016: Second edition for NSX 6.2.4. Added known                    issue.<br />                    09 September 2016: Third edition for NSX 6.2.4. Added known                    issue.<br />                    23 September 2016: Fourth edition for NSX 6.2.4. Moved 2                    known issues to resolved.<br />                    06 October 2016: Fifth edition for NSX 6.2.4. Added known                    issues.<br />                    16 November 2016: Sixth edition for NSX 6.2.4. Added KB.<br />                    28 November 2016: Sixth edition for NSX 6.2.4. Changes to                    bug 1685894.</p>                  <!-- ///*** end of content area ***/// -->                  <!-- ///*** Do not modify anything after this comment ***/// -->                </td>              </tr>            </tbody>          </table>        </div>        <!-- content -->      </div>      <!-- content-container -->      <div id="ifooter"></div>      <script src="/Release_Notes/scripts/techpub_relnote.js" type="text/javascript"></script>    </div>  </body></html>